{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_v0.zip'\n",
    "urllib.request.urlretrieve(url, 'data.zip')\n",
    "from zipfile import ZipFile\n",
    "with ZipFile('data.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in different directory\n",
    "   zipObj.extractall('data')\n",
    "    \n",
    "url = 'https://github.com/chrdiller/pyTorchChamferDistance/archive/master.zip'\n",
    "urllib.request.urlretrieve(url, 'chamfer.zip')\n",
    "with ZipFile('chamfer.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in different directory\n",
    "   zipObj.extractall('')\n",
    "    \n",
    "url = 'https://github.com/meder411/PyTorch-EMDLoss/archive/master.zip'\n",
    "urllib.request.urlretrieve(url, 'emd.zip')\n",
    "with ZipFile('emd.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in different directory\n",
    "   zipObj.extractall('')\n",
    "    \n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs have been detected:1\n",
      "Random Seed:  2866\n",
      "number of training data:15990\n",
      "number of testing data:1785\n",
      "model building...\n",
      "training mode ------------------\n",
      "epoch:0\n",
      "training loss is:6.619719982147217\n",
      "training loss is:0.019710224121809006\n",
      "training loss is:0.010472262278199196\n",
      "training loss is:0.015419594943523407\n",
      "training loss is:0.009701407514512539\n",
      "training loss is:0.014090017415583134\n",
      "training loss is:0.007824632339179516\n",
      "training loss is:0.010071864351630211\n",
      "training loss is:0.009033850394189358\n",
      "training loss is:0.010414700955152512\n",
      "training loss is:0.008028747513890266\n",
      "training loss is:0.005313782021403313\n",
      "training loss is:0.005707578733563423\n",
      "training loss is:0.008179374039173126\n",
      "training loss is:0.007255700416862965\n",
      "training loss is:0.007073071785271168\n",
      "training loss is:0.008052784949541092\n",
      "training loss is:0.006962824612855911\n",
      "training loss is:0.006793557666242123\n",
      "training loss is:0.004782738164067268\n",
      "Testing loss is:0.004579806700348854\n",
      "epoch:1\n",
      "training loss is:0.004969630856066942\n",
      "training loss is:0.005688470788300037\n",
      "training loss is:0.003705008653923869\n",
      "training loss is:0.005276533775031567\n",
      "training loss is:0.005707075819373131\n",
      "training loss is:0.004721121862530708\n",
      "training loss is:0.004297497216612101\n",
      "training loss is:0.0037472923286259174\n",
      "training loss is:0.0038746807258576155\n",
      "training loss is:0.010919686406850815\n",
      "training loss is:0.003636440495029092\n",
      "training loss is:0.0038057081401348114\n",
      "training loss is:0.003396245650947094\n",
      "training loss is:0.003861582837998867\n",
      "training loss is:0.002990569919347763\n",
      "training loss is:0.004335553385317326\n",
      "training loss is:0.003287198720499873\n",
      "training loss is:0.003775902558118105\n",
      "training loss is:0.0034133384469896555\n",
      "training loss is:0.0037164094392210245\n",
      "Testing loss is:0.0022537042386829853\n",
      "epoch:2\n",
      "training loss is:0.003109964542090893\n",
      "training loss is:0.0034109412226825953\n",
      "training loss is:0.003962901886552572\n",
      "training loss is:0.0035041850060224533\n",
      "training loss is:0.0029056151397526264\n",
      "training loss is:0.0024358450900763273\n",
      "training loss is:0.002696432638913393\n",
      "training loss is:0.002017732011154294\n",
      "training loss is:0.003422208596020937\n",
      "training loss is:0.003878606017678976\n",
      "training loss is:0.00386393116787076\n",
      "training loss is:0.0023744385689496994\n",
      "training loss is:0.003945428878068924\n",
      "training loss is:0.00468630064278841\n",
      "training loss is:0.002838045358657837\n",
      "training loss is:0.004983371589332819\n",
      "training loss is:0.0028164589311927557\n",
      "training loss is:0.0017553686629980803\n",
      "training loss is:0.002345171757042408\n",
      "training loss is:0.0024543688632547855\n",
      "Testing loss is:0.003454575315117836\n",
      "epoch:3\n",
      "training loss is:0.0037158941850066185\n",
      "training loss is:0.0034810653887689114\n",
      "training loss is:0.0028088893741369247\n",
      "training loss is:0.0039035778027027845\n",
      "training loss is:0.0025822047609835863\n",
      "training loss is:0.0024348811712116003\n",
      "training loss is:0.0022484411019831896\n",
      "training loss is:0.0028850026428699493\n",
      "training loss is:0.002209252445027232\n",
      "training loss is:0.002174020279198885\n",
      "training loss is:0.0018065047916024923\n",
      "training loss is:0.001707808580249548\n",
      "training loss is:0.0030358266085386276\n",
      "training loss is:0.0018581923795863986\n",
      "training loss is:0.002103672595694661\n",
      "training loss is:0.0020781082566827536\n",
      "training loss is:0.002526944037526846\n",
      "training loss is:0.0017185587203130126\n",
      "training loss is:0.0025730542838573456\n",
      "training loss is:0.002731594257056713\n",
      "Testing loss is:0.0022582216188311577\n",
      "epoch:4\n",
      "training loss is:0.002883145585656166\n",
      "training loss is:0.0026161260902881622\n",
      "training loss is:0.0017408798448741436\n",
      "training loss is:0.0016959444619715214\n",
      "training loss is:0.0017274970887228847\n",
      "training loss is:0.0017469096928834915\n",
      "training loss is:0.0024118698202073574\n",
      "training loss is:0.0025871661491692066\n",
      "training loss is:0.002037756610661745\n",
      "training loss is:0.0026741542387753725\n",
      "training loss is:0.003776137251406908\n",
      "training loss is:0.002662485698238015\n",
      "training loss is:0.002101275371387601\n",
      "training loss is:0.0021259398199617863\n",
      "training loss is:0.0028159404173493385\n",
      "training loss is:0.0026496935170143843\n",
      "training loss is:0.0018465295433998108\n",
      "training loss is:0.0016939506167545915\n",
      "training loss is:0.001155739650130272\n",
      "training loss is:0.002091181930154562\n",
      "Testing loss is:0.0014847649727016687\n",
      "epoch:5\n",
      "training loss is:0.0014865078264847398\n",
      "training loss is:0.001648578094318509\n",
      "training loss is:0.0020509306341409683\n",
      "training loss is:0.0016498866025358438\n",
      "training loss is:0.001526320818811655\n",
      "training loss is:0.002294190227985382\n",
      "training loss is:0.0014837670605629683\n",
      "training loss is:0.001741247484460473\n",
      "training loss is:0.00164405454415828\n",
      "training loss is:0.002171817934140563\n",
      "training loss is:0.0012897057458758354\n",
      "training loss is:0.0012255875626578927\n",
      "training loss is:0.001383309718221426\n",
      "training loss is:0.0026888535358011723\n",
      "training loss is:0.0016350842779502273\n",
      "training loss is:0.0015551659744232893\n",
      "training loss is:0.0013577865902334452\n",
      "training loss is:0.0015132514527067542\n",
      "training loss is:0.0017130018677562475\n",
      "training loss is:0.001954565290361643\n",
      "Testing loss is:0.002965946914628148\n",
      "epoch:6\n",
      "training loss is:0.0015916023403406143\n",
      "training loss is:0.0017815062310546637\n",
      "training loss is:0.0014509777538478374\n",
      "training loss is:0.0018367909360677004\n",
      "training loss is:0.0012900624424219131\n",
      "training loss is:0.00278416252695024\n",
      "training loss is:0.001317235641181469\n",
      "training loss is:0.002144153695553541\n",
      "training loss is:0.002102190162986517\n",
      "training loss is:0.0015342393890023232\n",
      "training loss is:0.00242989300750196\n",
      "training loss is:0.001565443817526102\n",
      "training loss is:0.0013831239193677902\n",
      "training loss is:0.0014572159852832556\n",
      "training loss is:0.001200598431751132\n",
      "training loss is:0.0011511859484016895\n",
      "training loss is:0.0016106115654110909\n",
      "training loss is:0.0017300512408837676\n",
      "training loss is:0.0015334668569266796\n",
      "training loss is:0.0016640385147184134\n",
      "Testing loss is:0.00481734424829483\n",
      "epoch:7\n",
      "training loss is:0.0018473600503057241\n",
      "training loss is:0.0016893665306270123\n",
      "training loss is:0.0018359804525971413\n",
      "training loss is:0.0019652056507766247\n",
      "training loss is:0.0013235791120678186\n",
      "training loss is:0.0016592607134953141\n",
      "training loss is:0.0017972288187593222\n",
      "training loss is:0.0015083621256053448\n",
      "training loss is:0.0019002044573426247\n",
      "training loss is:0.0015423968434333801\n",
      "training loss is:0.0011991405626758933\n",
      "training loss is:0.002219507936388254\n",
      "training loss is:0.0022396459244191647\n",
      "training loss is:0.00140550103969872\n",
      "training loss is:0.002234599320217967\n",
      "training loss is:0.0013567807618528605\n",
      "training loss is:0.001876199385151267\n",
      "training loss is:0.0020080851390957832\n",
      "training loss is:0.0014687932562083006\n",
      "training loss is:0.0016843837220221758\n",
      "Testing loss is:0.00211508572101593\n",
      "epoch:8\n",
      "training loss is:0.0014426996931433678\n",
      "training loss is:0.0012050975346937776\n",
      "training loss is:0.0015331091126427054\n",
      "training loss is:0.0012413434451445937\n",
      "training loss is:0.0015973832923918962\n",
      "training loss is:0.0016783863538876176\n",
      "training loss is:0.0010647972812876105\n",
      "training loss is:0.0011555409291759133\n",
      "training loss is:0.002551065059378743\n",
      "training loss is:0.0014717784943059087\n",
      "training loss is:0.0017053917981684208\n",
      "training loss is:0.0028807688504457474\n",
      "training loss is:0.0018895319662988186\n",
      "training loss is:0.0014928572345525026\n",
      "training loss is:0.0017386903055012226\n",
      "training loss is:0.0014803060330450535\n",
      "training loss is:0.0015130100073292851\n",
      "training loss is:0.0013973712921142578\n",
      "training loss is:0.0013389186933636665\n",
      "training loss is:0.001661001704633236\n",
      "Testing loss is:0.0013336344854906201\n",
      "epoch:9\n",
      "training loss is:0.0011061154073104262\n",
      "training loss is:0.0011429134756326675\n",
      "training loss is:0.0015894630923867226\n",
      "training loss is:0.001152889453805983\n",
      "training loss is:0.00137498346157372\n",
      "training loss is:0.001526320818811655\n",
      "training loss is:0.0013682888820767403\n",
      "training loss is:0.0011472731130197644\n",
      "training loss is:0.0011274245334789157\n",
      "training loss is:0.001526147359982133\n",
      "training loss is:0.0014892092440277338\n",
      "training loss is:0.001453996985219419\n",
      "training loss is:0.0013540107756853104\n",
      "training loss is:0.001243598060682416\n",
      "training loss is:0.0016716609243303537\n",
      "training loss is:0.002207089914008975\n",
      "training loss is:0.0010587162105366588\n",
      "training loss is:0.001472635194659233\n",
      "training loss is:0.0009914797265082598\n",
      "training loss is:0.0015237934421747923\n",
      "Testing loss is:0.0008746591629460454\n",
      "epoch:10\n",
      "training loss is:0.001832412788644433\n",
      "training loss is:0.0017200419679284096\n",
      "training loss is:0.0011676340363919735\n",
      "training loss is:0.001455649035051465\n",
      "training loss is:0.0014087334275245667\n",
      "training loss is:0.0019928787369281054\n",
      "training loss is:0.0014199527213349938\n",
      "training loss is:0.0013047773391008377\n",
      "training loss is:0.002079196972772479\n",
      "training loss is:0.0019727367907762527\n",
      "training loss is:0.00136562658008188\n",
      "training loss is:0.001524481805972755\n",
      "training loss is:0.0012005467433482409\n",
      "training loss is:0.001723828143440187\n",
      "training loss is:0.0011580053251236677\n",
      "training loss is:0.0014917865628376603\n",
      "training loss is:0.0013658653479069471\n",
      "training loss is:0.0012652953155338764\n",
      "training loss is:0.0021550932433456182\n",
      "training loss is:0.0012440956197679043\n",
      "Testing loss is:0.0011583701707422733\n",
      "epoch:11\n",
      "training loss is:0.0012769617605954409\n",
      "training loss is:0.002158590592443943\n",
      "training loss is:0.0015071735251694918\n",
      "training loss is:0.0014897193759679794\n",
      "training loss is:0.0011157754343003035\n",
      "training loss is:0.0010892902500927448\n",
      "training loss is:0.000963645288720727\n",
      "training loss is:0.0013827707152813673\n",
      "training loss is:0.001226658234372735\n",
      "training loss is:0.0012953004334121943\n",
      "training loss is:0.0012961805332452059\n",
      "training loss is:0.0015442746225744486\n",
      "training loss is:0.0013104657409712672\n",
      "training loss is:0.0012688359711319208\n",
      "training loss is:0.001128151547163725\n",
      "training loss is:0.001621586037799716\n",
      "training loss is:0.0013637605588883162\n",
      "training loss is:0.0017177269328385592\n",
      "training loss is:0.001121740322560072\n",
      "training loss is:0.0017651779344305396\n",
      "Testing loss is:0.001224983250722289\n",
      "epoch:12\n",
      "training loss is:0.0015251358272507787\n",
      "training loss is:0.0013484649825841188\n",
      "training loss is:0.0015203420771285892\n",
      "training loss is:0.0009784889407455921\n",
      "training loss is:0.0014525114092975855\n",
      "training loss is:0.0012830165214836597\n",
      "training loss is:0.0014397285412997007\n",
      "training loss is:0.0012566142249852419\n",
      "training loss is:0.0012750924797728658\n",
      "training loss is:0.0016288150800392032\n",
      "training loss is:0.0013120101066306233\n",
      "training loss is:0.0015903094317764044\n",
      "training loss is:0.001486150547862053\n",
      "training loss is:0.0009900727309286594\n",
      "training loss is:0.0014321007765829563\n",
      "training loss is:0.0009299095254391432\n",
      "training loss is:0.002228211145848036\n",
      "training loss is:0.0017247759969905019\n",
      "training loss is:0.00104609876871109\n",
      "training loss is:0.0013089508283883333\n",
      "Testing loss is:0.0009071552194654942\n",
      "epoch:13\n",
      "training loss is:0.0008957781828939915\n",
      "training loss is:0.0015565520152449608\n",
      "training loss is:0.0008857421344146132\n",
      "training loss is:0.0009945370256900787\n",
      "training loss is:0.001066678436473012\n",
      "training loss is:0.0013957147020846605\n",
      "training loss is:0.0011834749020636082\n",
      "training loss is:0.0013396915746852756\n",
      "training loss is:0.001120309578254819\n",
      "training loss is:0.0009728324948810041\n",
      "training loss is:0.0015812554629519582\n",
      "training loss is:0.0008610221557319164\n",
      "training loss is:0.0016336303669959307\n",
      "training loss is:0.0009891862282529473\n",
      "training loss is:0.0011477925581857562\n",
      "training loss is:0.0019323511514812708\n",
      "training loss is:0.0011591996299102902\n",
      "training loss is:0.0013352981768548489\n",
      "training loss is:0.0009385666926391423\n",
      "training loss is:0.0014284448698163033\n",
      "Testing loss is:0.0015931131783872843\n",
      "epoch:14\n",
      "training loss is:0.0011724615469574928\n",
      "training loss is:0.0010393294505774975\n",
      "training loss is:0.0011083936551585793\n",
      "training loss is:0.001332390122115612\n",
      "training loss is:0.0009552420815452933\n",
      "training loss is:0.0010907762916758657\n",
      "training loss is:0.0011932124616578221\n",
      "training loss is:0.0011664852499961853\n",
      "training loss is:0.0012311306782066822\n",
      "training loss is:0.0012134145945310593\n",
      "training loss is:0.0010530479485169053\n",
      "training loss is:0.000757662346586585\n",
      "training loss is:0.0010018262546509504\n",
      "training loss is:0.0020430278964340687\n",
      "training loss is:0.001339736394584179\n",
      "training loss is:0.0014897689688950777\n",
      "training loss is:0.0016649153549224138\n",
      "training loss is:0.0009688036516308784\n",
      "training loss is:0.001508089480921626\n",
      "training loss is:0.0012960137100890279\n",
      "Testing loss is:0.0008826861158013344\n",
      "epoch:15\n",
      "training loss is:0.0010516040492802858\n",
      "training loss is:0.0009561597835272551\n",
      "training loss is:0.0012366354931145906\n",
      "training loss is:0.0010430512484163046\n",
      "training loss is:0.0010489763226360083\n",
      "training loss is:0.0012321779504418373\n",
      "training loss is:0.0009195432066917419\n",
      "training loss is:0.0011743538780137897\n",
      "training loss is:0.0009279476944357157\n",
      "training loss is:0.0010716079268604517\n",
      "training loss is:0.0010198554955422878\n",
      "training loss is:0.001516950549557805\n",
      "training loss is:0.0012776930816471577\n",
      "training loss is:0.0008940111147239804\n",
      "training loss is:0.000840379623696208\n",
      "training loss is:0.001090517733246088\n",
      "training loss is:0.0008595087565481663\n",
      "training loss is:0.0009447542252019048\n",
      "training loss is:0.001364672789350152\n",
      "training loss is:0.0011523712892085314\n",
      "Testing loss is:0.001016351510770619\n",
      "epoch:16\n",
      "training loss is:0.0013432791456580162\n",
      "training loss is:0.0008476244984194636\n",
      "training loss is:0.0009913600515574217\n",
      "training loss is:0.0010311747901141644\n",
      "training loss is:0.0009787373710423708\n",
      "training loss is:0.0010260565904900432\n",
      "training loss is:0.001154790399596095\n",
      "training loss is:0.0007242041756398976\n",
      "training loss is:0.000883624772541225\n",
      "training loss is:0.0009714555344544351\n",
      "training loss is:0.0007990354206413031\n",
      "training loss is:0.0010004290379583836\n",
      "training loss is:0.0010403778869658709\n",
      "training loss is:0.0010971474694088101\n",
      "training loss is:0.00084020133363083\n",
      "training loss is:0.0009266623528674245\n",
      "training loss is:0.001227939035743475\n",
      "training loss is:0.0012079596053808928\n",
      "training loss is:0.001258823205716908\n",
      "training loss is:0.0008601488079875708\n",
      "Testing loss is:0.0009637449984438717\n",
      "epoch:17\n",
      "training loss is:0.0009998141322284937\n",
      "training loss is:0.0010849162936210632\n",
      "training loss is:0.0009518569568172097\n",
      "training loss is:0.0007533461903221905\n",
      "training loss is:0.0008448316948488355\n",
      "training loss is:0.0011383460368961096\n",
      "training loss is:0.0012691504089161754\n",
      "training loss is:0.0008931192569434643\n",
      "training loss is:0.0008629875956103206\n",
      "training loss is:0.0006937218131497502\n",
      "training loss is:0.0007347682258114219\n",
      "training loss is:0.0011770181590691209\n",
      "training loss is:0.0010729464702308178\n",
      "training loss is:0.0008189380168914795\n",
      "training loss is:0.0012811576016247272\n",
      "training loss is:0.0008157285628840327\n",
      "training loss is:0.0007343654287979007\n",
      "training loss is:0.0008955728262662888\n",
      "training loss is:0.0008120379643514752\n",
      "training loss is:0.0008624676847830415\n",
      "Testing loss is:0.0007970414008013904\n",
      "epoch:18\n",
      "training loss is:0.001057226094417274\n",
      "training loss is:0.0010294390376657248\n",
      "training loss is:0.0012365812435746193\n",
      "training loss is:0.0008861121023073792\n",
      "training loss is:0.0012155147269368172\n",
      "training loss is:0.0009808188769966364\n",
      "training loss is:0.0006924735498614609\n",
      "training loss is:0.0008355507743544877\n",
      "training loss is:0.0017555379308760166\n",
      "training loss is:0.0008885078132152557\n",
      "training loss is:0.0011267669033259153\n",
      "training loss is:0.0010271399514749646\n",
      "training loss is:0.0011952886125072837\n",
      "training loss is:0.0009856163524091244\n",
      "training loss is:0.0008823792450129986\n",
      "training loss is:0.0009907474741339684\n",
      "training loss is:0.0007671694038435817\n",
      "training loss is:0.0010320371948182583\n",
      "training loss is:0.0009766269940882921\n",
      "training loss is:0.0011684554629027843\n",
      "Testing loss is:0.0005168888019397855\n",
      "epoch:19\n",
      "training loss is:0.001075590611435473\n",
      "training loss is:0.000827285461127758\n",
      "training loss is:0.0013453473802655935\n",
      "training loss is:0.0013077714247629046\n",
      "training loss is:0.0009334793430753052\n",
      "training loss is:0.0008501791162416339\n",
      "training loss is:0.0009949536761268973\n",
      "training loss is:0.0009933710098266602\n",
      "training loss is:0.0009053315152414143\n",
      "training loss is:0.0008353567100130022\n",
      "training loss is:0.0007785378256812692\n",
      "training loss is:0.000663039565552026\n",
      "training loss is:0.0013855323195457458\n",
      "training loss is:0.0009289903682656586\n",
      "training loss is:0.0007059126510284841\n",
      "training loss is:0.0009939889423549175\n",
      "training loss is:0.0009358207462355494\n",
      "training loss is:0.0008011196041479707\n",
      "training loss is:0.0007725425530225039\n",
      "training loss is:0.0009933686815202236\n",
      "Testing loss is:0.0012285516131669283\n",
      "epoch:20\n",
      "training loss is:0.0012170234695076942\n",
      "training loss is:0.0009032210800796747\n",
      "training loss is:0.0006928390357643366\n",
      "training loss is:0.000885959598235786\n",
      "training loss is:0.0011393071617931128\n",
      "training loss is:0.0008264092030003667\n",
      "training loss is:0.0010666181333363056\n",
      "training loss is:0.001273652072995901\n",
      "training loss is:0.000802423688583076\n",
      "training loss is:0.000815183506347239\n",
      "training loss is:0.0012381619308143854\n",
      "training loss is:0.0005948435864411294\n",
      "training loss is:0.0010206922888755798\n",
      "training loss is:0.0012843150179833174\n",
      "training loss is:0.0010455166921019554\n",
      "training loss is:0.0009309671586379409\n",
      "training loss is:0.0007969279540702701\n",
      "training loss is:0.0008382939849980175\n",
      "training loss is:0.0008865892887115479\n",
      "training loss is:0.0008520593401044607\n",
      "Testing loss is:0.0008175033144652843\n",
      "epoch:21\n",
      "training loss is:0.000992759712971747\n",
      "training loss is:0.000919813581276685\n",
      "training loss is:0.000850402342621237\n",
      "training loss is:0.0008382101077586412\n",
      "training loss is:0.000887163623701781\n",
      "training loss is:0.0011860958766192198\n",
      "training loss is:0.0008725424995645881\n",
      "training loss is:0.0007716203690506518\n",
      "training loss is:0.000977922580204904\n",
      "training loss is:0.0008206090424209833\n",
      "training loss is:0.0007362636970356107\n",
      "training loss is:0.0008716555312275887\n",
      "training loss is:0.0006536307628266513\n",
      "training loss is:0.0007234563818201423\n",
      "training loss is:0.0008612944511696696\n",
      "training loss is:0.001044741366058588\n",
      "training loss is:0.0009424384916201234\n",
      "training loss is:0.0009891605004668236\n",
      "training loss is:0.0007750496733933687\n",
      "training loss is:0.001140290405601263\n",
      "Testing loss is:0.0009613740257918835\n",
      "epoch:22\n",
      "training loss is:0.0010683462023735046\n",
      "training loss is:0.000683506135828793\n",
      "training loss is:0.0007857708260416985\n",
      "training loss is:0.0007069536950439215\n",
      "training loss is:0.000729976745788008\n",
      "training loss is:0.0009432556107640266\n",
      "training loss is:0.0010472453432157636\n",
      "training loss is:0.001029308303259313\n",
      "training loss is:0.0009866433683782816\n",
      "training loss is:0.0009618353797122836\n",
      "training loss is:0.0009702049428597093\n",
      "training loss is:0.000933463335968554\n",
      "training loss is:0.001228921115398407\n",
      "training loss is:0.0007276113610714674\n",
      "training loss is:0.0008437518263235688\n",
      "training loss is:0.0007180094835348427\n",
      "training loss is:0.0006887870840728283\n",
      "training loss is:0.0007666083402000368\n",
      "training loss is:0.0009834507945924997\n",
      "training loss is:0.0007802713080309331\n",
      "Testing loss is:0.0013601604150608182\n",
      "epoch:23\n",
      "training loss is:0.000695828115567565\n",
      "training loss is:0.0008792649023234844\n",
      "training loss is:0.00062588817672804\n",
      "training loss is:0.0010452286805957556\n",
      "training loss is:0.0008919191313907504\n",
      "training loss is:0.0007333698449656367\n",
      "training loss is:0.0008433804614469409\n",
      "training loss is:0.0009679141221567988\n",
      "training loss is:0.001151554984971881\n",
      "training loss is:0.0007344273035414517\n",
      "training loss is:0.0006776453228667378\n",
      "training loss is:0.0009359422256238759\n",
      "training loss is:0.0009379911934956908\n",
      "training loss is:0.0009193876758217812\n",
      "training loss is:0.0009107732912525535\n",
      "training loss is:0.0007663938449695706\n",
      "training loss is:0.0008755432791076601\n",
      "training loss is:0.0008847700664773583\n",
      "training loss is:0.0006085781496949494\n",
      "training loss is:0.0007433460559695959\n",
      "Testing loss is:0.0008522897260263562\n",
      "epoch:24\n",
      "training loss is:0.000910906819626689\n",
      "training loss is:0.0006328512099571526\n",
      "training loss is:0.0006019357824698091\n",
      "training loss is:0.0008035444188863039\n",
      "training loss is:0.0006654385942965746\n",
      "training loss is:0.0009129437385126948\n",
      "training loss is:0.0008221520693041384\n",
      "training loss is:0.001043887110427022\n",
      "training loss is:0.0006237011984921992\n",
      "training loss is:0.0007691701175644994\n",
      "training loss is:0.0008215572452172637\n",
      "training loss is:0.0010353887919336557\n",
      "training loss is:0.0008153013186529279\n",
      "training loss is:0.000780872767791152\n",
      "training loss is:0.000673976493999362\n",
      "training loss is:0.0011554702650755644\n",
      "training loss is:0.0007252292707562447\n",
      "training loss is:0.0008340829517692327\n",
      "training loss is:0.000751390412915498\n",
      "training loss is:0.0006702652899548411\n",
      "Testing loss is:0.0010453646536916494\n",
      "epoch:25\n",
      "training loss is:0.0008221183670684695\n",
      "training loss is:0.0008672504918649793\n",
      "training loss is:0.0008839179063215852\n",
      "training loss is:0.0011332749854773283\n",
      "training loss is:0.0006546828662976623\n",
      "training loss is:0.0007762092864140868\n",
      "training loss is:0.0008283102652058005\n",
      "training loss is:0.0006141395424492657\n",
      "training loss is:0.0007811912801116705\n",
      "training loss is:0.000777824898250401\n",
      "training loss is:0.0008235482964664698\n",
      "training loss is:0.0010972653981298208\n",
      "training loss is:0.0006293565966188908\n",
      "training loss is:0.0009077979484573007\n",
      "training loss is:0.0009677596390247345\n",
      "training loss is:0.0009044327307492495\n",
      "training loss is:0.0007300475845113397\n",
      "training loss is:0.0007141492678783834\n",
      "training loss is:0.0010770519729703665\n",
      "training loss is:0.0005810253787785769\n",
      "Testing loss is:0.0010500361677259207\n",
      "epoch:26\n",
      "training loss is:0.0008496035588905215\n",
      "training loss is:0.0008696191944181919\n",
      "training loss is:0.0008945261361077428\n",
      "training loss is:0.0006820939015597105\n",
      "training loss is:0.0007680613780394197\n",
      "training loss is:0.0008281961199827492\n",
      "training loss is:0.0008161274599842727\n",
      "training loss is:0.000839992135297507\n",
      "training loss is:0.0009379161056131124\n",
      "training loss is:0.0009056490380316973\n",
      "training loss is:0.0009444188326597214\n",
      "training loss is:0.0007205290603451431\n",
      "training loss is:0.000895011005923152\n",
      "training loss is:0.0007056471076793969\n",
      "training loss is:0.0008148910710588098\n",
      "training loss is:0.0006651285802945495\n",
      "training loss is:0.0007421534974128008\n",
      "training loss is:0.0006176309543661773\n",
      "training loss is:0.0007186344591900706\n",
      "training loss is:0.0008216769201681018\n",
      "Testing loss is:0.0008775395108386874\n",
      "epoch:27\n",
      "training loss is:0.000561084016226232\n",
      "training loss is:0.0007607331499457359\n",
      "training loss is:0.0009277296485379338\n",
      "training loss is:0.0007205433212220669\n",
      "training loss is:0.0009293981129303575\n",
      "training loss is:0.0008434257470071316\n",
      "training loss is:0.0006883545429445803\n",
      "training loss is:0.0007707307813689113\n",
      "training loss is:0.0006584493676200509\n",
      "training loss is:0.0008536349632777274\n",
      "training loss is:0.0008606304181739688\n",
      "training loss is:0.0009075016714632511\n",
      "training loss is:0.0008466946892440319\n",
      "training loss is:0.0007508016424253583\n",
      "training loss is:0.0008853042381815612\n",
      "training loss is:0.0008456821087747812\n",
      "training loss is:0.00047892919974401593\n",
      "training loss is:0.000723842647857964\n",
      "training loss is:0.0007311546942219138\n",
      "training loss is:0.0006001335568726063\n",
      "Testing loss is:0.0004269953933544457\n",
      "epoch:28\n",
      "training loss is:0.0006065265042707324\n",
      "training loss is:0.0006274307379499078\n",
      "training loss is:0.0007067620754241943\n",
      "training loss is:0.0008190188091248274\n",
      "training loss is:0.0006852778606116772\n",
      "training loss is:0.0008728756220079958\n",
      "training loss is:0.0007899418706074357\n",
      "training loss is:0.0008284329669550061\n",
      "training loss is:0.0008731709094718099\n",
      "training loss is:0.0009738647495396435\n",
      "training loss is:0.0007360281888395548\n",
      "training loss is:0.0007564186817035079\n",
      "training loss is:0.0005117201944813132\n",
      "training loss is:0.0010068522533401847\n",
      "training loss is:0.0006033436511643231\n",
      "training loss is:0.000699917902238667\n",
      "training loss is:0.0008205457124859095\n",
      "training loss is:0.0008852230384945869\n",
      "training loss is:0.0010285306489095092\n",
      "training loss is:0.0006872927770018578\n",
      "Testing loss is:0.000620773178525269\n",
      "epoch:29\n",
      "training loss is:0.000787825440056622\n",
      "training loss is:0.0007645582081750035\n",
      "training loss is:0.0005868076113983989\n",
      "training loss is:0.0007601979305036366\n",
      "training loss is:0.0006489048828370869\n",
      "training loss is:0.0007341684540733695\n",
      "training loss is:0.0007187355658970773\n",
      "training loss is:0.0006612259894609451\n",
      "training loss is:0.0009214428719133139\n",
      "training loss is:0.0006837442051619291\n",
      "training loss is:0.000683305028360337\n",
      "training loss is:0.0007995244232006371\n",
      "training loss is:0.0006815746892243624\n",
      "training loss is:0.0007614262867718935\n",
      "training loss is:0.0007779495790600777\n",
      "training loss is:0.0007900022901594639\n",
      "training loss is:0.0008863494731485844\n",
      "training loss is:0.0006380402483046055\n",
      "training loss is:0.0007071876316331327\n",
      "training loss is:0.0007900656200945377\n",
      "Testing loss is:0.000648503948468715\n",
      "epoch:30\n",
      "training loss is:0.0007630856707692146\n",
      "training loss is:0.0005901476251892745\n",
      "training loss is:0.0007067585829645395\n",
      "training loss is:0.0007728743366897106\n",
      "training loss is:0.0006511673564091325\n",
      "training loss is:0.0006964346393942833\n",
      "training loss is:0.0007401613984256983\n",
      "training loss is:0.0007824648055247962\n",
      "training loss is:0.0007842237828299403\n",
      "training loss is:0.0008783383527770638\n",
      "training loss is:0.0007240775739774108\n",
      "training loss is:0.0006189189734868705\n",
      "training loss is:0.0006817179964855313\n",
      "training loss is:0.0006731063476763666\n",
      "training loss is:0.0006860489374957979\n",
      "training loss is:0.0005941180279478431\n",
      "training loss is:0.0006365064764395356\n",
      "training loss is:0.0007554453331977129\n",
      "training loss is:0.000545992748811841\n",
      "training loss is:0.0008293325081467628\n",
      "Testing loss is:0.0008191944216378033\n",
      "epoch:31\n",
      "training loss is:0.0005971743375994265\n",
      "training loss is:0.0007740131113678217\n",
      "training loss is:0.0007757446146570146\n",
      "training loss is:0.0006482455646619201\n",
      "training loss is:0.0007165742572396994\n",
      "training loss is:0.0006873317179270089\n",
      "training loss is:0.0005513297510333359\n",
      "training loss is:0.000737614172976464\n",
      "training loss is:0.0007819260936230421\n",
      "training loss is:0.0008653601398691535\n",
      "training loss is:0.0006501253228634596\n",
      "training loss is:0.0008663939079269767\n",
      "training loss is:0.0007554338662885129\n",
      "training loss is:0.0007296240655705333\n",
      "training loss is:0.000933896517381072\n",
      "training loss is:0.000768372556194663\n",
      "training loss is:0.0007440843619406223\n",
      "training loss is:0.0006817493704147637\n",
      "training loss is:0.0008840330992825329\n",
      "training loss is:0.0005621312884613872\n",
      "Testing loss is:0.0005855621420778334\n",
      "epoch:32\n",
      "training loss is:0.0006221305811777711\n",
      "training loss is:0.0007002771599218249\n",
      "training loss is:0.0007703346782363951\n",
      "training loss is:0.0006253654137253761\n",
      "training loss is:0.0006529890233650804\n",
      "training loss is:0.0007192770717665553\n",
      "training loss is:0.0008566153701394796\n",
      "training loss is:0.0006620449130423367\n",
      "training loss is:0.0007313723908737302\n",
      "training loss is:0.000575487851165235\n",
      "training loss is:0.0008057951927185059\n",
      "training loss is:0.000755535438656807\n",
      "training loss is:0.0007200369145721197\n",
      "training loss is:0.0007624902646057308\n",
      "training loss is:0.0006859436398372054\n",
      "training loss is:0.0007366226054728031\n",
      "training loss is:0.0007558417273685336\n",
      "training loss is:0.0007625253638252616\n",
      "training loss is:0.0006825907621532679\n",
      "training loss is:0.0005057794041931629\n",
      "Testing loss is:0.0005320395575836301\n",
      "epoch:33\n",
      "training loss is:0.0007159515516832471\n",
      "training loss is:0.0006507720099762082\n",
      "training loss is:0.0006746831932105124\n",
      "training loss is:0.0005832117749378085\n",
      "training loss is:0.0007365153287537396\n",
      "training loss is:0.0007826124783605337\n",
      "training loss is:0.0010081165237352252\n",
      "training loss is:0.0006124069914221764\n",
      "training loss is:0.0007089549326337874\n",
      "training loss is:0.0007061905926093459\n",
      "training loss is:0.0006654469179920852\n",
      "training loss is:0.0005930315237492323\n",
      "training loss is:0.0006703621475026011\n",
      "training loss is:0.0007817840669304132\n",
      "training loss is:0.0006048189243301749\n",
      "training loss is:0.0006087446818128228\n",
      "training loss is:0.0007118031498976052\n",
      "training loss is:0.0007266533211804926\n",
      "training loss is:0.0005809972062706947\n",
      "training loss is:0.0006890076911076903\n",
      "Testing loss is:0.0007137703360058367\n",
      "epoch:34\n",
      "training loss is:0.0006432535592466593\n",
      "training loss is:0.000565979047678411\n",
      "training loss is:0.0007825423963367939\n",
      "training loss is:0.0008403378305956721\n",
      "training loss is:0.0007465427042916417\n",
      "training loss is:0.0006581180496141315\n",
      "training loss is:0.0007203001878224313\n",
      "training loss is:0.0007265510503202677\n",
      "training loss is:0.0007378150476142764\n",
      "training loss is:0.0006555764703080058\n",
      "training loss is:0.0005793307209387422\n",
      "training loss is:0.0007939046481624246\n",
      "training loss is:0.0005861067911610007\n",
      "training loss is:0.0006315649952739477\n",
      "training loss is:0.000650020781904459\n",
      "training loss is:0.0005581973819062114\n",
      "training loss is:0.0005670644459314644\n",
      "training loss is:0.0006959517486393452\n",
      "training loss is:0.00041564059210941195\n",
      "training loss is:0.0007588912267237902\n",
      "Testing loss is:0.0007327263592742383\n",
      "epoch:35\n",
      "training loss is:0.0006669021095149219\n",
      "training loss is:0.000608219881542027\n",
      "training loss is:0.0006152830901555717\n",
      "training loss is:0.000506470154505223\n",
      "training loss is:0.0007690065540373325\n",
      "training loss is:0.0006596003659069538\n",
      "training loss is:0.0007662840653210878\n",
      "training loss is:0.0007045470993034542\n",
      "training loss is:0.0007623748970218003\n",
      "training loss is:0.0008864337578415871\n",
      "training loss is:0.0005962175782769918\n",
      "training loss is:0.0009249909780919552\n",
      "training loss is:0.0005656782304868102\n",
      "training loss is:0.0006472473032772541\n",
      "training loss is:0.0006346440641209483\n",
      "training loss is:0.0007285830215550959\n",
      "training loss is:0.0006741632241755724\n",
      "training loss is:0.00073508161585778\n",
      "training loss is:0.0006034607067704201\n",
      "training loss is:0.0006711039459332824\n",
      "Testing loss is:0.0007496578618884087\n",
      "epoch:36\n",
      "training loss is:0.0006358694517984986\n",
      "training loss is:0.0006534230196848512\n",
      "training loss is:0.0006072261021472514\n",
      "training loss is:0.0008425147389061749\n",
      "training loss is:0.0004655471129808575\n",
      "training loss is:0.0006204185774549842\n",
      "training loss is:0.0006854560924693942\n",
      "training loss is:0.0005082485731691122\n",
      "training loss is:0.0005667497171089053\n",
      "training loss is:0.0006253706524148583\n",
      "training loss is:0.0006261831149458885\n",
      "training loss is:0.0008924331632442772\n",
      "training loss is:0.00062037433963269\n",
      "training loss is:0.0006224242970347404\n",
      "training loss is:0.0005873051704838872\n",
      "training loss is:0.0006836937973275781\n",
      "training loss is:0.0005605029291473329\n",
      "training loss is:0.0006242021918296814\n",
      "training loss is:0.000653808587230742\n",
      "training loss is:0.000773272302467376\n",
      "Testing loss is:0.0006470533553510904\n",
      "epoch:37\n",
      "training loss is:0.0006055973935872316\n",
      "training loss is:0.0004926900728605688\n",
      "training loss is:0.0006578263128176332\n",
      "training loss is:0.0005347552942112088\n",
      "training loss is:0.0005044909194111824\n",
      "training loss is:0.0004800790920853615\n",
      "training loss is:0.000541447545401752\n",
      "training loss is:0.0007134083425626159\n",
      "training loss is:0.000647328794002533\n",
      "training loss is:0.0006633331067860126\n",
      "training loss is:0.0006236197659745812\n",
      "training loss is:0.0006339804967865348\n",
      "training loss is:0.0006848296034149826\n",
      "training loss is:0.0006637696642428637\n",
      "training loss is:0.0008012786274775863\n",
      "training loss is:0.0005719785112887621\n",
      "training loss is:0.000594568729866296\n",
      "training loss is:0.0005746131064370275\n",
      "training loss is:0.0006134940776973963\n",
      "training loss is:0.0007696772227063775\n",
      "Testing loss is:0.0007624404388479888\n",
      "epoch:38\n",
      "training loss is:0.000667803455144167\n",
      "training loss is:0.000631547998636961\n",
      "training loss is:0.0006529762758873403\n",
      "training loss is:0.0006800085538998246\n",
      "training loss is:0.0005405207630246878\n",
      "training loss is:0.0004889674601145089\n",
      "training loss is:0.0005163060268387198\n",
      "training loss is:0.0006391603965312243\n",
      "training loss is:0.00069596798857674\n",
      "training loss is:0.0005628353683277965\n",
      "training loss is:0.000648965360596776\n",
      "training loss is:0.0006136888405308127\n",
      "training loss is:0.0006257774075493217\n",
      "training loss is:0.0006957148434594274\n",
      "training loss is:0.000608869013376534\n",
      "training loss is:0.0006493692053481936\n",
      "training loss is:0.0004605333670042455\n",
      "training loss is:0.0006208275444805622\n",
      "training loss is:0.000644214334897697\n",
      "training loss is:0.0005593685782514513\n",
      "Testing loss is:0.0006490817759186029\n",
      "epoch:39\n",
      "training loss is:0.0006657293997704983\n",
      "training loss is:0.0007312851957976818\n",
      "training loss is:0.000561154680326581\n",
      "training loss is:0.0005739061161875725\n",
      "training loss is:0.0006445954786613584\n",
      "training loss is:0.0007207597373053432\n",
      "training loss is:0.000598474289290607\n",
      "training loss is:0.0006228735437616706\n",
      "training loss is:0.0006978202145546675\n",
      "training loss is:0.0005747913382947445\n",
      "training loss is:0.0006210461724549532\n",
      "training loss is:0.0005533068906515837\n",
      "training loss is:0.000565701921004802\n",
      "training loss is:0.0007208085153251886\n",
      "training loss is:0.0005347895785234869\n",
      "training loss is:0.0005905028083361685\n",
      "training loss is:0.0006670206785202026\n",
      "training loss is:0.0007115220068953931\n",
      "training loss is:0.0005801958031952381\n",
      "training loss is:0.0006721429526805878\n",
      "Testing loss is:0.0006415760144591331\n",
      "epoch:40\n",
      "training loss is:0.000652276212349534\n",
      "training loss is:0.0005625415360555053\n",
      "training loss is:0.0006298577063716948\n",
      "training loss is:0.0007409683312289417\n",
      "training loss is:0.0005106631433591247\n",
      "training loss is:0.00058435631217435\n",
      "training loss is:0.0006384900771081448\n",
      "training loss is:0.0006756979855708778\n",
      "training loss is:0.000528386328369379\n",
      "training loss is:0.0006477824063040316\n",
      "training loss is:0.0007523460080847144\n",
      "training loss is:0.0005555727402679622\n",
      "training loss is:0.0006075679557397962\n",
      "training loss is:0.0006745977443642914\n",
      "training loss is:0.0006873146630823612\n",
      "training loss is:0.0006386445602402091\n",
      "training loss is:0.000634300522506237\n",
      "training loss is:0.0007352347020059824\n",
      "training loss is:0.0006233715685084462\n",
      "training loss is:0.0006658255006186664\n",
      "Testing loss is:0.0004024340305477381\n",
      "epoch:41\n",
      "training loss is:0.000644153740722686\n",
      "training loss is:0.0004503206873778254\n",
      "training loss is:0.0006695989286527038\n",
      "training loss is:0.0006672902964055538\n",
      "training loss is:0.0006282042013481259\n",
      "training loss is:0.0006124681094661355\n",
      "training loss is:0.0006897641578689218\n",
      "training loss is:0.000545067130587995\n",
      "training loss is:0.0006509563536383212\n",
      "training loss is:0.0004893604200333357\n",
      "training loss is:0.0005556608666665852\n",
      "training loss is:0.0006734167691320181\n",
      "training loss is:0.0006297057261690497\n",
      "training loss is:0.0006556027219630778\n",
      "training loss is:0.0005050280597060919\n",
      "training loss is:0.0006212354055605829\n",
      "training loss is:0.0004999714437872171\n",
      "training loss is:0.0005904560093767941\n",
      "training loss is:0.0005277780001051724\n",
      "training loss is:0.0005396961933001876\n",
      "Testing loss is:0.0006199950003065169\n",
      "epoch:42\n",
      "training loss is:0.0005581415025517344\n",
      "training loss is:0.0005748186958953738\n",
      "training loss is:0.000544455018825829\n",
      "training loss is:0.0006286320858635008\n",
      "training loss is:0.0005479450337588787\n",
      "training loss is:0.00047481805086135864\n",
      "training loss is:0.0007527382695116103\n",
      "training loss is:0.0004930407158099115\n",
      "training loss is:0.0006852824008092284\n",
      "training loss is:0.0006731502944603562\n",
      "training loss is:0.0006936953868716955\n",
      "training loss is:0.00081944081466645\n",
      "training loss is:0.0006217187619768083\n",
      "training loss is:0.0005686904769390821\n",
      "training loss is:0.0005997708067297935\n",
      "training loss is:0.0006060752202756703\n",
      "training loss is:0.0006041256710886955\n",
      "training loss is:0.0006028402131050825\n",
      "training loss is:0.0004013340803794563\n",
      "training loss is:0.000700669246725738\n",
      "Testing loss is:0.0007237201789394021\n",
      "epoch:43\n",
      "training loss is:0.000546177732758224\n",
      "training loss is:0.0006773342611268163\n",
      "training loss is:0.0005703683709725738\n",
      "training loss is:0.0006298033986240625\n",
      "training loss is:0.0005579754943028092\n",
      "training loss is:0.0005555200623348355\n",
      "training loss is:0.0006023721070960164\n",
      "training loss is:0.0006640658248215914\n",
      "training loss is:0.0006119079189375043\n",
      "training loss is:0.0005522334249690175\n",
      "training loss is:0.0005660621682181954\n",
      "training loss is:0.0005303119542077184\n",
      "training loss is:0.0006889997748658061\n",
      "training loss is:0.0006005289615131915\n",
      "training loss is:0.000601672101765871\n",
      "training loss is:0.0004881463828496635\n",
      "training loss is:0.0006715920171700418\n",
      "training loss is:0.000630177790299058\n",
      "training loss is:0.0005366309778764844\n",
      "training loss is:0.000587278394959867\n",
      "Testing loss is:0.0006822500145062804\n",
      "epoch:44\n",
      "training loss is:0.0005995331448502839\n",
      "training loss is:0.0005063150892965496\n",
      "training loss is:0.0006660614162683487\n",
      "training loss is:0.0005924996803514659\n",
      "training loss is:0.0006899472209624946\n",
      "training loss is:0.0005938305985182524\n",
      "training loss is:0.0005658325972035527\n",
      "training loss is:0.0005301005439832807\n",
      "training loss is:0.0006769910687580705\n",
      "training loss is:0.0004761253367178142\n",
      "training loss is:0.0006071167881600559\n",
      "training loss is:0.0005408019060268998\n",
      "training loss is:0.0005299694603309035\n",
      "training loss is:0.0006495489506050944\n",
      "training loss is:0.0006019837455824018\n",
      "training loss is:0.0005678743473254144\n",
      "training loss is:0.0005552953807637095\n",
      "training loss is:0.0006417492404580116\n",
      "training loss is:0.0007291277288459241\n",
      "training loss is:0.000585871865041554\n",
      "Testing loss is:0.0007127110729925334\n",
      "epoch:45\n",
      "training loss is:0.0005807892885059118\n",
      "training loss is:0.0006028481293469667\n",
      "training loss is:0.0005555091192945838\n",
      "training loss is:0.0005906793521717191\n",
      "training loss is:0.0007123749237507582\n",
      "training loss is:0.0007198133389465511\n",
      "training loss is:0.0005367294070310891\n",
      "training loss is:0.0006419484270736575\n",
      "training loss is:0.0005990986246615648\n",
      "training loss is:0.00048421643441542983\n",
      "training loss is:0.000599654158577323\n",
      "training loss is:0.000528463046066463\n",
      "training loss is:0.0005668705562129617\n",
      "training loss is:0.0006284530973061919\n",
      "training loss is:0.0006654951721429825\n",
      "training loss is:0.0006318239029496908\n",
      "training loss is:0.0006018950371071696\n",
      "training loss is:0.0004982586833648384\n",
      "training loss is:0.0005254569696262479\n",
      "training loss is:0.0005858872318640351\n",
      "Testing loss is:0.000497420085594058\n",
      "epoch:46\n",
      "training loss is:0.0005505874869413674\n",
      "training loss is:0.0006494319532066584\n",
      "training loss is:0.0005380151560530066\n",
      "training loss is:0.0005758627085015178\n",
      "training loss is:0.0005350433057174087\n",
      "training loss is:0.0005351161817088723\n",
      "training loss is:0.0007344891200773418\n",
      "training loss is:0.0006694223848171532\n",
      "training loss is:0.0006171157583594322\n",
      "training loss is:0.0004678795230574906\n",
      "training loss is:0.0006589427357539535\n",
      "training loss is:0.0005593316163867712\n",
      "training loss is:0.0005379501963034272\n",
      "training loss is:0.000704682432115078\n",
      "training loss is:0.0005937387468293309\n",
      "training loss is:0.000561185646802187\n",
      "training loss is:0.0005426690331660211\n",
      "training loss is:0.0007255555829033256\n",
      "training loss is:0.0005076429224573076\n",
      "training loss is:0.0006521073519252241\n",
      "Testing loss is:0.0006249547004699707\n",
      "epoch:47\n",
      "training loss is:0.0005956533132120967\n",
      "training loss is:0.00039708963595330715\n",
      "training loss is:0.00059746322222054\n",
      "training loss is:0.0005608890787698328\n",
      "training loss is:0.000718068506103009\n",
      "training loss is:0.0005466073052957654\n",
      "training loss is:0.000467686535557732\n",
      "training loss is:0.0006781368283554912\n",
      "training loss is:0.000619088183157146\n",
      "training loss is:0.0005185240879654884\n",
      "training loss is:0.000510118727106601\n",
      "training loss is:0.0005984585150144994\n",
      "training loss is:0.0005717768799513578\n",
      "training loss is:0.0005267616943456233\n",
      "training loss is:0.000597413454670459\n",
      "training loss is:0.0006076351273804903\n",
      "training loss is:0.0005072051426395774\n",
      "training loss is:0.0006544737843796611\n",
      "training loss is:0.0005711709382012486\n",
      "training loss is:0.000602931366302073\n",
      "Testing loss is:0.0004766686470247805\n",
      "epoch:48\n",
      "training loss is:0.0005718684988096356\n",
      "training loss is:0.0005531094502657652\n",
      "training loss is:0.0005314465961419046\n",
      "training loss is:0.0007225341396406293\n",
      "training loss is:0.00038650009082630277\n",
      "training loss is:0.0006046242779120803\n",
      "training loss is:0.00044138479279354215\n",
      "training loss is:0.0006033116951584816\n",
      "training loss is:0.0005956281092949212\n",
      "training loss is:0.000550331431441009\n",
      "training loss is:0.0005583157762885094\n",
      "training loss is:0.0006292564794421196\n",
      "training loss is:0.0006478590075857937\n",
      "training loss is:0.0006352973869070411\n",
      "training loss is:0.0006668010610155761\n",
      "training loss is:0.0004097004421055317\n",
      "training loss is:0.0005914433277212083\n",
      "training loss is:0.0006237624911591411\n",
      "training loss is:0.0005674439016729593\n",
      "training loss is:0.0006290765013545752\n",
      "Testing loss is:0.0006707159336656332\n",
      "epoch:49\n",
      "training loss is:0.0005327307153493166\n",
      "training loss is:0.0005264486535452306\n",
      "training loss is:0.0005461849505081773\n",
      "training loss is:0.0005378527566790581\n",
      "training loss is:0.0005854637129232287\n",
      "training loss is:0.0005102098220959306\n",
      "training loss is:0.0005384813994169235\n",
      "training loss is:0.0006406678003259003\n",
      "training loss is:0.0005493820644915104\n",
      "training loss is:0.000547538511455059\n",
      "training loss is:0.00046643184032291174\n",
      "training loss is:0.0005679426249116659\n",
      "training loss is:0.000659828307107091\n",
      "training loss is:0.0005196587881073356\n",
      "training loss is:0.0006272558239288628\n",
      "training loss is:0.0005822358652949333\n",
      "training loss is:0.0005105232121422887\n",
      "training loss is:0.0004990843590348959\n",
      "training loss is:0.0006069912342354655\n",
      "training loss is:0.0007771645905449986\n",
      "Testing loss is:0.0004993549082428217\n",
      "epoch:50\n",
      "training loss is:0.0007125041447579861\n",
      "training loss is:0.0005140153225511312\n",
      "training loss is:0.0005381463561207056\n",
      "training loss is:0.00066213752143085\n",
      "training loss is:0.0005525284796021879\n",
      "training loss is:0.0005171060911379755\n",
      "training loss is:0.0005614170804619789\n",
      "training loss is:0.0005512816132977605\n",
      "training loss is:0.0006177030154503882\n",
      "training loss is:0.0005273265996947885\n",
      "training loss is:0.00045988080091774464\n",
      "training loss is:0.0005291600245982409\n",
      "training loss is:0.0005790321156382561\n",
      "training loss is:0.0005715592415072024\n",
      "training loss is:0.0005074615473859012\n",
      "training loss is:0.0006477843271568418\n",
      "training loss is:0.0005662682233378291\n",
      "training loss is:0.0006182066863402724\n",
      "training loss is:0.0006252371240407228\n",
      "training loss is:0.000511301914229989\n",
      "Testing loss is:0.00039574893889948726\n",
      "epoch:51\n",
      "training loss is:0.0006071797106415033\n",
      "training loss is:0.0005828588036820292\n",
      "training loss is:0.0007860159385018051\n",
      "training loss is:0.000573087832890451\n",
      "training loss is:0.0004733180976472795\n",
      "training loss is:0.0006182792130857706\n",
      "training loss is:0.0004641492269001901\n",
      "training loss is:0.0005833456525579095\n",
      "training loss is:0.0005424628616310656\n",
      "training loss is:0.0006624502129852772\n",
      "training loss is:0.0006466592894867063\n",
      "training loss is:0.0005674599669873714\n",
      "training loss is:0.0004615311336237937\n",
      "training loss is:0.000409371976274997\n",
      "training loss is:0.0005884385900571942\n",
      "training loss is:0.00041450129356235266\n",
      "training loss is:0.0006429477361962199\n",
      "training loss is:0.0005311223212629557\n",
      "training loss is:0.0005548298358917236\n",
      "training loss is:0.0006764124846085906\n",
      "Testing loss is:0.0004604943096637726\n",
      "epoch:52\n",
      "training loss is:0.0005206416826695204\n",
      "training loss is:0.0005451013566926122\n",
      "training loss is:0.0004968480789102614\n",
      "training loss is:0.0004913798184134066\n",
      "training loss is:0.0005116558750160038\n",
      "training loss is:0.0005425611743703485\n",
      "training loss is:0.000591724063269794\n",
      "training loss is:0.0005508585018105805\n",
      "training loss is:0.0004774538683705032\n",
      "training loss is:0.0006184932426549494\n",
      "training loss is:0.0005040434189140797\n",
      "training loss is:0.0005202528554946184\n",
      "training loss is:0.0006087303627282381\n",
      "training loss is:0.0005438746884465218\n",
      "training loss is:0.0006467715138569474\n",
      "training loss is:0.0005150315118953586\n",
      "training loss is:0.0005259650060907006\n",
      "training loss is:0.0004782821051776409\n",
      "training loss is:0.0006065934430807829\n",
      "training loss is:0.0004389490932226181\n",
      "Testing loss is:0.0004004002548754215\n",
      "epoch:53\n",
      "training loss is:0.00050688988994807\n",
      "training loss is:0.0005006407154724002\n",
      "training loss is:0.000612344010733068\n",
      "training loss is:0.0005211524548940361\n",
      "training loss is:0.0004894590238109231\n",
      "training loss is:0.00048526638420298696\n",
      "training loss is:0.0005554108647629619\n",
      "training loss is:0.0005183556932024658\n",
      "training loss is:0.0004924306413158774\n",
      "training loss is:0.0006170492270030081\n",
      "training loss is:0.0005300270859152079\n",
      "training loss is:0.0005006738938391209\n",
      "training loss is:0.0006553845596499741\n",
      "training loss is:0.0005187913775444031\n",
      "training loss is:0.0004930272116325796\n",
      "training loss is:0.0006152099231258035\n",
      "training loss is:0.0004691916692536324\n",
      "training loss is:0.0005036655347794294\n",
      "training loss is:0.0004917712649330497\n",
      "training loss is:0.00044835516018792987\n",
      "Testing loss is:0.00040317748789675534\n",
      "epoch:54\n",
      "training loss is:0.0005856027128174901\n",
      "training loss is:0.0004492026346269995\n",
      "training loss is:0.0005254709394648671\n",
      "training loss is:0.0005415285704657435\n",
      "training loss is:0.000440459989476949\n",
      "training loss is:0.0005401051021181047\n",
      "training loss is:0.0004748795472551137\n",
      "training loss is:0.0005994867533445358\n",
      "training loss is:0.00043582788202911615\n",
      "training loss is:0.0005554273375310004\n",
      "training loss is:0.0007337471470236778\n",
      "training loss is:0.0005495939403772354\n",
      "training loss is:0.0006422240985557437\n",
      "training loss is:0.00048354873433709145\n",
      "training loss is:0.0006258201319724321\n",
      "training loss is:0.0004894989542663097\n",
      "training loss is:0.0005875555798411369\n",
      "training loss is:0.00047425832599401474\n",
      "training loss is:0.0004880386986769736\n",
      "training loss is:0.0005086524761281908\n",
      "Testing loss is:0.00041878060437738895\n",
      "epoch:55\n",
      "training loss is:0.0004392836708575487\n",
      "training loss is:0.0004235139931552112\n",
      "training loss is:0.0005437529180198908\n",
      "training loss is:0.0004602159606292844\n",
      "training loss is:0.0006340596009977162\n",
      "training loss is:0.0004742601013276726\n",
      "training loss is:0.0005625345511361957\n",
      "training loss is:0.000621314044110477\n",
      "training loss is:0.00039353861939162016\n",
      "training loss is:0.0005546751199290156\n",
      "training loss is:0.0006129503017291427\n",
      "training loss is:0.0005007951986044645\n",
      "training loss is:0.0005444327834993601\n",
      "training loss is:0.0004913830198347569\n",
      "training loss is:0.0005103684961795807\n",
      "training loss is:0.0005097207031212747\n",
      "training loss is:0.00048182523460127413\n",
      "training loss is:0.0004849203396588564\n",
      "training loss is:0.0005563314189203084\n",
      "training loss is:0.0005692202830687165\n",
      "Testing loss is:0.000708596664480865\n",
      "epoch:56\n",
      "training loss is:0.0005130971549078822\n",
      "training loss is:0.0005079058464616537\n",
      "training loss is:0.0005016833310946822\n",
      "training loss is:0.00044848164543509483\n",
      "training loss is:0.0005203552427701652\n",
      "training loss is:0.0004778420552611351\n",
      "training loss is:0.0005595010006800294\n",
      "training loss is:0.0005872572073712945\n",
      "training loss is:0.0005936598172411323\n",
      "training loss is:0.000489110010676086\n",
      "training loss is:0.0005031121545471251\n",
      "training loss is:0.0004258106928318739\n",
      "training loss is:0.0005600470467470586\n",
      "training loss is:0.0005694375140592456\n",
      "training loss is:0.00047211209312081337\n",
      "training loss is:0.00044806243386119604\n",
      "training loss is:0.0004855266015511006\n",
      "training loss is:0.0005823599640280008\n",
      "training loss is:0.0005555334500968456\n",
      "training loss is:0.0005532141658477485\n",
      "Testing loss is:0.0005712172132916749\n",
      "epoch:57\n",
      "training loss is:0.0005130700301378965\n",
      "training loss is:0.0005834263283759356\n",
      "training loss is:0.0005402305396273732\n",
      "training loss is:0.00044160897959955037\n",
      "training loss is:0.0005000910023227334\n",
      "training loss is:0.0004747689818032086\n",
      "training loss is:0.0005322158103808761\n",
      "training loss is:0.00047953848843462765\n",
      "training loss is:0.0005171311204321682\n",
      "training loss is:0.000521425565239042\n",
      "training loss is:0.0005939693655818701\n",
      "training loss is:0.0005296425661072135\n",
      "training loss is:0.0005011578905396163\n",
      "training loss is:0.0005920964758843184\n",
      "training loss is:0.0005168896168470383\n",
      "training loss is:0.00041009392589330673\n",
      "training loss is:0.0004972920287400484\n",
      "training loss is:0.0005123814335092902\n",
      "training loss is:0.0005069923354312778\n",
      "training loss is:0.0005794520839117467\n",
      "Testing loss is:0.00047951837768778205\n",
      "epoch:58\n",
      "training loss is:0.00060566037427634\n",
      "training loss is:0.0004754181136377156\n",
      "training loss is:0.0004900238709524274\n",
      "training loss is:0.0004275346582289785\n",
      "training loss is:0.0005406400887295604\n",
      "training loss is:0.0005713982391171157\n",
      "training loss is:0.0005764942616224289\n",
      "training loss is:0.0005689985118806362\n",
      "training loss is:0.0005116799147799611\n",
      "training loss is:0.0006077095167711377\n",
      "training loss is:0.0006079531740397215\n",
      "training loss is:0.000508236582390964\n",
      "training loss is:0.00047758620348758996\n",
      "training loss is:0.0004941840888932347\n",
      "training loss is:0.00048042001435533166\n",
      "training loss is:0.00039764208486303687\n",
      "training loss is:0.0005604644538834691\n",
      "training loss is:0.0004637691890820861\n",
      "training loss is:0.00048125311150215566\n",
      "training loss is:0.0005453542107716203\n",
      "Testing loss is:0.00040493899723514915\n",
      "epoch:59\n",
      "training loss is:0.0005807103589177132\n",
      "training loss is:0.0003897075657732785\n",
      "training loss is:0.0005274056456983089\n",
      "training loss is:0.0005578647833317518\n",
      "training loss is:0.000513940816745162\n",
      "training loss is:0.0005286310333758593\n",
      "training loss is:0.0005210241070017219\n",
      "training loss is:0.00045774687896482646\n",
      "training loss is:0.0005597779527306557\n",
      "training loss is:0.0005058427341282368\n",
      "training loss is:0.0005008417647331953\n",
      "training loss is:0.0006116894073784351\n",
      "training loss is:0.0005143350572325289\n",
      "training loss is:0.0005546213360503316\n",
      "training loss is:0.0005453426856547594\n",
      "training loss is:0.000517257722094655\n",
      "training loss is:0.0005851827445439994\n",
      "training loss is:0.0005014154594391584\n",
      "training loss is:0.0005390791920945048\n",
      "training loss is:0.00048591429367661476\n",
      "Testing loss is:0.0004713214875664562\n",
      "epoch:60\n",
      "training loss is:0.0006223395466804504\n",
      "training loss is:0.0005290440749377012\n",
      "training loss is:0.0005260270554572344\n",
      "training loss is:0.0004456493188627064\n",
      "training loss is:0.0004929121350869536\n",
      "training loss is:0.0005702017806470394\n",
      "training loss is:0.0005395967164076865\n",
      "training loss is:0.000498552864883095\n",
      "training loss is:0.0005926800076849759\n",
      "training loss is:0.0005154858808964491\n",
      "training loss is:0.00039505408494733274\n",
      "training loss is:0.00042200571624562144\n",
      "training loss is:0.0005346934776753187\n",
      "training loss is:0.000581827131099999\n",
      "training loss is:0.0005736494204029441\n",
      "training loss is:0.000525643234141171\n",
      "training loss is:0.0005616602138616145\n",
      "training loss is:0.0005376490880735219\n",
      "training loss is:0.0005323674995452166\n",
      "training loss is:0.0004688288609031588\n",
      "Testing loss is:0.0005145894829183817\n",
      "epoch:61\n",
      "training loss is:0.0005159834399819374\n",
      "training loss is:0.0005316612659953535\n",
      "training loss is:0.0006399868871085346\n",
      "training loss is:0.0004939832142554224\n",
      "training loss is:0.0005415779887698591\n",
      "training loss is:0.0004286944749765098\n",
      "training loss is:0.0005159071879461408\n",
      "training loss is:0.0005526163149625063\n",
      "training loss is:0.0006168129038996994\n",
      "training loss is:0.0005277519812807441\n",
      "training loss is:0.0003984490758739412\n",
      "training loss is:0.00047523947432637215\n",
      "training loss is:0.000504364026710391\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from datasets import PartDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "from pic2points_model import pic2points\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from chamfer_distance import ChamferDistance\n",
    "from emd import EMDLoss\n",
    "\n",
    "dist =  EMDLoss()\n",
    "\n",
    "chamferDist = ChamferDistance()\n",
    "\n",
    "def main():\n",
    "    manualSeed = random.randint(1, 10000) # fix seed\n",
    "    print(\"Random Seed: \", manualSeed)\n",
    "    random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "\n",
    "    dataset = PartDataset(root = 'data/PartAnnotation/', pic2point = True, npoints = 2500)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=8)\n",
    "    print(\"number of training data:\"+ str(len(dataset)))\n",
    "    test_dataset = PartDataset(root = 'data/PartAnnotation/', pic2point = True, train = False, npoints = 2500)\n",
    "    testdataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16,shuffle=True, num_workers=8)\n",
    "    print(\"number of testing data:\"+ str(len(test_dataset)))\n",
    "\n",
    "    # creat model\n",
    "    print(\"model building...\")\n",
    "    model = pic2points(num_points=2500)\n",
    "    model.cuda()\n",
    "\n",
    "    # load pre-existing weights\n",
    "  #  if opt.model != '':\n",
    "   #     model.load_state_dict(torch.load(opt.model))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    num_batch = len(dataset) / 32\n",
    "\n",
    "    print('training mode ------------------')\n",
    "    for epoch in range(100):\n",
    "        print(\"epoch:\"+str(epoch))\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            im, points = data\n",
    "            im, points = Variable(im), Variable(points)\n",
    "            im, points = im.cuda(), points.cuda()\n",
    "            pred = model(im)\n",
    "            dist1, dist2 = chamferDist(pred, points)\n",
    "            loss = (torch.mean(dist1)) + (torch.mean(dist2))\n",
    "            emd_cost = torch.sum(dist(pred.cuda().double(), points.cuda().double()))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 50 is 0:\n",
    "                print(\"training loss is:\" + str(loss.item()))\n",
    "\n",
    "        loss_test = 0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            im_test, points_test = data\n",
    "            im_test, points_test = Variable(im_test), Variable(points_test)\n",
    "            im_test, points_test = im_test.cuda(), points_test.cuda()\n",
    "            pred_test = model(im_test)\n",
    "            dist1, dist2 = chamferDist(pred_test, points_test)\n",
    "            loss_test = (torch.mean(dist1)) + (torch.mean(dist2))\n",
    "            emd_test = torch.sum(dist(pred_test.cuda().double(), points_test.cuda().double()))\n",
    "        print(\"Testing loss is:\" + str(loss_test.item()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_cuda = cuda.device_count()\n",
    "    print(\"number of GPUs have been detected:\" + str(num_cuda))\n",
    "    #with torch.cuda.device(1):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
