{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "\n",
    "# url = 'https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_v0.zip'\n",
    "# urllib.request.urlretrieve(url, 'data.zip')\n",
    "# from zipfile import ZipFile\n",
    "# with ZipFile('data.zip', 'r') as zipObj:\n",
    "#    # Extract all the contents of zip file in different directory\n",
    "#    zipObj.extractall('data')\n",
    "    \n",
    "# url = 'https://github.com/chrdiller/pyTorchChamferDistance/archive/master.zip'\n",
    "# urllib.request.urlretrieve(url, 'chamfer.zip')\n",
    "# with ZipFile('chamfer.zip', 'r') as zipObj:\n",
    "#    # Extract all the contents of zip file in different directory\n",
    "#    zipObj.extractall('')\n",
    "    \n",
    "# url = 'https://github.com/meder411/PyTorch-EMDLoss/archive/master.zip'\n",
    "# urllib.request.urlretrieve(url, 'emd.zip')\n",
    "# with ZipFile('emd.zip', 'r') as zipObj:\n",
    "#    # Extract all the contents of zip file in different directory\n",
    "#    zipObj.extractall('')\n",
    "    \n",
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  3907\n",
      "model building...\n",
      "training mode ------------------\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from datasets import PartDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "from pic2points_model import pic2points\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from chamfer_distance import ChamferDistance\n",
    "from data_loader import XDataset, get_loader\n",
    "#from emd import EMDLoss\n",
    "\n",
    "#dist =  EMDLoss()\n",
    "\n",
    "chamferDist = ChamferDistance()\n",
    "\n",
    "# def main():\n",
    "manualSeed = random.randint(1, 10000) # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "#     dataset = PartDataset(root = 'data/PartAnnotation/', pic2point = True, npoints = 2500)\n",
    "#     dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=8)\n",
    "#     print(\"number of training data:\"+ str(len(dataset)))\n",
    "#     test_dataset = PartDataset(root = 'data/PartAnnotation/', pic2point = True, train = False, npoints = 2500)\n",
    "#     testdataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16,shuffle=True, num_workers=8)\n",
    "#     print(\"number of testing data:\"+ str(len(test_dataset)))\n",
    "\n",
    "\n",
    "# Decide on GPU or CPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_or_cpu = torch.device('cuda')\n",
    "else:\n",
    "    gpu_or_cpu = torch.device('cpu')\n",
    "\n",
    "# Training Configuration\n",
    "image_root = \"./../../../datasets/cs253-wi20-public/ShapeNetRendering/\"\n",
    "point_cloud_root = \"./../../../datasets/cs253-wi20-public/ShapeNet_pointclouds/\"\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "num_workers = 8\n",
    "use_2048 = True\n",
    "img_size = 227\n",
    "learning_rate = 1e-3\n",
    "num_points = 2500\n",
    "transform = transforms.Compose([transforms.Resize(img_size,interpolation=2),transforms.CenterCrop(img_size)])\n",
    "\n",
    "# Data loader\n",
    "data_loader = get_loader(image_root, point_cloud_root, use_2048, transform, batch_size, shuffle, num_workers)\n",
    "\n",
    "# create model\n",
    "print(\"model building...\")\n",
    "model = pic2points(num_points=num_points)\n",
    "model.to(device=gpu_or_cpu)\n",
    "\n",
    "# load pre-existing weights\n",
    "#  if opt.model != '':\n",
    "#     model.load_state_dict(torch.load(opt.model))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     num_batch = len(dataset) / batch_size\n",
    "\n",
    "print('training mode ------------------')\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     num_cuda = cuda.device_count()\n",
    "#     print(\"number of GPUs have been detected:\" + str(num_cuda))\n",
    "#     #with torch.cuda.device(1):\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262704"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "training loss, Batch 0: 0.006113636773079634\n",
      "training loss, Batch 50: 0.017141668125987053\n",
      "training loss, Batch 100: 0.006510607432574034\n",
      "training loss, Batch 150: 0.01705508679151535\n",
      "training loss, Batch 200: 0.006557357497513294\n",
      "training loss, Batch 250: 0.004852999933063984\n",
      "training loss, Batch 300: 0.005551069043576717\n",
      "training loss, Batch 350: 0.008703025057911873\n",
      "training loss, Batch 400: 0.007491051685065031\n",
      "training loss, Batch 450: 0.019877925515174866\n",
      "training loss, Batch 500: 0.007612662389874458\n",
      "training loss, Batch 550: 0.006605297327041626\n",
      "training loss, Batch 600: 0.010538516566157341\n",
      "training loss, Batch 650: 0.008715455420315266\n",
      "training loss, Batch 700: 0.004682359751313925\n",
      "training loss, Batch 750: 0.008188946172595024\n",
      "training loss, Batch 800: 0.004377533681690693\n",
      "training loss, Batch 850: 0.009123516269028187\n",
      "training loss, Batch 900: 0.00956349540501833\n",
      "training loss, Batch 950: 0.0069265482015907764\n",
      "training loss, Batch 1000: 0.005137745290994644\n",
      "training loss, Batch 1050: 0.00658192066475749\n",
      "training loss, Batch 1100: 0.009850118309259415\n",
      "training loss, Batch 1150: 0.008798538707196712\n",
      "training loss, Batch 1200: 0.005443422589451075\n",
      "training loss, Batch 1250: 0.005638746079057455\n",
      "training loss, Batch 1300: 0.011434223502874374\n",
      "training loss, Batch 1350: 0.013295709155499935\n",
      "training loss, Batch 1400: 0.00651758024469018\n",
      "training loss, Batch 1450: 0.007253765128552914\n",
      "training loss, Batch 1500: 0.005552120506763458\n",
      "training loss, Batch 1550: 0.008797455579042435\n",
      "training loss, Batch 1600: 0.023381078615784645\n",
      "training loss, Batch 1650: 0.00703800842165947\n",
      "training loss, Batch 1700: 0.005534281022846699\n",
      "training loss, Batch 1750: 0.006338379345834255\n",
      "training loss, Batch 1800: 0.00381529051810503\n",
      "training loss, Batch 1850: 0.004357824567705393\n",
      "training loss, Batch 1900: 0.0057957055978477\n",
      "training loss, Batch 1950: 0.00589604489505291\n",
      "training loss, Batch 2000: 0.004994835704565048\n",
      "training loss, Batch 2050: 0.005432139150798321\n",
      "training loss, Batch 2100: 0.0044152033515274525\n",
      "training loss, Batch 2150: 0.00824613869190216\n",
      "training loss, Batch 2200: 0.004494122229516506\n",
      "training loss, Batch 2250: 0.013656513765454292\n",
      "training loss, Batch 2300: 0.005317118018865585\n",
      "training loss, Batch 2350: 0.004057459998875856\n",
      "training loss, Batch 2400: 0.003462416585534811\n",
      "training loss, Batch 2450: 0.007679633796215057\n",
      "training loss, Batch 2500: 0.004807719029486179\n",
      "training loss, Batch 2550: 0.006001239642500877\n",
      "training loss, Batch 2600: 0.010080361738801003\n",
      "training loss, Batch 2650: 0.006738371215760708\n",
      "training loss, Batch 2700: 0.005942309275269508\n",
      "training loss, Batch 2750: 0.00731294322758913\n",
      "training loss, Batch 2800: 0.007257958874106407\n",
      "training loss, Batch 2850: 0.006589079275727272\n",
      "training loss, Batch 2900: 0.005474365781992674\n",
      "training loss, Batch 2950: 0.005090582184493542\n",
      "training loss, Batch 3000: 0.007045653183013201\n",
      "training loss, Batch 3050: 0.008165855892002583\n",
      "training loss, Batch 3100: 0.004209128208458424\n",
      "training loss, Batch 3150: 0.004886036738753319\n",
      "training loss, Batch 3200: 0.0049694306217134\n",
      "training loss, Batch 3250: 0.004252654500305653\n",
      "training loss, Batch 3300: 0.011579609476029873\n",
      "training loss, Batch 3350: 0.0045287697575986385\n",
      "training loss, Batch 3400: 0.0030494886450469494\n",
      "training loss, Batch 3450: 0.003804902546107769\n",
      "training loss, Batch 3500: 0.0030975881963968277\n",
      "training loss, Batch 3550: 0.0038315271958708763\n",
      "training loss, Batch 3600: 0.00499225128442049\n",
      "training loss, Batch 3650: 0.0061447499319911\n",
      "training loss, Batch 3700: 0.013513758778572083\n",
      "training loss, Batch 3750: 0.004528110846877098\n",
      "training loss, Batch 3800: 0.007156883832067251\n",
      "training loss, Batch 3850: 0.007054178509861231\n",
      "training loss, Batch 3900: 0.007546048145741224\n",
      "training loss, Batch 3950: 0.006046428345143795\n",
      "training loss, Batch 4000: 0.0038159615360200405\n",
      "training loss, Batch 4050: 0.013633876107633114\n",
      "training loss, Batch 4100: 0.0050680688582360744\n",
      "training loss, Batch 4150: 0.005969482474029064\n",
      "training loss, Batch 4200: 0.003994292579591274\n",
      "training loss, Batch 4250: 0.0037199687212705612\n",
      "training loss, Batch 4300: 0.008738363161683083\n",
      "training loss, Batch 4350: 0.004747673869132996\n",
      "training loss, Batch 4400: 0.004915319383144379\n",
      "training loss, Batch 4450: 0.004267770331352949\n",
      "training loss, Batch 4500: 0.006084039807319641\n",
      "training loss, Batch 4550: 0.008830022066831589\n",
      "training loss, Batch 4600: 0.007240531966090202\n",
      "training loss, Batch 4650: 0.0059019895270466805\n",
      "training loss, Batch 4700: 0.005446997936815023\n",
      "training loss, Batch 4750: 0.004322489723563194\n",
      "training loss, Batch 4800: 0.008838792331516743\n",
      "training loss, Batch 4850: 0.004696749150753021\n",
      "training loss, Batch 4900: 0.004219360649585724\n",
      "training loss, Batch 4950: 0.0044486187398433685\n",
      "training loss, Batch 5000: 0.011175893247127533\n",
      "training loss, Batch 5050: 0.009520786814391613\n",
      "training loss, Batch 5100: 0.005819222889840603\n",
      "training loss, Batch 5150: 0.006041397340595722\n",
      "training loss, Batch 5200: 0.00535194855183363\n",
      "training loss, Batch 5250: 0.0036742035299539566\n",
      "training loss, Batch 5300: 0.004500551614910364\n",
      "training loss, Batch 5350: 0.005191249772906303\n",
      "training loss, Batch 5400: 0.00469566136598587\n",
      "training loss, Batch 5450: 0.0065796710550785065\n",
      "training loss, Batch 5500: 0.0041771698743104935\n",
      "training loss, Batch 5550: 0.003673672443255782\n",
      "training loss, Batch 5600: 0.009612951427698135\n",
      "training loss, Batch 5650: 0.009519179351627827\n",
      "training loss, Batch 5700: 0.0076560284942388535\n",
      "training loss, Batch 5750: 0.008104156702756882\n",
      "training loss, Batch 5800: 0.005067714024335146\n",
      "training loss, Batch 5850: 0.0035763795021921396\n",
      "training loss, Batch 5900: 0.004495571833103895\n",
      "training loss, Batch 5950: 0.0036110468208789825\n",
      "training loss, Batch 6000: 0.00458230497315526\n",
      "training loss, Batch 6050: 0.005329409148544073\n",
      "training loss, Batch 6100: 0.006454039830714464\n",
      "training loss, Batch 6150: 0.0042608678340911865\n",
      "training loss, Batch 6200: 0.00779271824285388\n",
      "training loss, Batch 6250: 0.005180994980037212\n",
      "training loss, Batch 6300: 0.004292816389352083\n",
      "training loss, Batch 6350: 0.00381622021086514\n",
      "training loss, Batch 6400: 0.0066580139100551605\n",
      "training loss, Batch 6450: 0.006025355309247971\n",
      "training loss, Batch 6500: 0.004749865736812353\n",
      "training loss, Batch 6550: 0.006375826895236969\n",
      "training loss, Batch 6600: 0.003383741481229663\n",
      "training loss, Batch 6650: 0.005048797465860844\n",
      "training loss, Batch 6700: 0.004077736753970385\n",
      "training loss, Batch 6750: 0.005241259932518005\n",
      "training loss, Batch 6800: 0.006653342861682177\n",
      "training loss, Batch 6850: 0.006423069164156914\n",
      "training loss, Batch 6900: 0.00704193115234375\n",
      "training loss, Batch 6950: 0.006249980069696903\n",
      "training loss, Batch 7000: 0.007053812500089407\n",
      "training loss, Batch 7050: 0.008741097524762154\n",
      "training loss, Batch 7100: 0.0037822723388671875\n",
      "training loss, Batch 7150: 0.008448398672044277\n",
      "training loss, Batch 7200: 0.003401112975552678\n",
      "training loss, Batch 7250: 0.004117710515856743\n",
      "training loss, Batch 7300: 0.005080471280962229\n",
      "training loss, Batch 7350: 0.005135506857186556\n",
      "training loss, Batch 7400: 0.0042750220745801926\n",
      "training loss, Batch 7450: 0.009337973780930042\n",
      "training loss, Batch 7500: 0.014124797657132149\n",
      "training loss, Batch 7550: 0.0056607043370604515\n",
      "training loss, Batch 7600: 0.003921621013432741\n",
      "training loss, Batch 7650: 0.004878222476691008\n",
      "training loss, Batch 7700: 0.009149346500635147\n",
      "training loss, Batch 7750: 0.0036506918258965015\n",
      "training loss, Batch 7800: 0.007202894426882267\n",
      "training loss, Batch 7850: 0.0054681794717907906\n",
      "training loss, Batch 7900: 0.004833368584513664\n",
      "training loss, Batch 7950: 0.00352828623726964\n",
      "training loss, Batch 8000: 0.004586410708725452\n",
      "training loss, Batch 8050: 0.003548775101080537\n",
      "training loss, Batch 8100: 0.0022080924827605486\n",
      "training loss, Batch 8150: 0.00298089487478137\n",
      "training loss, Batch 8200: 0.005165251903235912\n",
      "training loss, Batch 8250: 0.00353622785769403\n",
      "training loss, Batch 8300: 0.004485543817281723\n",
      "training loss, Batch 8350: 0.008481479249894619\n",
      "training loss, Batch 8400: 0.0071830544620752335\n",
      "training loss, Batch 8450: 0.005851115100085735\n",
      "training loss, Batch 8500: 0.006990705616772175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 8550: 0.004610544536262751\n",
      "training loss, Batch 8600: 0.00423802062869072\n",
      "training loss, Batch 8650: 0.006742898840457201\n",
      "training loss, Batch 8700: 0.004402045160531998\n",
      "training loss, Batch 8750: 0.005068531259894371\n",
      "training loss, Batch 8800: 0.0030988899525254965\n",
      "training loss, Batch 8850: 0.009968934580683708\n",
      "training loss, Batch 8900: 0.0034082215279340744\n",
      "training loss, Batch 8950: 0.006077205762267113\n",
      "training loss, Batch 9000: 0.00435540871694684\n",
      "training loss, Batch 9050: 0.0030963404569774866\n",
      "training loss, Batch 9100: 0.004657597281038761\n",
      "training loss, Batch 9150: 0.019309328868985176\n",
      "training loss, Batch 9200: 0.004748796112835407\n",
      "training loss, Batch 9250: 0.007040685974061489\n",
      "training loss, Batch 9300: 0.006272414233535528\n",
      "training loss, Batch 9350: 0.002766710240393877\n",
      "training loss, Batch 9400: 0.005343594588339329\n",
      "training loss, Batch 9450: 0.0037859634030610323\n",
      "training loss, Batch 9500: 0.011338794603943825\n",
      "training loss, Batch 9550: 0.003713092068210244\n",
      "training loss, Batch 9600: 0.004717408679425716\n",
      "training loss, Batch 9650: 0.007968141697347164\n",
      "training loss, Batch 9700: 0.008654824458062649\n",
      "training loss, Batch 9750: 0.008762985467910767\n",
      "training loss, Batch 9800: 0.006640749983489513\n",
      "training loss, Batch 9850: 0.015801602974534035\n",
      "training loss, Batch 9900: 0.00449009845033288\n",
      "training loss, Batch 9950: 0.008779574185609818\n",
      "training loss, Batch 10000: 0.0037186499685049057\n",
      "training loss, Batch 10050: 0.005250202026218176\n",
      "training loss, Batch 10100: 0.005725710187107325\n",
      "training loss, Batch 10150: 0.004087801557034254\n",
      "training loss, Batch 10200: 0.0034006531350314617\n",
      "training loss, Batch 10250: 0.00931805744767189\n",
      "training loss, Batch 10300: 0.007877794094383717\n",
      "training loss, Batch 10350: 0.0032214734237641096\n",
      "training loss, Batch 10400: 0.003955367486923933\n",
      "training loss, Batch 10450: 0.0034628920257091522\n",
      "training loss, Batch 10500: 0.004459829069674015\n",
      "training loss, Batch 10550: 0.006329202093183994\n",
      "training loss, Batch 10600: 0.004460368771106005\n",
      "training loss, Batch 10650: 0.003422331064939499\n",
      "training loss, Batch 10700: 0.008291327394545078\n",
      "training loss, Batch 10750: 0.003941422328352928\n",
      "training loss, Batch 10800: 0.013201430439949036\n",
      "training loss, Batch 10850: 0.0037102114874869585\n",
      "training loss, Batch 10900: 0.006023898255079985\n",
      "training loss, Batch 10950: 0.0030838290695101023\n",
      "training loss, Batch 11000: 0.0017265694914385676\n",
      "training loss, Batch 11050: 0.005314205773174763\n",
      "training loss, Batch 11100: 0.006125868763774633\n",
      "training loss, Batch 11150: 0.005742974579334259\n",
      "training loss, Batch 11200: 0.005964751821011305\n",
      "training loss, Batch 11250: 0.00923363771289587\n",
      "training loss, Batch 11300: 0.003698532935231924\n",
      "training loss, Batch 11350: 0.004082401283085346\n",
      "training loss, Batch 11400: 0.006431040354073048\n",
      "training loss, Batch 11450: 0.0051683085039258\n",
      "training loss, Batch 11500: 0.004534732550382614\n",
      "training loss, Batch 11550: 0.015405543148517609\n",
      "training loss, Batch 11600: 0.0041356636211276054\n",
      "training loss, Batch 11650: 0.015503162518143654\n",
      "training loss, Batch 11700: 0.007201209664344788\n",
      "training loss, Batch 11750: 0.004326416179537773\n",
      "training loss, Batch 11800: 0.005112841259688139\n",
      "training loss, Batch 11850: 0.006953613832592964\n",
      "training loss, Batch 11900: 0.011172656901180744\n",
      "training loss, Batch 11950: 0.006638295017182827\n",
      "training loss, Batch 12000: 0.005370613653212786\n",
      "training loss, Batch 12050: 0.013469927944242954\n",
      "training loss, Batch 12100: 0.004250021185725927\n",
      "training loss, Batch 12150: 0.004148927517235279\n",
      "training loss, Batch 12200: 0.002368431305512786\n",
      "training loss, Batch 12250: 0.004619027487933636\n",
      "training loss, Batch 12300: 0.008612706325948238\n",
      "training loss, Batch 12350: 0.018498580902814865\n",
      "training loss, Batch 12400: 0.00886537041515112\n",
      "training loss, Batch 12450: 0.004520053509622812\n",
      "training loss, Batch 12500: 0.00401188712567091\n",
      "training loss, Batch 12550: 0.006182405631989241\n",
      "training loss, Batch 12600: 0.008300388231873512\n",
      "training loss, Batch 12650: 0.0045740073546767235\n",
      "training loss, Batch 12700: 0.006664857733994722\n",
      "training loss, Batch 12750: 0.004256780259311199\n",
      "training loss, Batch 12800: 0.0021822897251695395\n",
      "training loss, Batch 12850: 0.00414799340069294\n",
      "training loss, Batch 12900: 0.005890025291591883\n",
      "training loss, Batch 12950: 0.005276555195450783\n",
      "training loss, Batch 13000: 0.003625719342380762\n",
      "training loss, Batch 13050: 0.0076637063175439835\n",
      "training loss, Batch 13100: 0.005154094193130732\n",
      "training loss, Batch 13150: 0.00919158011674881\n",
      "training loss, Batch 13200: 0.0052840872667729855\n",
      "training loss, Batch 13250: 0.0035744463093578815\n",
      "training loss, Batch 13300: 0.0032710274681448936\n",
      "training loss, Batch 13350: 0.00352343637496233\n",
      "training loss, Batch 13400: 0.010982254520058632\n",
      "training loss, Batch 13450: 0.005638604983687401\n",
      "training loss, Batch 13500: 0.005610913969576359\n",
      "training loss, Batch 13550: 0.004381060600280762\n",
      "training loss, Batch 13600: 0.011606690473854542\n",
      "training loss, Batch 13650: 0.0046040709130465984\n",
      "training loss, Batch 13700: 0.0036155576817691326\n",
      "training loss, Batch 13750: 0.004165073856711388\n",
      "training loss, Batch 13800: 0.005497097969055176\n",
      "training loss, Batch 13850: 0.004630363546311855\n",
      "training loss, Batch 13900: 0.007953416556119919\n",
      "training loss, Batch 13950: 0.006999061442911625\n",
      "training loss, Batch 14000: 0.005696948617696762\n",
      "training loss, Batch 14050: 0.010530185885727406\n",
      "training loss, Batch 14100: 0.003552943468093872\n",
      "training loss, Batch 14150: 0.005760712083429098\n",
      "training loss, Batch 14200: 0.003611138556152582\n",
      "training loss, Batch 14250: 0.0025220667012035847\n",
      "training loss, Batch 14300: 0.004348261281847954\n",
      "training loss, Batch 14350: 0.0023924766574054956\n",
      "training loss, Batch 14400: 0.0031000555027276278\n",
      "training loss, Batch 14450: 0.003340918105095625\n",
      "training loss, Batch 14500: 0.00690738670527935\n",
      "training loss, Batch 14550: 0.00587761215865612\n",
      "training loss, Batch 14600: 0.00757251912727952\n",
      "training loss, Batch 14650: 0.00612945482134819\n",
      "training loss, Batch 14700: 0.00429300032556057\n",
      "training loss, Batch 14750: 0.005282144993543625\n",
      "training loss, Batch 14800: 0.013337700627744198\n",
      "training loss, Batch 14850: 0.007755915634334087\n",
      "training loss, Batch 14900: 0.004040812142193317\n",
      "training loss, Batch 14950: 0.0048127975314855576\n",
      "training loss, Batch 15000: 0.005026539787650108\n",
      "training loss, Batch 15050: 0.003720628097653389\n",
      "training loss, Batch 15100: 0.0028880489990115166\n",
      "training loss, Batch 15150: 0.005574502982199192\n",
      "training loss, Batch 15200: 0.004539468325674534\n",
      "training loss, Batch 15250: 0.002995643764734268\n",
      "training loss, Batch 15300: 0.005208040587604046\n",
      "training loss, Batch 15350: 0.006558079272508621\n",
      "training loss, Batch 15400: 0.016697298735380173\n",
      "training loss, Batch 15450: 0.004072726704180241\n",
      "training loss, Batch 15500: 0.0034120893105864525\n",
      "training loss, Batch 15550: 0.004236094653606415\n",
      "training loss, Batch 15600: 0.005138087552040815\n",
      "training loss, Batch 15650: 0.004127924330532551\n",
      "training loss, Batch 15700: 0.008966194465756416\n",
      "training loss, Batch 15750: 0.008692778646945953\n",
      "training loss, Batch 15800: 0.005874237976968288\n",
      "training loss, Batch 15850: 0.0031991044525057077\n",
      "training loss, Batch 15900: 0.004667515866458416\n",
      "training loss, Batch 15950: 0.005392382852733135\n",
      "training loss, Batch 16000: 0.007384699769318104\n",
      "training loss, Batch 16050: 0.005784326698631048\n",
      "training loss, Batch 16100: 0.00370787363499403\n",
      "training loss, Batch 16150: 0.0041384007781744\n",
      "training loss, Batch 16200: 0.0047979336231946945\n",
      "training loss, Batch 16250: 0.005921464879065752\n",
      "training loss, Batch 16300: 0.004028935916721821\n",
      "training loss, Batch 16350: 0.0039020469412207603\n",
      "training loss, Batch 16400: 0.008212396875023842\n",
      "training loss, Batch 16450: 0.007932402193546295\n",
      "training loss, Batch 16500: 0.004438744857907295\n",
      "training loss, Batch 16550: 0.005075075663626194\n",
      "training loss, Batch 16600: 0.009341118857264519\n",
      "training loss, Batch 16650: 0.007088866084814072\n",
      "training loss, Batch 16700: 0.009922295808792114\n",
      "training loss, Batch 16750: 0.005277104675769806\n",
      "training loss, Batch 16800: 0.005774000193923712\n",
      "training loss, Batch 16850: 0.003725765272974968\n",
      "training loss, Batch 16900: 0.005236364901065826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 16950: 0.0034919166937470436\n",
      "training loss, Batch 17000: 0.005305989645421505\n",
      "training loss, Batch 17050: 0.005321683827787638\n",
      "training loss, Batch 17100: 0.005682835821062326\n",
      "training loss, Batch 17150: 0.0056601837277412415\n",
      "training loss, Batch 17200: 0.002602561842650175\n",
      "training loss, Batch 17250: 0.005928915925323963\n",
      "training loss, Batch 17300: 0.005809373687952757\n",
      "training loss, Batch 17350: 0.0024105471093207598\n",
      "training loss, Batch 17400: 0.009708695113658905\n",
      "training loss, Batch 17450: 0.005236022174358368\n",
      "training loss, Batch 17500: 0.0044760024175047874\n",
      "training loss, Batch 17550: 0.003495407523587346\n",
      "training loss, Batch 17600: 0.006960595957934856\n",
      "training loss, Batch 17650: 0.009940465912222862\n",
      "training loss, Batch 17700: 0.0034862940665334463\n",
      "training loss, Batch 17750: 0.005680235102772713\n",
      "training loss, Batch 17800: 0.004447353072464466\n",
      "training loss, Batch 17850: 0.0062599778175354\n",
      "training loss, Batch 17900: 0.010235847905278206\n",
      "training loss, Batch 17950: 0.003950975369662046\n",
      "training loss, Batch 18000: 0.002097821794450283\n",
      "training loss, Batch 18050: 0.005624591372907162\n",
      "training loss, Batch 18100: 0.002552421297878027\n",
      "training loss, Batch 18150: 0.004023349843919277\n",
      "training loss, Batch 18200: 0.004225591663271189\n",
      "training loss, Batch 18250: 0.004798282403498888\n",
      "training loss, Batch 18300: 0.005595440976321697\n",
      "training loss, Batch 18350: 0.00260570808313787\n",
      "training loss, Batch 18400: 0.0062056370079517365\n",
      "training loss, Batch 18450: 0.008444888517260551\n",
      "training loss, Batch 18500: 0.003427354618906975\n",
      "training loss, Batch 18550: 0.005197753664106131\n",
      "training loss, Batch 18600: 0.0037114867009222507\n",
      "training loss, Batch 18650: 0.009385162964463234\n",
      "training loss, Batch 18700: 0.002881408901885152\n",
      "training loss, Batch 18750: 0.004329230170696974\n",
      "training loss, Batch 18800: 0.008032985962927341\n",
      "training loss, Batch 18850: 0.007022421807050705\n",
      "training loss, Batch 18900: 0.004092385061085224\n",
      "training loss, Batch 18950: 0.005217349156737328\n",
      "training loss, Batch 19000: 0.003843570128083229\n",
      "training loss, Batch 19050: 0.0080549456179142\n",
      "training loss, Batch 19100: 0.005202528089284897\n",
      "training loss, Batch 19150: 0.004406215623021126\n",
      "training loss, Batch 19200: 0.013377854600548744\n",
      "training loss, Batch 19250: 0.00421158317476511\n",
      "training loss, Batch 19300: 0.007238683290779591\n",
      "training loss, Batch 19350: 0.0062956735491752625\n",
      "training loss, Batch 19400: 0.0023800821509212255\n",
      "training loss, Batch 19450: 0.002100171521306038\n",
      "training loss, Batch 19500: 0.0035162202548235655\n",
      "training loss, Batch 19550: 0.005563761107623577\n",
      "training loss, Batch 19600: 0.0037706319708377123\n",
      "training loss, Batch 19650: 0.004969967063516378\n",
      "training loss, Batch 19700: 0.00404639495536685\n",
      "training loss, Batch 19750: 0.002492548432201147\n",
      "training loss, Batch 19800: 0.006814778316766024\n",
      "training loss, Batch 19850: 0.003531374968588352\n",
      "training loss, Batch 19900: 0.010082525201141834\n",
      "training loss, Batch 19950: 0.00628024572506547\n",
      "training loss, Batch 20000: 0.004467995371669531\n",
      "training loss, Batch 20050: 0.005464727990329266\n",
      "training loss, Batch 20100: 0.0058195507153868675\n",
      "training loss, Batch 20150: 0.005303109064698219\n",
      "training loss, Batch 20200: 0.0017600648570805788\n",
      "training loss, Batch 20250: 0.004752129781991243\n",
      "training loss, Batch 20300: 0.0035510496236383915\n",
      "training loss, Batch 20350: 0.011292741633951664\n",
      "training loss, Batch 20400: 0.006688697263598442\n",
      "training loss, Batch 20450: 0.004212808329612017\n",
      "training loss, Batch 20500: 0.004315171856433153\n",
      "training loss, Batch 20550: 0.002570865908637643\n",
      "training loss, Batch 20600: 0.0030009569600224495\n",
      "training loss, Batch 20650: 0.004886927083134651\n",
      "training loss, Batch 20700: 0.0035154628567397594\n",
      "training loss, Batch 20750: 0.006394841708242893\n",
      "training loss, Batch 20800: 0.006329577416181564\n",
      "training loss, Batch 20850: 0.00458627101033926\n",
      "training loss, Batch 20900: 0.007440952584147453\n",
      "training loss, Batch 20950: 0.006267879158258438\n",
      "training loss, Batch 21000: 0.0015400410629808903\n",
      "training loss, Batch 21050: 0.004675618838518858\n",
      "training loss, Batch 21100: 0.0038376764860004187\n",
      "training loss, Batch 21150: 0.005042222328484058\n",
      "training loss, Batch 21200: 0.009436013177037239\n",
      "training loss, Batch 21250: 0.004134263843297958\n",
      "training loss, Batch 21300: 0.0064790695905685425\n",
      "training loss, Batch 21350: 0.004877245984971523\n",
      "training loss, Batch 21400: 0.00651159742847085\n",
      "training loss, Batch 21450: 0.006977538578212261\n",
      "training loss, Batch 21500: 0.004370992071926594\n",
      "training loss, Batch 21550: 0.00967397727072239\n",
      "training loss, Batch 21600: 0.0020248605869710445\n",
      "training loss, Batch 21650: 0.0036527984775602818\n",
      "training loss, Batch 21700: 0.0024011507630348206\n",
      "training loss, Batch 21750: 0.0035996881779283285\n",
      "training loss, Batch 21800: 0.007551609538495541\n",
      "training loss, Batch 21850: 0.0031921276822686195\n",
      "training loss, Batch 21900: 0.003610016778111458\n",
      "training loss, Batch 21950: 0.0024506039917469025\n",
      "training loss, Batch 22000: 0.004673767369240522\n",
      "training loss, Batch 22050: 0.005895128473639488\n",
      "training loss, Batch 22100: 0.003042384050786495\n",
      "training loss, Batch 22150: 0.004632899537682533\n",
      "training loss, Batch 22200: 0.00483553484082222\n",
      "training loss, Batch 22250: 0.0026154592633247375\n",
      "training loss, Batch 22300: 0.002817826811224222\n",
      "training loss, Batch 22350: 0.0029797209426760674\n",
      "training loss, Batch 22400: 0.004131479188799858\n",
      "training loss, Batch 22450: 0.0060428231954574585\n",
      "training loss, Batch 22500: 0.010565211065113544\n",
      "training loss, Batch 22550: 0.005154338665306568\n",
      "training loss, Batch 22600: 0.0033196790609508753\n",
      "training loss, Batch 22650: 0.0033314365427941084\n",
      "training loss, Batch 22700: 0.003047318197786808\n",
      "training loss, Batch 22750: 0.003009526990354061\n",
      "training loss, Batch 22800: 0.005146860610693693\n",
      "training loss, Batch 22850: 0.00374606903642416\n",
      "training loss, Batch 22900: 0.006566853262484074\n",
      "training loss, Batch 22950: 0.0071076806634664536\n",
      "training loss, Batch 23000: 0.006271777208894491\n",
      "training loss, Batch 23050: 0.004767339676618576\n",
      "training loss, Batch 23100: 0.012300880625844002\n",
      "training loss, Batch 23150: 0.005348307080566883\n",
      "training loss, Batch 23200: 0.006564343348145485\n",
      "training loss, Batch 23250: 0.007089409977197647\n",
      "training loss, Batch 23300: 0.008445866405963898\n",
      "training loss, Batch 23350: 0.0024874655064195395\n",
      "training loss, Batch 23400: 0.003702745772898197\n",
      "training loss, Batch 23450: 0.00319703109562397\n",
      "training loss, Batch 23500: 0.004831022582948208\n",
      "training loss, Batch 23550: 0.0047064791433513165\n",
      "training loss, Batch 23600: 0.004518808331340551\n",
      "training loss, Batch 23650: 0.0035096020437777042\n",
      "training loss, Batch 23700: 0.0031079896725714207\n",
      "training loss, Batch 23750: 0.0052405837923288345\n",
      "training loss, Batch 23800: 0.0027097787242382765\n",
      "training loss, Batch 23850: 0.006406175903975964\n",
      "training loss, Batch 23900: 0.004924902226775885\n",
      "training loss, Batch 23950: 0.006485311314463615\n",
      "training loss, Batch 24000: 0.014138410799205303\n",
      "training loss, Batch 24050: 0.004622048698365688\n",
      "training loss, Batch 24100: 0.0064636473543941975\n",
      "training loss, Batch 24150: 0.004459419287741184\n",
      "training loss, Batch 24200: 0.005204327404499054\n",
      "training loss, Batch 24250: 0.0063980212435126305\n",
      "training loss, Batch 24300: 0.003811236936599016\n",
      "training loss, Batch 24350: 0.005766420625150204\n",
      "training loss, Batch 24400: 0.008550716564059258\n",
      "training loss, Batch 24450: 0.010711018927395344\n",
      "training loss, Batch 24500: 0.005442898720502853\n",
      "training loss, Batch 24550: 0.003674678970128298\n",
      "training loss, Batch 24600: 0.005020863842219114\n",
      "training loss, Batch 24650: 0.005255437456071377\n",
      "training loss, Batch 24700: 0.009505752474069595\n",
      "training loss, Batch 24750: 0.0036274725571274757\n",
      "training loss, Batch 24800: 0.003237594850361347\n",
      "training loss, Batch 24850: 0.008373033255338669\n",
      "training loss, Batch 24900: 0.003187075722962618\n",
      "training loss, Batch 24950: 0.006981067359447479\n",
      "training loss, Batch 25000: 0.002428513253107667\n",
      "training loss, Batch 25050: 0.0033935774117708206\n",
      "training loss, Batch 25100: 0.011897093616425991\n",
      "training loss, Batch 25150: 0.0037058922462165356\n",
      "training loss, Batch 25200: 0.009522689506411552\n",
      "training loss, Batch 25250: 0.007506317459046841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 25300: 0.00604806374758482\n",
      "training loss, Batch 25350: 0.003679070621728897\n",
      "training loss, Batch 25400: 0.005833845119923353\n",
      "training loss, Batch 25450: 0.0037771929055452347\n",
      "training loss, Batch 25500: 0.007679418660700321\n",
      "training loss, Batch 25550: 0.005182782653719187\n",
      "training loss, Batch 25600: 0.002079431200399995\n",
      "training loss, Batch 25650: 0.003146043512970209\n",
      "training loss, Batch 25700: 0.006759571842849255\n",
      "training loss, Batch 25750: 0.0049604088999331\n",
      "training loss, Batch 25800: 0.004399582743644714\n",
      "training loss, Batch 25850: 0.00351519463583827\n",
      "training loss, Batch 25900: 0.0054592955857515335\n",
      "training loss, Batch 25950: 0.0033131628297269344\n",
      "training loss, Batch 26000: 0.003932913765311241\n",
      "training loss, Batch 26050: 0.004677544813603163\n",
      "training loss, Batch 26100: 0.004942447412759066\n",
      "training loss, Batch 26150: 0.0074350303038954735\n",
      "training loss, Batch 26200: 0.0032895677722990513\n",
      "training loss, Batch 26250: 0.0018793377093970776\n",
      "training loss, Batch 26300: 0.003989985212683678\n",
      "training loss, Batch 26350: 0.004720911383628845\n",
      "training loss, Batch 26400: 0.004599481821060181\n",
      "training loss, Batch 26450: 0.00302337808534503\n",
      "training loss, Batch 26500: 0.008487614803016186\n",
      "training loss, Batch 26550: 0.005397601518779993\n",
      "training loss, Batch 26600: 0.0033287156838923693\n",
      "training loss, Batch 26650: 0.0033606779761612415\n",
      "training loss, Batch 26700: 0.004028709139674902\n",
      "training loss, Batch 26750: 0.008415775373578072\n",
      "training loss, Batch 26800: 0.003866013139486313\n",
      "training loss, Batch 26850: 0.002692224457859993\n",
      "training loss, Batch 26900: 0.01562419068068266\n",
      "training loss, Batch 26950: 0.005141690373420715\n",
      "training loss, Batch 27000: 0.0033560022711753845\n",
      "training loss, Batch 27050: 0.003308630548417568\n",
      "training loss, Batch 27100: 0.003979324828833342\n",
      "training loss, Batch 27150: 0.0030185612849891186\n",
      "training loss, Batch 27200: 0.004900144413113594\n",
      "training loss, Batch 27250: 0.0030645716469734907\n",
      "training loss, Batch 27300: 0.006952144671231508\n",
      "training loss, Batch 27350: 0.003137654857710004\n",
      "training loss, Batch 27400: 0.004973685368895531\n",
      "training loss, Batch 27450: 0.008448438718914986\n",
      "training loss, Batch 27500: 0.0032717501744627953\n",
      "training loss, Batch 27550: 0.0037420345470309258\n",
      "training loss, Batch 27600: 0.0027113324031233788\n",
      "training loss, Batch 27650: 0.002979400334879756\n",
      "training loss, Batch 27700: 0.004999274853616953\n",
      "training loss, Batch 27750: 0.011162662878632545\n",
      "training loss, Batch 27800: 0.00308782234787941\n",
      "training loss, Batch 27850: 0.005506521090865135\n",
      "training loss, Batch 27900: 0.006648260168731213\n",
      "training loss, Batch 27950: 0.005561728961765766\n",
      "training loss, Batch 28000: 0.006134306080639362\n",
      "training loss, Batch 28050: 0.004056878853589296\n",
      "training loss, Batch 28100: 0.002451864071190357\n",
      "training loss, Batch 28150: 0.004188910126686096\n",
      "training loss, Batch 28200: 0.0029262048192322254\n",
      "training loss, Batch 28250: 0.001803071005269885\n",
      "training loss, Batch 28300: 0.0024312310852110386\n",
      "training loss, Batch 28350: 0.004334283526986837\n",
      "training loss, Batch 28400: 0.0034313693176954985\n",
      "training loss, Batch 28450: 0.004277893807739019\n",
      "training loss, Batch 28500: 0.0033771777525544167\n",
      "training loss, Batch 28550: 0.005876727402210236\n",
      "training loss, Batch 28600: 0.0022385520860552788\n",
      "training loss, Batch 28650: 0.006334085948765278\n",
      "training loss, Batch 28700: 0.0024406956508755684\n",
      "training loss, Batch 28750: 0.001936275977641344\n",
      "training loss, Batch 28800: 0.004097477067261934\n",
      "training loss, Batch 28850: 0.002839749213308096\n",
      "training loss, Batch 28900: 0.006879827007651329\n",
      "training loss, Batch 28950: 0.003753534983843565\n",
      "training loss, Batch 29000: 0.002894660457968712\n",
      "training loss, Batch 29050: 0.0026590898633003235\n",
      "training loss, Batch 29100: 0.0021700819488614798\n",
      "training loss, Batch 29150: 0.016497230157256126\n",
      "training loss, Batch 29200: 0.00741864088922739\n",
      "training loss, Batch 29250: 0.005397417116910219\n",
      "training loss, Batch 29300: 0.005437848158180714\n",
      "training loss, Batch 29350: 0.004519080743193626\n",
      "training loss, Batch 29400: 0.004381445236504078\n",
      "training loss, Batch 29450: 0.003038397990167141\n",
      "training loss, Batch 29500: 0.007344279438257217\n",
      "training loss, Batch 29550: 0.006970938295125961\n",
      "training loss, Batch 29600: 0.006362625863403082\n",
      "training loss, Batch 29650: 0.007558726239949465\n",
      "training loss, Batch 29700: 0.0038819569163024426\n",
      "training loss, Batch 29750: 0.0043923379853367805\n",
      "training loss, Batch 29800: 0.0029625864699482918\n",
      "training loss, Batch 29850: 0.007348822429776192\n",
      "training loss, Batch 29900: 0.00788445770740509\n",
      "training loss, Batch 29950: 0.0032059084624052048\n",
      "training loss, Batch 30000: 0.003744030836969614\n",
      "training loss, Batch 30050: 0.00506957434117794\n",
      "training loss, Batch 30100: 0.0025854967534542084\n",
      "training loss, Batch 30150: 0.0025313114747405052\n",
      "training loss, Batch 30200: 0.004317007958889008\n",
      "training loss, Batch 30250: 0.004075909964740276\n",
      "training loss, Batch 30300: 0.004849706310778856\n",
      "training loss, Batch 30350: 0.005622014869004488\n",
      "training loss, Batch 30400: 0.00444648414850235\n",
      "training loss, Batch 30450: 0.003112281206995249\n",
      "training loss, Batch 30500: 0.005404609255492687\n",
      "training loss, Batch 30550: 0.0061936331912875175\n",
      "training loss, Batch 30600: 0.011618643999099731\n",
      "training loss, Batch 30650: 0.010851245373487473\n",
      "training loss, Batch 30700: 0.0037210402078926563\n",
      "training loss, Batch 30750: 0.003636788809671998\n",
      "training loss, Batch 30800: 0.009031811729073524\n",
      "training loss, Batch 30850: 0.0037779880221933126\n",
      "training loss, Batch 30900: 0.0031177219934761524\n",
      "training loss, Batch 30950: 0.002954063704237342\n",
      "training loss, Batch 31000: 0.004230454098433256\n",
      "training loss, Batch 31050: 0.0038803620263934135\n",
      "training loss, Batch 31100: 0.0033386594150215387\n",
      "training loss, Batch 31150: 0.010096419602632523\n",
      "training loss, Batch 31200: 0.003970431163907051\n",
      "training loss, Batch 31250: 0.007241849321871996\n",
      "training loss, Batch 31300: 0.005942435935139656\n",
      "training loss, Batch 31350: 0.005536501295864582\n",
      "training loss, Batch 31400: 0.005479920189827681\n",
      "training loss, Batch 31450: 0.00868513435125351\n",
      "training loss, Batch 31500: 0.005113949067890644\n",
      "training loss, Batch 31550: 0.010967135429382324\n",
      "training loss, Batch 31600: 0.005581130739301443\n",
      "training loss, Batch 31650: 0.00469304621219635\n",
      "training loss, Batch 31700: 0.004238351248204708\n",
      "training loss, Batch 31750: 0.006592420395463705\n",
      "training loss, Batch 31800: 0.004163016565144062\n",
      "training loss, Batch 31850: 0.003955616615712643\n",
      "training loss, Batch 31900: 0.008297341875731945\n",
      "training loss, Batch 31950: 0.002917176578193903\n",
      "training loss, Batch 32000: 0.004512077197432518\n",
      "training loss, Batch 32050: 0.005199231673032045\n",
      "training loss, Batch 32100: 0.005443292669951916\n",
      "training loss, Batch 32150: 0.003339691087603569\n",
      "training loss, Batch 32200: 0.004347639158368111\n",
      "training loss, Batch 32250: 0.003911071456968784\n",
      "training loss, Batch 32300: 0.004387527704238892\n",
      "training loss, Batch 32350: 0.0051394272595644\n",
      "training loss, Batch 32400: 0.005308086052536964\n",
      "training loss, Batch 32450: 0.006528610363602638\n",
      "training loss, Batch 32500: 0.004694398492574692\n",
      "training loss, Batch 32550: 0.004319595173001289\n",
      "training loss, Batch 32600: 0.004364603199064732\n",
      "training loss, Batch 32650: 0.0035965614952147007\n",
      "training loss, Batch 32700: 0.006632785312831402\n",
      "training loss, Batch 32750: 0.004296169150620699\n",
      "training loss, Batch 32800: 0.006254325620830059\n",
      "training loss, Batch 32850: 0.004979969002306461\n",
      "training loss, Batch 32900: 0.0035908669233322144\n",
      "training loss, Batch 32950: 0.002931468654423952\n",
      "training loss, Batch 33000: 0.004424483515322208\n",
      "training loss, Batch 33050: 0.003429573029279709\n",
      "training loss, Batch 33100: 0.006834298837929964\n",
      "training loss, Batch 33150: 0.00597894424572587\n",
      "training loss, Batch 33200: 0.004409241955727339\n",
      "training loss, Batch 33250: 0.005656038876622915\n",
      "training loss, Batch 33300: 0.003244974184781313\n",
      "training loss, Batch 33350: 0.0055563785135746\n",
      "training loss, Batch 33400: 0.00669444352388382\n",
      "training loss, Batch 33450: 0.0067349569872021675\n",
      "training loss, Batch 33500: 0.002615791279822588\n",
      "training loss, Batch 33550: 0.004105319269001484\n",
      "training loss, Batch 33600: 0.0022788613568991423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 33650: 0.0048675984144210815\n",
      "training loss, Batch 33700: 0.0036408137530088425\n",
      "training loss, Batch 33750: 0.004517735913395882\n",
      "training loss, Batch 33800: 0.0032747462391853333\n",
      "training loss, Batch 33850: 0.0035774498246610165\n",
      "training loss, Batch 33900: 0.006899246014654636\n",
      "training loss, Batch 33950: 0.003888916689902544\n",
      "training loss, Batch 34000: 0.010050100274384022\n",
      "training loss, Batch 34050: 0.0027087097987532616\n",
      "training loss, Batch 34100: 0.00901820883154869\n",
      "training loss, Batch 34150: 0.004557190928608179\n",
      "training loss, Batch 34200: 0.006500198505818844\n",
      "training loss, Batch 34250: 0.005741229746490717\n",
      "training loss, Batch 34300: 0.004154632333666086\n",
      "training loss, Batch 34350: 0.0035498901270329952\n",
      "training loss, Batch 34400: 0.0020682201720774174\n",
      "training loss, Batch 34450: 0.009524148888885975\n",
      "training loss, Batch 34500: 0.004761679098010063\n",
      "training loss, Batch 34550: 0.005918489769101143\n",
      "training loss, Batch 34600: 0.006431677378714085\n",
      "training loss, Batch 34650: 0.004654083400964737\n",
      "training loss, Batch 34700: 0.0035059149377048016\n",
      "training loss, Batch 34750: 0.007902663201093674\n",
      "training loss, Batch 34800: 0.007913406938314438\n",
      "training loss, Batch 34850: 0.003820340149104595\n",
      "training loss, Batch 34900: 0.007291822694242001\n",
      "training loss, Batch 34950: 0.004265985451638699\n",
      "training loss, Batch 35000: 0.006736431270837784\n",
      "training loss, Batch 35050: 0.003786254907026887\n",
      "training loss, Batch 35100: 0.002575310878455639\n",
      "training loss, Batch 35150: 0.003693146165460348\n",
      "training loss, Batch 35200: 0.004169662017375231\n",
      "training loss, Batch 35250: 0.004590895958244801\n",
      "training loss, Batch 35300: 0.0034295106306672096\n",
      "training loss, Batch 35350: 0.005427239462733269\n",
      "training loss, Batch 35400: 0.005690615624189377\n",
      "training loss, Batch 35450: 0.0032546904403716326\n",
      "training loss, Batch 35500: 0.002927761757746339\n",
      "training loss, Batch 35550: 0.0034372135996818542\n",
      "training loss, Batch 35600: 0.0051073841750621796\n",
      "training loss, Batch 35650: 0.004893382545560598\n",
      "training loss, Batch 35700: 0.0037364864256232977\n",
      "training loss, Batch 35750: 0.0038997745141386986\n",
      "training loss, Batch 35800: 0.0051683830097317696\n",
      "training loss, Batch 35850: 0.0038798809982836246\n",
      "training loss, Batch 35900: 0.004857315681874752\n",
      "training loss, Batch 35950: 0.003358489368110895\n",
      "training loss, Batch 36000: 0.0032212487421929836\n",
      "training loss, Batch 36050: 0.007311719469726086\n",
      "training loss, Batch 36100: 0.006247907876968384\n",
      "training loss, Batch 36150: 0.004992799833416939\n",
      "training loss, Batch 36200: 0.006481674499809742\n",
      "training loss, Batch 36250: 0.010145542211830616\n",
      "training loss, Batch 36300: 0.005899306386709213\n",
      "training loss, Batch 36350: 0.0035659729037433863\n",
      "training loss, Batch 36400: 0.005226765759289265\n",
      "training loss, Batch 36450: 0.0030586617067456245\n",
      "training loss, Batch 36500: 0.0029659755527973175\n",
      "training loss, Batch 36550: 0.004144626669585705\n",
      "training loss, Batch 36600: 0.0019401762401685119\n",
      "training loss, Batch 36650: 0.004237509332597256\n",
      "training loss, Batch 36700: 0.0028422046452760696\n",
      "training loss, Batch 36750: 0.0030341879464685917\n",
      "training loss, Batch 36800: 0.003122589085251093\n",
      "training loss, Batch 36850: 0.011104980483651161\n",
      "training loss, Batch 36900: 0.004300737753510475\n",
      "training loss, Batch 36950: 0.00405496172606945\n",
      "training loss, Batch 37000: 0.004188491031527519\n",
      "training loss, Batch 37050: 0.005298091098666191\n",
      "training loss, Batch 37100: 0.0075986869633197784\n",
      "training loss, Batch 37150: 0.005089171230792999\n",
      "training loss, Batch 37200: 0.010226680897176266\n",
      "training loss, Batch 37250: 0.002589559881016612\n",
      "training loss, Batch 37300: 0.00411755358800292\n",
      "training loss, Batch 37350: 0.005061549134552479\n",
      "training loss, Batch 37400: 0.004650081507861614\n",
      "training loss, Batch 37450: 0.0044859182089567184\n",
      "training loss, Batch 37500: 0.002622260944917798\n",
      "training loss, Batch 37550: 0.004112645052373409\n",
      "training loss, Batch 37600: 0.00481034442782402\n",
      "training loss, Batch 37650: 0.003167805727571249\n",
      "training loss, Batch 37700: 0.004095430951565504\n",
      "training loss, Batch 37750: 0.0021498422138392925\n",
      "training loss, Batch 37800: 0.002582496963441372\n",
      "training loss, Batch 37850: 0.008939332328736782\n",
      "training loss, Batch 37900: 0.004856463987380266\n",
      "training loss, Batch 37950: 0.0037428943905979395\n",
      "training loss, Batch 38000: 0.006497920490801334\n",
      "training loss, Batch 38050: 0.0058844368904829025\n",
      "training loss, Batch 38100: 0.004405353683978319\n",
      "training loss, Batch 38150: 0.003292057430371642\n",
      "training loss, Batch 38200: 0.005106980912387371\n",
      "training loss, Batch 38250: 0.0035106500145047903\n",
      "training loss, Batch 38300: 0.002437323797494173\n",
      "training loss, Batch 38350: 0.004253944382071495\n",
      "training loss, Batch 38400: 0.0031516565941274166\n",
      "training loss, Batch 38450: 0.008742354810237885\n",
      "training loss, Batch 38500: 0.006734127178788185\n",
      "training loss, Batch 38550: 0.005197333171963692\n",
      "training loss, Batch 38600: 0.005616197362542152\n",
      "training loss, Batch 38650: 0.005017303396016359\n",
      "training loss, Batch 38700: 0.004206893965601921\n",
      "training loss, Batch 38750: 0.008404728025197983\n",
      "training loss, Batch 38800: 0.006593432277441025\n",
      "training loss, Batch 38850: 0.005071289837360382\n",
      "training loss, Batch 38900: 0.004076135344803333\n",
      "training loss, Batch 38950: 0.0020802270155400038\n",
      "training loss, Batch 39000: 0.00426414143294096\n",
      "training loss, Batch 39050: 0.004628191702067852\n",
      "training loss, Batch 39100: 0.003327616024762392\n",
      "training loss, Batch 39150: 0.0042142122983932495\n",
      "training loss, Batch 39200: 0.008554348722100258\n",
      "training loss, Batch 39250: 0.007810644339770079\n",
      "training loss, Batch 39300: 0.002987383399158716\n",
      "training loss, Batch 39350: 0.007206516340374947\n",
      "training loss, Batch 39400: 0.006579122506082058\n",
      "training loss, Batch 39450: 0.006290121003985405\n",
      "training loss, Batch 39500: 0.004245202988386154\n",
      "training loss, Batch 39550: 0.005344713572412729\n",
      "training loss, Batch 39600: 0.004149766638875008\n",
      "training loss, Batch 39650: 0.00424570869654417\n",
      "training loss, Batch 39700: 0.003468900453299284\n",
      "training loss, Batch 39750: 0.0031645684503018856\n",
      "training loss, Batch 39800: 0.007787995971739292\n",
      "training loss, Batch 39850: 0.0031563658267259598\n",
      "training loss, Batch 39900: 0.007140836212784052\n",
      "training loss, Batch 39950: 0.008181510493159294\n",
      "training loss, Batch 40000: 0.004781608935445547\n",
      "training loss, Batch 40050: 0.00814458541572094\n",
      "training loss, Batch 40100: 0.0032786214724183083\n",
      "training loss, Batch 40150: 0.005978702567517757\n",
      "training loss, Batch 40200: 0.0037461714819073677\n",
      "training loss, Batch 40250: 0.006440277211368084\n",
      "training loss, Batch 40300: 0.005769199691712856\n",
      "training loss, Batch 40350: 0.010571524500846863\n",
      "training loss, Batch 40400: 0.003108493983745575\n",
      "training loss, Batch 40450: 0.012939194217324257\n",
      "training loss, Batch 40500: 0.004909983836114407\n",
      "training loss, Batch 40550: 0.002519447822123766\n",
      "training loss, Batch 40600: 0.009947560727596283\n",
      "training loss, Batch 40650: 0.005841382779181004\n",
      "training loss, Batch 40700: 0.003365900833159685\n",
      "training loss, Batch 40750: 0.0031666492577642202\n",
      "training loss, Batch 40800: 0.0032632220536470413\n",
      "training loss, Batch 40850: 0.007606578525155783\n",
      "training loss, Batch 40900: 0.0016586866695433855\n",
      "training loss, Batch 40950: 0.004830840043723583\n",
      "training loss, Batch 41000: 0.008333018980920315\n",
      "training loss, Batch 41050: 0.0057156882248818874\n",
      "training loss, Batch 41100: 0.002309277653694153\n",
      "training loss, Batch 41150: 0.004741395357996225\n",
      "training loss, Batch 41200: 0.0033177919685840607\n",
      "training loss, Batch 41250: 0.018833830952644348\n",
      "training loss, Batch 41300: 0.0035399855114519596\n",
      "training loss, Batch 41350: 0.005922714248299599\n",
      "training loss, Batch 41400: 0.004204938188195229\n",
      "training loss, Batch 41450: 0.005400558467954397\n",
      "training loss, Batch 41500: 0.006071759387850761\n",
      "training loss, Batch 41550: 0.009922021068632603\n",
      "training loss, Batch 41600: 0.005984151270240545\n",
      "training loss, Batch 41650: 0.005743545480072498\n",
      "training loss, Batch 41700: 0.004032927565276623\n",
      "training loss, Batch 41750: 0.0019896901212632656\n",
      "training loss, Batch 41800: 0.003821884747594595\n",
      "training loss, Batch 41850: 0.011098111048340797\n",
      "training loss, Batch 41900: 0.002209385856986046\n",
      "training loss, Batch 41950: 0.003801017301157117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 42000: 0.0030543054454028606\n",
      "training loss, Batch 42050: 0.0054839979857206345\n",
      "training loss, Batch 42100: 0.006668302230536938\n",
      "training loss, Batch 42150: 0.00882733054459095\n",
      "training loss, Batch 42200: 0.003591278800740838\n",
      "training loss, Batch 42250: 0.006105178967118263\n",
      "training loss, Batch 42300: 0.0036301405634731054\n",
      "training loss, Batch 42350: 0.008841874077916145\n",
      "training loss, Batch 42400: 0.0025107054971158504\n",
      "training loss, Batch 42450: 0.0023278328590095043\n",
      "training loss, Batch 42500: 0.002847387921065092\n",
      "training loss, Batch 42550: 0.008122687228024006\n",
      "training loss, Batch 42600: 0.0024633863940835\n",
      "training loss, Batch 42650: 0.005772334523499012\n",
      "training loss, Batch 42700: 0.004146336577832699\n",
      "training loss, Batch 42750: 0.007827743887901306\n",
      "training loss, Batch 42800: 0.008218629285693169\n",
      "training loss, Batch 42850: 0.007218176033347845\n",
      "training loss, Batch 42900: 0.008816980756819248\n",
      "training loss, Batch 42950: 0.0026928558945655823\n",
      "training loss, Batch 43000: 0.004903891589492559\n",
      "training loss, Batch 43050: 0.009203446097671986\n",
      "training loss, Batch 43100: 0.003483982291072607\n",
      "training loss, Batch 43150: 0.0030878374818712473\n",
      "training loss, Batch 43200: 0.0032945177517831326\n",
      "training loss, Batch 43250: 0.0038254475221037865\n",
      "training loss, Batch 43300: 0.010799765586853027\n",
      "training loss, Batch 43350: 0.0038897551130503416\n",
      "training loss, Batch 43400: 0.0036562904715538025\n",
      "training loss, Batch 43450: 0.0037972305435687304\n",
      "training loss, Batch 43500: 0.005577562376856804\n",
      "training loss, Batch 43550: 0.0036960539873689413\n",
      "training loss, Batch 43600: 0.006315373815596104\n",
      "training loss, Batch 43650: 0.003942530602216721\n",
      "training loss, Batch 43700: 0.009879272431135178\n",
      "training loss, Batch 43750: 0.007172488607466221\n",
      "training loss, Batch 43800: 0.006822654046118259\n",
      "training loss, Batch 43850: 0.009988763369619846\n",
      "training loss, Batch 43900: 0.0051969001069664955\n",
      "training loss, Batch 43950: 0.004034389741718769\n",
      "training loss, Batch 44000: 0.006931074894964695\n",
      "training loss, Batch 44050: 0.004165150225162506\n",
      "training loss, Batch 44100: 0.002420642413198948\n",
      "training loss, Batch 44150: 0.0035863176453858614\n",
      "training loss, Batch 44200: 0.005138040520250797\n",
      "training loss, Batch 44250: 0.005443689413368702\n",
      "training loss, Batch 44300: 0.004841221962124109\n",
      "training loss, Batch 44350: 0.00428954791277647\n",
      "training loss, Batch 44400: 0.003654047381132841\n",
      "training loss, Batch 44450: 0.00561550073325634\n",
      "training loss, Batch 44500: 0.003809510264545679\n",
      "training loss, Batch 44550: 0.004512638784945011\n",
      "training loss, Batch 44600: 0.006272515747696161\n",
      "training loss, Batch 44650: 0.0038979812525212765\n",
      "training loss, Batch 44700: 0.004020872060209513\n",
      "training loss, Batch 44750: 0.004401837475597858\n",
      "training loss, Batch 44800: 0.003129510674625635\n",
      "training loss, Batch 44850: 0.005377109162509441\n",
      "training loss, Batch 44900: 0.0029215433169156313\n",
      "training loss, Batch 44950: 0.007568765431642532\n",
      "training loss, Batch 45000: 0.0045341313816607\n",
      "training loss, Batch 45050: 0.00287602748721838\n",
      "training loss, Batch 45100: 0.004276517778635025\n",
      "training loss, Batch 45150: 0.007677397225052118\n",
      "training loss, Batch 45200: 0.006345890928059816\n",
      "training loss, Batch 45250: 0.002755258232355118\n",
      "training loss, Batch 45300: 0.0052751218900084496\n",
      "training loss, Batch 45350: 0.007685263641178608\n",
      "training loss, Batch 45400: 0.004956611432135105\n",
      "training loss, Batch 45450: 0.0049319518730044365\n",
      "training loss, Batch 45500: 0.0035344166681170464\n",
      "training loss, Batch 45550: 0.00370968459174037\n",
      "training loss, Batch 45600: 0.0034646803978830576\n",
      "training loss, Batch 45650: 0.00599066074937582\n",
      "training loss, Batch 45700: 0.004244104027748108\n",
      "training loss, Batch 45750: 0.002970491535961628\n",
      "training loss, Batch 45800: 0.0031920087058097124\n",
      "training loss, Batch 45850: 0.0027645183727145195\n",
      "training loss, Batch 45900: 0.003551362780854106\n",
      "training loss, Batch 45950: 0.007911317050457\n",
      "training loss, Batch 46000: 0.0021362185943871737\n",
      "training loss, Batch 46050: 0.0036201858893036842\n",
      "training loss, Batch 46100: 0.0034472919069230556\n",
      "training loss, Batch 46150: 0.0026955073699355125\n",
      "training loss, Batch 46200: 0.0016104201786220074\n",
      "training loss, Batch 46250: 0.0029221437871456146\n",
      "training loss, Batch 46300: 0.007155384868383408\n",
      "training loss, Batch 46350: 0.004093576222658157\n",
      "training loss, Batch 46400: 0.009857534430921078\n",
      "training loss, Batch 46450: 0.005107315257191658\n",
      "training loss, Batch 46500: 0.0049532135017216206\n",
      "training loss, Batch 46550: 0.003224035259336233\n",
      "training loss, Batch 46600: 0.0033232562709599733\n",
      "training loss, Batch 46650: 0.010874363593757153\n",
      "training loss, Batch 46700: 0.011807153932750225\n",
      "training loss, Batch 46750: 0.0050002047792077065\n",
      "training loss, Batch 46800: 0.01566336676478386\n",
      "training loss, Batch 46850: 0.006021636538207531\n",
      "training loss, Batch 46900: 0.004013112746179104\n",
      "training loss, Batch 46950: 0.002696569776162505\n",
      "training loss, Batch 47000: 0.0022889766842126846\n",
      "training loss, Batch 47050: 0.004874536767601967\n",
      "training loss, Batch 47100: 0.00430466141551733\n",
      "training loss, Batch 47150: 0.00228913314640522\n",
      "training loss, Batch 47200: 0.005988424643874168\n",
      "training loss, Batch 47250: 0.003997397609055042\n",
      "training loss, Batch 47300: 0.004433741793036461\n",
      "training loss, Batch 47350: 0.0039665489457547665\n",
      "training loss, Batch 47400: 0.0024545318447053432\n",
      "training loss, Batch 47450: 0.005148633383214474\n",
      "training loss, Batch 47500: 0.0040077948942780495\n",
      "training loss, Batch 47550: 0.004126575775444508\n",
      "training loss, Batch 47600: 0.003702959045767784\n",
      "training loss, Batch 47650: 0.004550867713987827\n",
      "training loss, Batch 47700: 0.007341356948018074\n",
      "training loss, Batch 47750: 0.00530336145311594\n",
      "training loss, Batch 47800: 0.003540611360222101\n",
      "training loss, Batch 47850: 0.002184727229177952\n",
      "training loss, Batch 47900: 0.005835556425154209\n",
      "training loss, Batch 47950: 0.0037229040171951056\n",
      "training loss, Batch 48000: 0.0033508113119751215\n",
      "training loss, Batch 48050: 0.0040165698155760765\n",
      "training loss, Batch 48100: 0.0024406109005212784\n",
      "training loss, Batch 48150: 0.005207633599638939\n",
      "training loss, Batch 48200: 0.0060624973848462105\n",
      "training loss, Batch 48250: 0.00476793572306633\n",
      "training loss, Batch 48300: 0.00287957233376801\n",
      "training loss, Batch 48350: 0.0035949659068137407\n",
      "training loss, Batch 48400: 0.0047339689917862415\n",
      "training loss, Batch 48450: 0.0033286460675299168\n",
      "training loss, Batch 48500: 0.007910287007689476\n",
      "training loss, Batch 48550: 0.007022761274129152\n",
      "training loss, Batch 48600: 0.004202783107757568\n",
      "training loss, Batch 48650: 0.005975295789539814\n",
      "training loss, Batch 48700: 0.004885528702288866\n",
      "training loss, Batch 48750: 0.004457215312868357\n",
      "training loss, Batch 48800: 0.00893389992415905\n",
      "training loss, Batch 48850: 0.005356835667043924\n",
      "training loss, Batch 48900: 0.00549385417252779\n",
      "training loss, Batch 48950: 0.005920395255088806\n",
      "training loss, Batch 49000: 0.002572515280917287\n",
      "training loss, Batch 49050: 0.005210069008171558\n",
      "training loss, Batch 49100: 0.00200949446298182\n",
      "training loss, Batch 49150: 0.0032136866357177496\n",
      "training loss, Batch 49200: 0.0046535516157746315\n",
      "training loss, Batch 49250: 0.006994257215410471\n",
      "training loss, Batch 49300: 0.003688513534143567\n",
      "training loss, Batch 49350: 0.004982423037290573\n",
      "training loss, Batch 49400: 0.005733871832489967\n",
      "training loss, Batch 49450: 0.005250849761068821\n",
      "training loss, Batch 49500: 0.010163435712456703\n",
      "training loss, Batch 49550: 0.0040208217687904835\n",
      "training loss, Batch 49600: 0.011635401286184788\n",
      "training loss, Batch 49650: 0.00594540499150753\n",
      "training loss, Batch 49700: 0.011127734556794167\n",
      "training loss, Batch 49750: 0.004478996619582176\n",
      "training loss, Batch 49800: 0.0054327561520040035\n",
      "training loss, Batch 49850: 0.002377074211835861\n",
      "training loss, Batch 49900: 0.005662492476403713\n",
      "training loss, Batch 49950: 0.003052810672670603\n",
      "training loss, Batch 50000: 0.005790024064481258\n",
      "training loss, Batch 50050: 0.006685217842459679\n",
      "training loss, Batch 50100: 0.01223454624414444\n",
      "training loss, Batch 50150: 0.004649775102734566\n",
      "training loss, Batch 50200: 0.0045327963307499886\n",
      "training loss, Batch 50250: 0.0033340766094624996\n",
      "training loss, Batch 50300: 0.003598815994337201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 50350: 0.004311084281653166\n",
      "training loss, Batch 50400: 0.008389485068619251\n",
      "training loss, Batch 50450: 0.006958366837352514\n",
      "training loss, Batch 50500: 0.004001360386610031\n",
      "training loss, Batch 50550: 0.007153522688895464\n",
      "training loss, Batch 50600: 0.0030475715175271034\n",
      "training loss, Batch 50650: 0.0027817445807158947\n",
      "training loss, Batch 50700: 0.003102746559306979\n",
      "training loss, Batch 50750: 0.006217248737812042\n",
      "training loss, Batch 50800: 0.002981673926115036\n",
      "training loss, Batch 50850: 0.004896404221653938\n",
      "training loss, Batch 50900: 0.004838569089770317\n",
      "training loss, Batch 50950: 0.004607624374330044\n",
      "training loss, Batch 51000: 0.0034691253677010536\n",
      "training loss, Batch 51050: 0.006631567142903805\n",
      "training loss, Batch 51100: 0.002848829608410597\n",
      "training loss, Batch 51150: 0.0029630016069859266\n",
      "training loss, Batch 51200: 0.004980514757335186\n",
      "training loss, Batch 51250: 0.004371946211904287\n",
      "training loss, Batch 51300: 0.003631128929555416\n",
      "training loss, Batch 51350: 0.006078540347516537\n",
      "training loss, Batch 51400: 0.006469909567385912\n",
      "training loss, Batch 51450: 0.004536413121968508\n",
      "training loss, Batch 51500: 0.008401819504797459\n",
      "training loss, Batch 51550: 0.004450999200344086\n",
      "training loss, Batch 51600: 0.008869576267898083\n",
      "training loss, Batch 51650: 0.00390998087823391\n",
      "training loss, Batch 51700: 0.0024989238008856773\n",
      "training loss, Batch 51750: 0.013589811511337757\n",
      "training loss, Batch 51800: 0.002239239402115345\n",
      "training loss, Batch 51850: 0.00300526968203485\n",
      "training loss, Batch 51900: 0.005387012846767902\n",
      "training loss, Batch 51950: 0.007463816553354263\n",
      "training loss, Batch 52000: 0.004697337746620178\n",
      "training loss, Batch 52050: 0.004544362425804138\n",
      "training loss, Batch 52100: 0.0038893031887710094\n",
      "training loss, Batch 52150: 0.0037424948532134295\n",
      "training loss, Batch 52200: 0.006321819964796305\n",
      "training loss, Batch 52250: 0.0036687010433524847\n",
      "training loss, Batch 52300: 0.0034652454778552055\n",
      "training loss, Batch 52350: 0.0030579494778066874\n",
      "training loss, Batch 52400: 0.004161939024925232\n",
      "training loss, Batch 52450: 0.0032388183753937483\n",
      "training loss, Batch 52500: 0.004132854752242565\n",
      "training loss, Batch 52550: 0.003929897211492062\n",
      "training loss, Batch 52600: 0.004399258643388748\n",
      "training loss, Batch 52650: 0.003984915092587471\n",
      "training loss, Batch 52700: 0.0033206494990736246\n",
      "training loss, Batch 52750: 0.0034656054340302944\n",
      "training loss, Batch 52800: 0.00440005399286747\n",
      "training loss, Batch 52850: 0.0033119621220976114\n",
      "training loss, Batch 52900: 0.0037475863937288523\n",
      "training loss, Batch 52950: 0.0029523195698857307\n",
      "training loss, Batch 53000: 0.00866745412349701\n",
      "training loss, Batch 53050: 0.004012494347989559\n",
      "training loss, Batch 53100: 0.00740476232022047\n",
      "training loss, Batch 53150: 0.009091475047171116\n",
      "training loss, Batch 53200: 0.0013519767671823502\n",
      "training loss, Batch 53250: 0.0050657447427511215\n",
      "training loss, Batch 53300: 0.0029817079193890095\n",
      "training loss, Batch 53350: 0.005355897359549999\n",
      "training loss, Batch 53400: 0.006936901248991489\n",
      "training loss, Batch 53450: 0.004730530083179474\n",
      "training loss, Batch 53500: 0.0035558261442929506\n",
      "training loss, Batch 53550: 0.0047183348797261715\n",
      "training loss, Batch 53600: 0.0037425970658659935\n",
      "training loss, Batch 53650: 0.004250119440257549\n",
      "training loss, Batch 53700: 0.0024074940010905266\n",
      "training loss, Batch 53750: 0.0026924042031168938\n",
      "training loss, Batch 53800: 0.003690238343551755\n",
      "training loss, Batch 53850: 0.0037920125760138035\n",
      "training loss, Batch 53900: 0.002048090100288391\n",
      "training loss, Batch 53950: 0.00805329903960228\n",
      "training loss, Batch 54000: 0.004473267123103142\n",
      "training loss, Batch 54050: 0.004284410737454891\n",
      "training loss, Batch 54100: 0.003917878493666649\n",
      "training loss, Batch 54150: 0.0031987293623387814\n",
      "training loss, Batch 54200: 0.004076534882187843\n",
      "training loss, Batch 54250: 0.003849424421787262\n",
      "training loss, Batch 54300: 0.006300783716142178\n",
      "training loss, Batch 54350: 0.004206117708235979\n",
      "training loss, Batch 54400: 0.0033493167720735073\n",
      "training loss, Batch 54450: 0.0032909847795963287\n",
      "training loss, Batch 54500: 0.005150164943188429\n",
      "training loss, Batch 54550: 0.003524226602166891\n",
      "training loss, Batch 54600: 0.00347367231734097\n",
      "training loss, Batch 54650: 0.0027589043602347374\n",
      "training loss, Batch 54700: 0.003047072561457753\n",
      "training loss, Batch 54750: 0.0031031568069010973\n",
      "training loss, Batch 54800: 0.005038839299231768\n",
      "training loss, Batch 54850: 0.0027051123324781656\n",
      "training loss, Batch 54900: 0.0025310926139354706\n",
      "training loss, Batch 54950: 0.005343486554920673\n",
      "training loss, Batch 55000: 0.002375092590227723\n",
      "training loss, Batch 55050: 0.009284285828471184\n",
      "training loss, Batch 55100: 0.003922505769878626\n",
      "training loss, Batch 55150: 0.004484767094254494\n",
      "training loss, Batch 55200: 0.007484227418899536\n",
      "training loss, Batch 55250: 0.009332168847322464\n",
      "training loss, Batch 55300: 0.005734446924179792\n",
      "training loss, Batch 55350: 0.004490765742957592\n",
      "training loss, Batch 55400: 0.0026082913391292095\n",
      "training loss, Batch 55450: 0.0028236261568963528\n",
      "training loss, Batch 55500: 0.003514840267598629\n",
      "training loss, Batch 55550: 0.005530213937163353\n",
      "training loss, Batch 55600: 0.006442839279770851\n",
      "training loss, Batch 55650: 0.0055638705380260944\n",
      "training loss, Batch 55700: 0.015429958701133728\n",
      "training loss, Batch 55750: 0.004655703902244568\n",
      "training loss, Batch 55800: 0.0035396551247686148\n",
      "training loss, Batch 55850: 0.004222753457725048\n",
      "training loss, Batch 55900: 0.00342191057279706\n",
      "training loss, Batch 55950: 0.005358334165066481\n",
      "training loss, Batch 56000: 0.002297158818691969\n",
      "training loss, Batch 56050: 0.013051353394985199\n",
      "training loss, Batch 56100: 0.005655335262417793\n",
      "training loss, Batch 56150: 0.00319722481071949\n",
      "training loss, Batch 56200: 0.004085334483534098\n",
      "training loss, Batch 56250: 0.0031941146589815617\n",
      "training loss, Batch 56300: 0.004395653493702412\n",
      "training loss, Batch 56350: 0.005396801978349686\n",
      "training loss, Batch 56400: 0.0034020114690065384\n",
      "training loss, Batch 56450: 0.00851330254226923\n",
      "training loss, Batch 56500: 0.00474822660908103\n",
      "training loss, Batch 56550: 0.004687456414103508\n",
      "training loss, Batch 56600: 0.005623948760330677\n",
      "training loss, Batch 56650: 0.007816174998879433\n",
      "training loss, Batch 56700: 0.006108380854129791\n",
      "training loss, Batch 56750: 0.010166717693209648\n",
      "training loss, Batch 56800: 0.0061368816532194614\n",
      "training loss, Batch 56850: 0.003976964391767979\n",
      "training loss, Batch 56900: 0.009776946157217026\n",
      "training loss, Batch 56950: 0.004102887585759163\n",
      "training loss, Batch 57000: 0.006333500146865845\n",
      "training loss, Batch 57050: 0.007451147772371769\n",
      "training loss, Batch 57100: 0.007353197317570448\n",
      "training loss, Batch 57150: 0.009827706962823868\n",
      "training loss, Batch 57200: 0.006063901819288731\n",
      "training loss, Batch 57250: 0.003598498646169901\n",
      "training loss, Batch 57300: 0.008425991982221603\n",
      "training loss, Batch 57350: 0.006686932407319546\n",
      "training loss, Batch 57400: 0.0036791216116398573\n",
      "training loss, Batch 57450: 0.0030480041168630123\n",
      "training loss, Batch 57500: 0.007671543397009373\n",
      "training loss, Batch 57550: 0.0038388508837670088\n",
      "training loss, Batch 57600: 0.003926718141883612\n",
      "training loss, Batch 57650: 0.005910102277994156\n",
      "training loss, Batch 57700: 0.005157190375030041\n",
      "training loss, Batch 57750: 0.004881055559962988\n",
      "training loss, Batch 57800: 0.005378068890422583\n",
      "training loss, Batch 57850: 0.004358568694442511\n",
      "training loss, Batch 57900: 0.002136711962521076\n",
      "training loss, Batch 57950: 0.002683316357433796\n",
      "training loss, Batch 58000: 0.004405150655657053\n",
      "training loss, Batch 58050: 0.007367498241364956\n",
      "training loss, Batch 58100: 0.00586120318621397\n",
      "training loss, Batch 58150: 0.006541375070810318\n",
      "training loss, Batch 58200: 0.0034923902712762356\n",
      "training loss, Batch 58250: 0.0043881116434931755\n",
      "training loss, Batch 58300: 0.002704838290810585\n",
      "training loss, Batch 58350: 0.0073577421717345715\n",
      "training loss, Batch 58400: 0.004664410836994648\n",
      "training loss, Batch 58450: 0.005446270573884249\n",
      "training loss, Batch 58500: 0.005905143916606903\n",
      "training loss, Batch 58550: 0.006504072807729244\n",
      "training loss, Batch 58600: 0.0038713752292096615\n",
      "training loss, Batch 58650: 0.0032060269732028246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 58700: 0.00261390651576221\n",
      "training loss, Batch 58750: 0.0035668343771249056\n",
      "training loss, Batch 58800: 0.0024722968228161335\n",
      "training loss, Batch 58850: 0.003911037929356098\n",
      "training loss, Batch 58900: 0.006293602287769318\n",
      "training loss, Batch 58950: 0.00469607999548316\n",
      "training loss, Batch 59000: 0.007283516228199005\n",
      "training loss, Batch 59050: 0.0050926972180604935\n",
      "training loss, Batch 59100: 0.006159760057926178\n",
      "training loss, Batch 59150: 0.004821536596864462\n",
      "training loss, Batch 59200: 0.004551693797111511\n",
      "training loss, Batch 59250: 0.0028757904656231403\n",
      "training loss, Batch 59300: 0.002587137743830681\n",
      "training loss, Batch 59350: 0.003178642364218831\n",
      "training loss, Batch 59400: 0.004765381570905447\n",
      "training loss, Batch 59450: 0.004137044306844473\n",
      "training loss, Batch 59500: 0.007457680068910122\n",
      "training loss, Batch 59550: 0.004596211016178131\n",
      "training loss, Batch 59600: 0.002050291048362851\n",
      "training loss, Batch 59650: 0.004511740058660507\n",
      "training loss, Batch 59700: 0.001378262764774263\n",
      "training loss, Batch 59750: 0.002933608368039131\n",
      "training loss, Batch 59800: 0.0024243067018687725\n",
      "training loss, Batch 59850: 0.0056211622431874275\n",
      "training loss, Batch 59900: 0.0027438835240900517\n",
      "training loss, Batch 59950: 0.003996378276497126\n",
      "training loss, Batch 60000: 0.012049028649926186\n",
      "training loss, Batch 60050: 0.0037918284069746733\n",
      "training loss, Batch 60100: 0.004918891005218029\n",
      "training loss, Batch 60150: 0.011713270097970963\n",
      "training loss, Batch 60200: 0.003550910856574774\n",
      "training loss, Batch 60250: 0.005942290183156729\n",
      "training loss, Batch 60300: 0.004952187649905682\n",
      "training loss, Batch 60350: 0.009080979973077774\n",
      "training loss, Batch 60400: 0.0035040355287492275\n",
      "training loss, Batch 60450: 0.004363002255558968\n",
      "training loss, Batch 60500: 0.007107253186404705\n",
      "training loss, Batch 60550: 0.0030997684225440025\n",
      "training loss, Batch 60600: 0.0031131012365221977\n",
      "training loss, Batch 60650: 0.007726630195975304\n",
      "training loss, Batch 60700: 0.004280914552509785\n",
      "training loss, Batch 60750: 0.004553818143904209\n",
      "training loss, Batch 60800: 0.008211995474994183\n",
      "training loss, Batch 60850: 0.007430630270391703\n",
      "training loss, Batch 60900: 0.0035104891285300255\n",
      "training loss, Batch 60950: 0.005304205231368542\n",
      "training loss, Batch 61000: 0.003900709096342325\n",
      "training loss, Batch 61050: 0.006879499182105064\n",
      "training loss, Batch 61100: 0.005585184320807457\n",
      "training loss, Batch 61150: 0.00546723697334528\n",
      "training loss, Batch 61200: 0.004062084946781397\n",
      "training loss, Batch 61250: 0.0038989740423858166\n",
      "training loss, Batch 61300: 0.0043035708367824554\n",
      "training loss, Batch 61350: 0.004533004015684128\n",
      "training loss, Batch 61400: 0.004972624592483044\n",
      "training loss, Batch 61450: 0.012612194754183292\n",
      "training loss, Batch 61500: 0.0055614132434129715\n",
      "training loss, Batch 61550: 0.002871384145691991\n",
      "training loss, Batch 61600: 0.005485227331519127\n",
      "training loss, Batch 61650: 0.007974434643983841\n",
      "training loss, Batch 61700: 0.010607387870550156\n",
      "training loss, Batch 61750: 0.003005905309692025\n",
      "training loss, Batch 61800: 0.00550021231174469\n",
      "training loss, Batch 61850: 0.004654057789593935\n",
      "training loss, Batch 61900: 0.006975466851145029\n",
      "training loss, Batch 61950: 0.0028417007997632027\n",
      "training loss, Batch 62000: 0.0039542559534311295\n",
      "training loss, Batch 62050: 0.005632768385112286\n",
      "training loss, Batch 62100: 0.006518359296023846\n",
      "training loss, Batch 62150: 0.004444076679646969\n",
      "training loss, Batch 62200: 0.002902072388678789\n",
      "training loss, Batch 62250: 0.004692610818892717\n",
      "training loss, Batch 62300: 0.008918694220483303\n",
      "training loss, Batch 62350: 0.003650027560070157\n",
      "training loss, Batch 62400: 0.0035816978197544813\n",
      "training loss, Batch 62450: 0.002810921985656023\n",
      "training loss, Batch 62500: 0.004783797077834606\n",
      "training loss, Batch 62550: 0.005293686408549547\n",
      "training loss, Batch 62600: 0.0024035791866481304\n",
      "training loss, Batch 62650: 0.00883355364203453\n",
      "training loss, Batch 62700: 0.004993102978914976\n",
      "training loss, Batch 62750: 0.004896919708698988\n",
      "training loss, Batch 62800: 0.0051530697382986546\n",
      "training loss, Batch 62850: 0.003254033625125885\n",
      "training loss, Batch 62900: 0.002532072365283966\n",
      "training loss, Batch 62950: 0.002441932912915945\n",
      "training loss, Batch 63000: 0.005508315749466419\n",
      "training loss, Batch 63050: 0.0017297584563493729\n",
      "training loss, Batch 63100: 0.0049515776336193085\n",
      "training loss, Batch 63150: 0.0028440970927476883\n",
      "training loss, Batch 63200: 0.002264286857098341\n",
      "training loss, Batch 63250: 0.002496052533388138\n",
      "training loss, Batch 63300: 0.005581502802670002\n",
      "training loss, Batch 63350: 0.008786860853433609\n",
      "training loss, Batch 63400: 0.00958816334605217\n",
      "training loss, Batch 63450: 0.002393878996372223\n",
      "training loss, Batch 63500: 0.005457138177007437\n",
      "training loss, Batch 63550: 0.003559177042916417\n",
      "training loss, Batch 63600: 0.0023475694470107555\n",
      "training loss, Batch 63650: 0.003598111681640148\n",
      "training loss, Batch 63700: 0.005944170989096165\n",
      "training loss, Batch 63750: 0.004731123335659504\n",
      "training loss, Batch 63800: 0.0027745321858674288\n",
      "training loss, Batch 63850: 0.005551607813686132\n",
      "training loss, Batch 63900: 0.002974334405735135\n",
      "training loss, Batch 63950: 0.004752051085233688\n",
      "training loss, Batch 64000: 0.008993427269160748\n",
      "training loss, Batch 64050: 0.00348853156901896\n",
      "training loss, Batch 64100: 0.003428984899073839\n",
      "training loss, Batch 64150: 0.0077486648224294186\n",
      "training loss, Batch 64200: 0.0037891282700002193\n",
      "training loss, Batch 64250: 0.008329581469297409\n",
      "training loss, Batch 64300: 0.004985443316400051\n",
      "training loss, Batch 64350: 0.006302190478891134\n",
      "training loss, Batch 64400: 0.005900874268263578\n",
      "training loss, Batch 64450: 0.00355055695399642\n",
      "training loss, Batch 64500: 0.00484080333262682\n",
      "training loss, Batch 64550: 0.005678039975464344\n",
      "training loss, Batch 64600: 0.002757646143436432\n",
      "training loss, Batch 64650: 0.00459482055157423\n",
      "training loss, Batch 64700: 0.004874878562986851\n",
      "training loss, Batch 64750: 0.004169954918324947\n",
      "training loss, Batch 64800: 0.004301839973777533\n",
      "training loss, Batch 64850: 0.002299030777066946\n",
      "training loss, Batch 64900: 0.0040047913789749146\n",
      "training loss, Batch 64950: 0.003407500684261322\n",
      "training loss, Batch 65000: 0.006244689226150513\n",
      "training loss, Batch 65050: 0.005641601048409939\n",
      "training loss, Batch 65100: 0.004374013748019934\n",
      "training loss, Batch 65150: 0.0030471659265458584\n",
      "training loss, Batch 65200: 0.0038475212641060352\n",
      "training loss, Batch 65250: 0.003923055715858936\n",
      "training loss, Batch 65300: 0.0034265974536538124\n",
      "training loss, Batch 65350: 0.003762606531381607\n",
      "training loss, Batch 65400: 0.00352281890809536\n",
      "training loss, Batch 65450: 0.009696194902062416\n",
      "training loss, Batch 65500: 0.003875483525916934\n",
      "training loss, Batch 65550: 0.005140717141330242\n",
      "training loss, Batch 65600: 0.004688509739935398\n",
      "training loss, Batch 65650: 0.010565800592303276\n",
      "training loss, Batch 65700: 0.004211149178445339\n",
      "training loss, Batch 65750: 0.0046534109860658646\n",
      "training loss, Batch 65800: 0.009309393353760242\n",
      "training loss, Batch 65850: 0.0021710810251533985\n",
      "training loss, Batch 65900: 0.004208053462207317\n",
      "training loss, Batch 65950: 0.005472453311085701\n",
      "training loss, Batch 66000: 0.0030305027030408382\n",
      "training loss, Batch 66050: 0.0020090476609766483\n",
      "training loss, Batch 66100: 0.004305621609091759\n",
      "training loss, Batch 66150: 0.003759917104616761\n",
      "training loss, Batch 66200: 0.0025436514988541603\n",
      "training loss, Batch 66250: 0.0020069913007318974\n",
      "training loss, Batch 66300: 0.003346968675032258\n",
      "training loss, Batch 66350: 0.0026471770834177732\n",
      "training loss, Batch 66400: 0.0037730494514107704\n",
      "training loss, Batch 66450: 0.005846473854035139\n",
      "training loss, Batch 66500: 0.003061138791963458\n",
      "training loss, Batch 66550: 0.002167552011087537\n",
      "training loss, Batch 66600: 0.012622196227312088\n",
      "training loss, Batch 66650: 0.004553128965198994\n",
      "training loss, Batch 66700: 0.002241005189716816\n",
      "training loss, Batch 66750: 0.009048624895513058\n",
      "training loss, Batch 66800: 0.0021559149026870728\n",
      "training loss, Batch 66850: 0.003995841834694147\n",
      "training loss, Batch 66900: 0.004249827936291695\n",
      "training loss, Batch 66950: 0.004329722840338945\n",
      "training loss, Batch 67000: 0.0055801281705498695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 67050: 0.006291533820331097\n",
      "training loss, Batch 67100: 0.002901681698858738\n",
      "training loss, Batch 67150: 0.005286046769469976\n",
      "training loss, Batch 67200: 0.00392626179382205\n",
      "training loss, Batch 67250: 0.004189091268926859\n",
      "training loss, Batch 67300: 0.0025148377753794193\n",
      "training loss, Batch 67350: 0.0040634265169501305\n",
      "training loss, Batch 67400: 0.006315738428384066\n",
      "training loss, Batch 67450: 0.003131601493805647\n",
      "training loss, Batch 67500: 0.00475555844604969\n",
      "training loss, Batch 67550: 0.001940812449902296\n",
      "training loss, Batch 67600: 0.0022228434681892395\n",
      "training loss, Batch 67650: 0.00423961877822876\n",
      "training loss, Batch 67700: 0.0050654299557209015\n",
      "training loss, Batch 67750: 0.002020654734224081\n",
      "training loss, Batch 67800: 0.002445533871650696\n",
      "training loss, Batch 67850: 0.0012291832827031612\n",
      "training loss, Batch 67900: 0.001984959002584219\n",
      "training loss, Batch 67950: 0.0058616409078240395\n",
      "training loss, Batch 68000: 0.006025298032909632\n",
      "training loss, Batch 68050: 0.0021383105777204037\n",
      "training loss, Batch 68100: 0.004321601707488298\n",
      "training loss, Batch 68150: 0.0038157792296260595\n",
      "training loss, Batch 68200: 0.006769675761461258\n",
      "training loss, Batch 68250: 0.00281825615093112\n",
      "training loss, Batch 68300: 0.004284756723791361\n",
      "training loss, Batch 68350: 0.003982182592153549\n",
      "training loss, Batch 68400: 0.0033498466946184635\n",
      "training loss, Batch 68450: 0.0038443636149168015\n",
      "training loss, Batch 68500: 0.003981981426477432\n",
      "training loss, Batch 68550: 0.004611392505466938\n",
      "training loss, Batch 68600: 0.013340326026082039\n",
      "training loss, Batch 68650: 0.00784302968531847\n",
      "training loss, Batch 68700: 0.00913739763200283\n",
      "training loss, Batch 68750: 0.0054616788402199745\n",
      "training loss, Batch 68800: 0.005303096026182175\n",
      "training loss, Batch 68850: 0.0012305759591981769\n",
      "training loss, Batch 68900: 0.004764365032315254\n",
      "training loss, Batch 68950: 0.007422031834721565\n",
      "training loss, Batch 69000: 0.006177493836730719\n",
      "training loss, Batch 69050: 0.005035554990172386\n",
      "training loss, Batch 69100: 0.004137189593166113\n",
      "training loss, Batch 69150: 0.004122816026210785\n",
      "training loss, Batch 69200: 0.005424237810075283\n",
      "training loss, Batch 69250: 0.004437356721609831\n",
      "training loss, Batch 69300: 0.003192197298631072\n",
      "training loss, Batch 69350: 0.006028198637068272\n",
      "training loss, Batch 69400: 0.0033602798357605934\n",
      "training loss, Batch 69450: 0.005734860897064209\n",
      "training loss, Batch 69500: 0.002761996351182461\n",
      "training loss, Batch 69550: 0.00467044860124588\n",
      "training loss, Batch 69600: 0.010047635063529015\n",
      "training loss, Batch 69650: 0.0026636822149157524\n",
      "training loss, Batch 69700: 0.0030598610173910856\n",
      "training loss, Batch 69750: 0.010055525228381157\n",
      "training loss, Batch 69800: 0.005913703236728907\n",
      "training loss, Batch 69850: 0.004021336790174246\n",
      "training loss, Batch 69900: 0.0032129979226738214\n",
      "training loss, Batch 69950: 0.0028622308745980263\n",
      "training loss, Batch 70000: 0.00550042325630784\n",
      "training loss, Batch 70050: 0.0031735217198729515\n",
      "training loss, Batch 70100: 0.004124398343265057\n",
      "training loss, Batch 70150: 0.004832928068935871\n",
      "training loss, Batch 70200: 0.0034040603786706924\n",
      "training loss, Batch 70250: 0.00666891410946846\n",
      "training loss, Batch 70300: 0.004043110180646181\n",
      "training loss, Batch 70350: 0.008531739003956318\n",
      "training loss, Batch 70400: 0.0027513434179127216\n",
      "training loss, Batch 70450: 0.0040629212744534016\n",
      "training loss, Batch 70500: 0.0028307209722697735\n",
      "training loss, Batch 70550: 0.003725804388523102\n",
      "training loss, Batch 70600: 0.006251289509236813\n",
      "training loss, Batch 70650: 0.004576269071549177\n",
      "training loss, Batch 70700: 0.0034683593548834324\n",
      "training loss, Batch 70750: 0.01101939007639885\n",
      "training loss, Batch 70800: 0.004857078660279512\n",
      "training loss, Batch 70850: 0.005436626262962818\n",
      "training loss, Batch 70900: 0.011191540397703648\n",
      "training loss, Batch 70950: 0.0071005080826580524\n",
      "training loss, Batch 71000: 0.005725889932364225\n",
      "training loss, Batch 71050: 0.0027585788629949093\n",
      "training loss, Batch 71100: 0.0031940010376274586\n",
      "training loss, Batch 71150: 0.004670439288020134\n",
      "training loss, Batch 71200: 0.006351182237267494\n",
      "training loss, Batch 71250: 0.005995769985020161\n",
      "training loss, Batch 71300: 0.00545884296298027\n",
      "training loss, Batch 71350: 0.0037699141539633274\n",
      "training loss, Batch 71400: 0.0034856614656746387\n",
      "training loss, Batch 71450: 0.005989049561321735\n",
      "training loss, Batch 71500: 0.004987283609807491\n",
      "training loss, Batch 71550: 0.0043572294525802135\n",
      "training loss, Batch 71600: 0.005230449605733156\n",
      "training loss, Batch 71650: 0.005458764731884003\n",
      "training loss, Batch 71700: 0.006786450743675232\n",
      "training loss, Batch 71750: 0.004759564995765686\n",
      "training loss, Batch 71800: 0.003399524837732315\n",
      "training loss, Batch 71850: 0.004633529111742973\n",
      "training loss, Batch 71900: 0.0061616357415914536\n",
      "training loss, Batch 71950: 0.003414916805922985\n",
      "training loss, Batch 72000: 0.00199036649428308\n",
      "training loss, Batch 72050: 0.004033597651869059\n",
      "training loss, Batch 72100: 0.004351520445197821\n",
      "training loss, Batch 72150: 0.004408416338264942\n",
      "training loss, Batch 72200: 0.003944017458707094\n",
      "training loss, Batch 72250: 0.0053397053852677345\n",
      "training loss, Batch 72300: 0.0047517018392682076\n",
      "training loss, Batch 72350: 0.0036652374546974897\n",
      "training loss, Batch 72400: 0.004088707268238068\n",
      "training loss, Batch 72450: 0.006320785265415907\n",
      "training loss, Batch 72500: 0.005593231879174709\n",
      "training loss, Batch 72550: 0.004063374362885952\n",
      "training loss, Batch 72600: 0.003998203203082085\n",
      "training loss, Batch 72650: 0.0029500836972147226\n",
      "training loss, Batch 72700: 0.006829812191426754\n",
      "training loss, Batch 72750: 0.0014693012926727533\n",
      "training loss, Batch 72800: 0.00473793875426054\n",
      "training loss, Batch 72850: 0.0053850822150707245\n",
      "training loss, Batch 72900: 0.005515198688954115\n",
      "training loss, Batch 72950: 0.004879472777247429\n",
      "training loss, Batch 73000: 0.004459328018128872\n",
      "training loss, Batch 73050: 0.003498663194477558\n",
      "training loss, Batch 73100: 0.005838276352733374\n",
      "training loss, Batch 73150: 0.0023484504781663418\n",
      "training loss, Batch 73200: 0.006310981698334217\n",
      "training loss, Batch 73250: 0.00589885376393795\n",
      "training loss, Batch 73300: 0.004282338544726372\n",
      "training loss, Batch 73350: 0.0036518173292279243\n",
      "training loss, Batch 73400: 0.0029878532513976097\n",
      "training loss, Batch 73450: 0.0037331259809434414\n",
      "training loss, Batch 73500: 0.0027470672503113747\n",
      "training loss, Batch 73550: 0.0021006485912948847\n",
      "training loss, Batch 73600: 0.0036667061503976583\n",
      "training loss, Batch 73650: 0.0034516621381044388\n",
      "training loss, Batch 73700: 0.008129964582622051\n",
      "training loss, Batch 73750: 0.002874238882213831\n",
      "training loss, Batch 73800: 0.0018763788975775242\n",
      "training loss, Batch 73850: 0.0026588039472699165\n",
      "training loss, Batch 73900: 0.005108671262860298\n",
      "training loss, Batch 73950: 0.0050176577642560005\n",
      "training loss, Batch 74000: 0.0032127252779901028\n",
      "training loss, Batch 74050: 0.0036010961048305035\n",
      "training loss, Batch 74100: 0.004297077655792236\n",
      "training loss, Batch 74150: 0.0044747693464159966\n",
      "training loss, Batch 74200: 0.003564074169844389\n",
      "training loss, Batch 74250: 0.004792059306055307\n",
      "training loss, Batch 74300: 0.0031738481484353542\n",
      "training loss, Batch 74350: 0.0028372362721711397\n",
      "training loss, Batch 74400: 0.0027198786847293377\n",
      "training loss, Batch 74450: 0.002749064937233925\n",
      "training loss, Batch 74500: 0.003003048710525036\n",
      "training loss, Batch 74550: 0.0029393923468887806\n",
      "training loss, Batch 74600: 0.006112192757427692\n",
      "training loss, Batch 74650: 0.00547653017565608\n",
      "training loss, Batch 74700: 0.0021908553317189217\n",
      "training loss, Batch 74750: 0.0022016798611730337\n",
      "training loss, Batch 74800: 0.0062231761403381824\n",
      "training loss, Batch 74850: 0.003980549983680248\n",
      "training loss, Batch 74900: 0.0020214843098074198\n",
      "training loss, Batch 74950: 0.003922799602150917\n",
      "training loss, Batch 75000: 0.0031573325395584106\n",
      "training loss, Batch 75050: 0.002416018396615982\n",
      "training loss, Batch 75100: 0.006401600781828165\n",
      "training loss, Batch 75150: 0.0031094772275537252\n",
      "training loss, Batch 75200: 0.0031967484392225742\n",
      "training loss, Batch 75250: 0.005248373374342918\n",
      "training loss, Batch 75300: 0.005689930636435747\n",
      "training loss, Batch 75350: 0.0058182962238788605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 75400: 0.002342087682336569\n",
      "training loss, Batch 75450: 0.006430222652852535\n",
      "training loss, Batch 75500: 0.004099898971617222\n",
      "training loss, Batch 75550: 0.004371586721390486\n",
      "training loss, Batch 75600: 0.005985279101878405\n",
      "training loss, Batch 75650: 0.003383682342246175\n",
      "training loss, Batch 75700: 0.006096102762967348\n",
      "training loss, Batch 75750: 0.004305298440158367\n",
      "training loss, Batch 75800: 0.004612617660313845\n",
      "training loss, Batch 75850: 0.003088835161179304\n",
      "training loss, Batch 75900: 0.0029143178835511208\n",
      "training loss, Batch 75950: 0.005182644352316856\n",
      "training loss, Batch 76000: 0.004231157246977091\n",
      "training loss, Batch 76050: 0.005643718875944614\n",
      "training loss, Batch 76100: 0.0024309824220836163\n",
      "training loss, Batch 76150: 0.0050589838065207005\n",
      "training loss, Batch 76200: 0.0033653853461146355\n",
      "training loss, Batch 76250: 0.004142321180552244\n",
      "training loss, Batch 76300: 0.006652764044702053\n",
      "training loss, Batch 76350: 0.002860147738829255\n",
      "training loss, Batch 76400: 0.003493374912068248\n",
      "training loss, Batch 76450: 0.005255933851003647\n",
      "training loss, Batch 76500: 0.010922029614448547\n",
      "training loss, Batch 76550: 0.0027520398143678904\n",
      "training loss, Batch 76600: 0.006015160121023655\n",
      "training loss, Batch 76650: 0.009093607775866985\n",
      "training loss, Batch 76700: 0.012151047587394714\n",
      "training loss, Batch 76750: 0.013269783928990364\n",
      "training loss, Batch 76800: 0.004735746420919895\n",
      "training loss, Batch 76850: 0.002353878691792488\n",
      "training loss, Batch 76900: 0.005362829193472862\n",
      "training loss, Batch 76950: 0.006447470746934414\n",
      "training loss, Batch 77000: 0.004371862858533859\n",
      "training loss, Batch 77050: 0.0055552637204527855\n",
      "training loss, Batch 77100: 0.0023804185912013054\n",
      "training loss, Batch 77150: 0.0017825494287535548\n",
      "training loss, Batch 77200: 0.004396525677293539\n",
      "training loss, Batch 77250: 0.006565555930137634\n",
      "training loss, Batch 77300: 0.004187213722616434\n",
      "training loss, Batch 77350: 0.009556816890835762\n",
      "training loss, Batch 77400: 0.003880669828504324\n",
      "training loss, Batch 77450: 0.0022000232711434364\n",
      "training loss, Batch 77500: 0.004442207980901003\n",
      "training loss, Batch 77550: 0.002460935153067112\n",
      "training loss, Batch 77600: 0.0019695917144417763\n",
      "training loss, Batch 77650: 0.004398453049361706\n",
      "training loss, Batch 77700: 0.006105490028858185\n",
      "training loss, Batch 77750: 0.004178760107606649\n",
      "training loss, Batch 77800: 0.008404618129134178\n",
      "training loss, Batch 77850: 0.0045872898772358894\n",
      "training loss, Batch 77900: 0.003367075463756919\n",
      "training loss, Batch 77950: 0.003950105980038643\n",
      "training loss, Batch 78000: 0.0019722545985132456\n",
      "training loss, Batch 78050: 0.004683283157646656\n",
      "training loss, Batch 78100: 0.0046953363344073296\n",
      "training loss, Batch 78150: 0.003922780975699425\n",
      "training loss, Batch 78200: 0.002924403641372919\n",
      "training loss, Batch 78250: 0.00887913629412651\n",
      "training loss, Batch 78300: 0.006290853023529053\n",
      "training loss, Batch 78350: 0.0028801513835787773\n",
      "training loss, Batch 78400: 0.008847682736814022\n",
      "training loss, Batch 78450: 0.0029284218326210976\n",
      "training loss, Batch 78500: 0.007262182421982288\n",
      "training loss, Batch 78550: 0.005036736838519573\n",
      "training loss, Batch 78600: 0.005778191611170769\n",
      "training loss, Batch 78650: 0.0058177318423986435\n",
      "training loss, Batch 78700: 0.004697172436863184\n",
      "training loss, Batch 78750: 0.004496495705097914\n",
      "training loss, Batch 78800: 0.002143074991181493\n",
      "training loss, Batch 78850: 0.004077791236341\n",
      "training loss, Batch 78900: 0.004735895432531834\n",
      "training loss, Batch 78950: 0.004276377148926258\n",
      "training loss, Batch 79000: 0.0020932783372700214\n",
      "training loss, Batch 79050: 0.004733347333967686\n",
      "training loss, Batch 79100: 0.00401361845433712\n",
      "training loss, Batch 79150: 0.003810132620856166\n",
      "training loss, Batch 79200: 0.006450671702623367\n",
      "training loss, Batch 79250: 0.011470893397927284\n",
      "training loss, Batch 79300: 0.0037518772296607494\n",
      "training loss, Batch 79350: 0.008551526814699173\n",
      "training loss, Batch 79400: 0.006223936565220356\n",
      "training loss, Batch 79450: 0.0029305098578333855\n",
      "training loss, Batch 79500: 0.0037116515450179577\n",
      "training loss, Batch 79550: 0.003591785905882716\n",
      "training loss, Batch 79600: 0.009239067323505878\n",
      "training loss, Batch 79650: 0.003419280517846346\n",
      "training loss, Batch 79700: 0.0028321854770183563\n",
      "training loss, Batch 79750: 0.003553213318809867\n",
      "training loss, Batch 79800: 0.00731234485283494\n",
      "training loss, Batch 79850: 0.0025644181296229362\n",
      "training loss, Batch 79900: 0.003949173726141453\n",
      "training loss, Batch 79950: 0.003966514486819506\n",
      "training loss, Batch 80000: 0.0016823592595756054\n",
      "training loss, Batch 80050: 0.004152513109147549\n",
      "training loss, Batch 80100: 0.0038899900391697884\n",
      "training loss, Batch 80150: 0.0038033644668757915\n",
      "training loss, Batch 80200: 0.003647567704319954\n",
      "training loss, Batch 80250: 0.0030120774172246456\n",
      "training loss, Batch 80300: 0.0034873918630182743\n",
      "training loss, Batch 80350: 0.0025995310861617327\n",
      "training loss, Batch 80400: 0.002834497019648552\n",
      "training loss, Batch 80450: 0.001679491950199008\n",
      "training loss, Batch 80500: 0.002696446143090725\n",
      "training loss, Batch 80550: 0.0038515222258865833\n",
      "training loss, Batch 80600: 0.004237647168338299\n",
      "training loss, Batch 80650: 0.0030218414030969143\n",
      "training loss, Batch 80700: 0.0041867466643452644\n",
      "training loss, Batch 80750: 0.004402356222271919\n",
      "training loss, Batch 80800: 0.00488482229411602\n",
      "training loss, Batch 80850: 0.006046427879482508\n",
      "training loss, Batch 80900: 0.007395371329039335\n",
      "training loss, Batch 80950: 0.0021947850473225117\n",
      "training loss, Batch 81000: 0.005448770243674517\n",
      "training loss, Batch 81050: 0.003180593252182007\n",
      "training loss, Batch 81100: 0.004526523873209953\n",
      "training loss, Batch 81150: 0.003961114212870598\n",
      "training loss, Batch 81200: 0.0037119067274034023\n",
      "training loss, Batch 81250: 0.005508743226528168\n",
      "training loss, Batch 81300: 0.00193052738904953\n",
      "training loss, Batch 81350: 0.0033102724701166153\n",
      "training loss, Batch 81400: 0.00382614741101861\n",
      "training loss, Batch 81450: 0.004756235051900148\n",
      "training loss, Batch 81500: 0.005858112126588821\n",
      "training loss, Batch 81550: 0.009776493534445763\n",
      "training loss, Batch 81600: 0.0041800737380981445\n",
      "training loss, Batch 81650: 0.00478278798982501\n",
      "training loss, Batch 81700: 0.005203220061957836\n",
      "training loss, Batch 81750: 0.005028583109378815\n",
      "training loss, Batch 81800: 0.004092080518603325\n",
      "training loss, Batch 81850: 0.004199853632599115\n",
      "training loss, Batch 81900: 0.004824248608201742\n",
      "training loss, Batch 81950: 0.0047670393250882626\n",
      "training loss, Batch 82000: 0.0062304912135005\n",
      "training loss, Batch 82050: 0.004000894259661436\n",
      "training loss, Batch 82100: 0.005237441975623369\n",
      "training loss, Batch 82150: 0.006965178996324539\n",
      "training loss, Batch 82200: 0.0030465598683804274\n",
      "training loss, Batch 82250: 0.0051056984812021255\n",
      "training loss, Batch 82300: 0.006151179783046246\n",
      "training loss, Batch 82350: 0.0027505336329340935\n",
      "training loss, Batch 82400: 0.01803443394601345\n",
      "training loss, Batch 82450: 0.003218660829588771\n",
      "training loss, Batch 82500: 0.005155336577445269\n",
      "training loss, Batch 82550: 0.0075644226744771\n",
      "training loss, Batch 82600: 0.00266994908452034\n",
      "training loss, Batch 82650: 0.00454344367608428\n",
      "training loss, Batch 82700: 0.005009945947676897\n",
      "training loss, Batch 82750: 0.0057891071774065495\n",
      "training loss, Batch 82800: 0.002135990187525749\n",
      "training loss, Batch 82850: 0.005046245642006397\n",
      "training loss, Batch 82900: 0.004404030740261078\n",
      "training loss, Batch 82950: 0.0036688626278191805\n",
      "training loss, Batch 83000: 0.005906348116695881\n",
      "training loss, Batch 83050: 0.009375529363751411\n",
      "training loss, Batch 83100: 0.0023936559446156025\n",
      "training loss, Batch 83150: 0.006091443821787834\n",
      "training loss, Batch 83200: 0.0020345994271337986\n",
      "training loss, Batch 83250: 0.009633516892790794\n",
      "training loss, Batch 83300: 0.002579193329438567\n",
      "training loss, Batch 83350: 0.0030285371467471123\n",
      "training loss, Batch 83400: 0.005122657865285873\n",
      "training loss, Batch 83450: 0.002394025446847081\n",
      "training loss, Batch 83500: 0.0047803521156311035\n",
      "training loss, Batch 83550: 0.002557530999183655\n",
      "training loss, Batch 83600: 0.00538519537076354\n",
      "training loss, Batch 83650: 0.003229502122849226\n",
      "training loss, Batch 83700: 0.012020385824143887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 83750: 0.0040305666625499725\n",
      "training loss, Batch 83800: 0.005241568200290203\n",
      "training loss, Batch 83850: 0.002453867346048355\n",
      "training loss, Batch 83900: 0.002873561345040798\n",
      "training loss, Batch 83950: 0.0034123000223189592\n",
      "training loss, Batch 84000: 0.005676247179508209\n",
      "training loss, Batch 84050: 0.0031702222768217325\n",
      "training loss, Batch 84100: 0.004454525653272867\n",
      "training loss, Batch 84150: 0.0036645729560405016\n",
      "training loss, Batch 84200: 0.0036019966937601566\n",
      "training loss, Batch 84250: 0.011837322264909744\n",
      "training loss, Batch 84300: 0.004198558162897825\n",
      "training loss, Batch 84350: 0.00287842471152544\n",
      "training loss, Batch 84400: 0.005346015561372042\n",
      "training loss, Batch 84450: 0.004824092146009207\n",
      "training loss, Batch 84500: 0.004057079553604126\n",
      "training loss, Batch 84550: 0.0049470714293420315\n",
      "training loss, Batch 84600: 0.0029455646872520447\n",
      "training loss, Batch 84650: 0.004208800382912159\n",
      "training loss, Batch 84700: 0.006033005192875862\n",
      "training loss, Batch 84750: 0.004895907826721668\n",
      "training loss, Batch 84800: 0.004028645809739828\n",
      "training loss, Batch 84850: 0.004287504591047764\n",
      "training loss, Batch 84900: 0.0041656941175460815\n",
      "training loss, Batch 84950: 0.004216963425278664\n",
      "training loss, Batch 85000: 0.0070986561477184296\n",
      "training loss, Batch 85050: 0.002906212117522955\n",
      "training loss, Batch 85100: 0.002855449914932251\n",
      "training loss, Batch 85150: 0.006810564547777176\n",
      "training loss, Batch 85200: 0.007011512294411659\n",
      "training loss, Batch 85250: 0.004729459993541241\n",
      "training loss, Batch 85300: 0.003359205089509487\n",
      "training loss, Batch 85350: 0.0022781570442020893\n",
      "training loss, Batch 85400: 0.004084986634552479\n",
      "training loss, Batch 85450: 0.0025457837618887424\n",
      "training loss, Batch 85500: 0.0026636479888111353\n",
      "training loss, Batch 85550: 0.0036371638998389244\n",
      "training loss, Batch 85600: 0.003956273198127747\n",
      "training loss, Batch 85650: 0.0021806787699460983\n",
      "training loss, Batch 85700: 0.004354175645858049\n",
      "training loss, Batch 85750: 0.003516983473673463\n",
      "training loss, Batch 85800: 0.004592089913785458\n",
      "training loss, Batch 85850: 0.003117274260148406\n",
      "training loss, Batch 85900: 0.0016042122151702642\n",
      "training loss, Batch 85950: 0.003719029948115349\n",
      "training loss, Batch 86000: 0.0034121633507311344\n",
      "training loss, Batch 86050: 0.0034887646324932575\n",
      "training loss, Batch 86100: 0.0016567015554755926\n",
      "training loss, Batch 86150: 0.003993343561887741\n",
      "training loss, Batch 86200: 0.0044930679723620415\n",
      "training loss, Batch 86250: 0.006508710794150829\n",
      "training loss, Batch 86300: 0.0027917432598769665\n",
      "training loss, Batch 86350: 0.005686149001121521\n",
      "training loss, Batch 86400: 0.0023818903136998415\n",
      "training loss, Batch 86450: 0.002605244517326355\n",
      "training loss, Batch 86500: 0.0041985115967690945\n",
      "training loss, Batch 86550: 0.0022005129139870405\n",
      "training loss, Batch 86600: 0.0035071708261966705\n",
      "training loss, Batch 86650: 0.007984497584402561\n",
      "training loss, Batch 86700: 0.004128524102270603\n",
      "training loss, Batch 86750: 0.00402761809527874\n",
      "training loss, Batch 86800: 0.0035485182888805866\n",
      "training loss, Batch 86850: 0.005905113648623228\n",
      "training loss, Batch 86900: 0.004138844087719917\n",
      "training loss, Batch 86950: 0.015082783997058868\n",
      "training loss, Batch 87000: 0.002128679770976305\n",
      "training loss, Batch 87050: 0.0034091980196535587\n",
      "training loss, Batch 87100: 0.008283916860818863\n",
      "training loss, Batch 87150: 0.007366920821368694\n",
      "training loss, Batch 87200: 0.0062300339341163635\n",
      "training loss, Batch 87250: 0.0023523326963186264\n",
      "training loss, Batch 87300: 0.0027569495141506195\n",
      "training loss, Batch 87350: 0.004482673481106758\n",
      "training loss, Batch 87400: 0.006000363267958164\n",
      "training loss, Batch 87450: 0.0023729875683784485\n",
      "training loss, Batch 87500: 0.005654175765812397\n",
      "training loss, Batch 87550: 0.0072054071351885796\n",
      "training loss, Batch 87600: 0.0028490889817476273\n",
      "training loss, Batch 87650: 0.003133563557639718\n",
      "training loss, Batch 87700: 0.005118886008858681\n",
      "training loss, Batch 87750: 0.0034973372239619493\n",
      "training loss, Batch 87800: 0.002820488763973117\n",
      "training loss, Batch 87850: 0.0038681214209645987\n",
      "training loss, Batch 87900: 0.005316779483109713\n",
      "training loss, Batch 87950: 0.0029962188564240932\n",
      "training loss, Batch 88000: 0.003118006745353341\n",
      "training loss, Batch 88050: 0.004910646006464958\n",
      "training loss, Batch 88100: 0.0016304438468068838\n",
      "training loss, Batch 88150: 0.004954637959599495\n",
      "training loss, Batch 88200: 0.004759537056088448\n",
      "training loss, Batch 88250: 0.0035614892840385437\n",
      "training loss, Batch 88300: 0.005317274481058121\n",
      "training loss, Batch 88350: 0.003570693777874112\n",
      "training loss, Batch 88400: 0.0027231071144342422\n",
      "training loss, Batch 88450: 0.005826863925904036\n",
      "training loss, Batch 88500: 0.006448677275329828\n",
      "training loss, Batch 88550: 0.003160451538860798\n",
      "training loss, Batch 88600: 0.004098573699593544\n",
      "training loss, Batch 88650: 0.0029747141525149345\n",
      "training loss, Batch 88700: 0.012495226226747036\n",
      "training loss, Batch 88750: 0.00664717610925436\n",
      "training loss, Batch 88800: 0.004086919128894806\n",
      "training loss, Batch 88850: 0.0035896175540983677\n",
      "training loss, Batch 88900: 0.0015809796750545502\n",
      "training loss, Batch 88950: 0.004522594623267651\n",
      "training loss, Batch 89000: 0.002146574202924967\n",
      "training loss, Batch 89050: 0.0019459256436675787\n",
      "training loss, Batch 89100: 0.00305106770247221\n",
      "training loss, Batch 89150: 0.0046798791736364365\n",
      "training loss, Batch 89200: 0.004057713784277439\n",
      "training loss, Batch 89250: 0.005951078608632088\n",
      "training loss, Batch 89300: 0.0016596957575529814\n",
      "training loss, Batch 89350: 0.0016250840853899717\n",
      "training loss, Batch 89400: 0.002964540384709835\n",
      "training loss, Batch 89450: 0.00405039731413126\n",
      "training loss, Batch 89500: 0.0050499774515628815\n",
      "training loss, Batch 89550: 0.007909062318503857\n",
      "training loss, Batch 89600: 0.004229101352393627\n",
      "training loss, Batch 89650: 0.005770073737949133\n",
      "training loss, Batch 89700: 0.002778335940092802\n",
      "training loss, Batch 89750: 0.0053717344999313354\n",
      "training loss, Batch 89800: 0.002930115442723036\n",
      "training loss, Batch 89850: 0.003982169553637505\n",
      "training loss, Batch 89900: 0.005567849613726139\n",
      "training loss, Batch 89950: 0.0035501420497894287\n",
      "training loss, Batch 90000: 0.0022830984089523554\n",
      "training loss, Batch 90050: 0.004926275461912155\n",
      "training loss, Batch 90100: 0.005787364207208157\n",
      "training loss, Batch 90150: 0.005477965343743563\n",
      "training loss, Batch 90200: 0.01419784128665924\n",
      "training loss, Batch 90250: 0.003198404796421528\n",
      "training loss, Batch 90300: 0.003687518183141947\n",
      "training loss, Batch 90350: 0.005944880656898022\n",
      "training loss, Batch 90400: 0.004191530868411064\n",
      "training loss, Batch 90450: 0.003202567808330059\n",
      "training loss, Batch 90500: 0.0031377822160720825\n",
      "training loss, Batch 90550: 0.003945180214941502\n",
      "training loss, Batch 90600: 0.0028926958329975605\n",
      "training loss, Batch 90650: 0.0034452767577022314\n",
      "training loss, Batch 90700: 0.005046983249485493\n",
      "training loss, Batch 90750: 0.0033524353057146072\n",
      "training loss, Batch 90800: 0.003561264369636774\n",
      "training loss, Batch 90850: 0.0053134942427277565\n",
      "training loss, Batch 90900: 0.003210926428437233\n",
      "training loss, Batch 90950: 0.004172950051724911\n",
      "training loss, Batch 91000: 0.010528572835028172\n",
      "training loss, Batch 91050: 0.0064629968255758286\n",
      "training loss, Batch 91100: 0.007488748989999294\n",
      "training loss, Batch 91150: 0.004645698703825474\n",
      "training loss, Batch 91200: 0.005386432632803917\n",
      "training loss, Batch 91250: 0.0069400048814713955\n",
      "training loss, Batch 91300: 0.006253335624933243\n",
      "training loss, Batch 91350: 0.00310659222304821\n",
      "training loss, Batch 91400: 0.001713556470349431\n",
      "training loss, Batch 91450: 0.004050537943840027\n",
      "training loss, Batch 91500: 0.005348650272935629\n",
      "training loss, Batch 91550: 0.009001814760267735\n",
      "training loss, Batch 91600: 0.005334607325494289\n",
      "training loss, Batch 91650: 0.0034975006710737944\n",
      "training loss, Batch 91700: 0.004413586109876633\n",
      "training loss, Batch 91750: 0.0035055801272392273\n",
      "training loss, Batch 91800: 0.002528469543904066\n",
      "training loss, Batch 91850: 0.0019430876709520817\n",
      "training loss, Batch 91900: 0.0039933668449521065\n",
      "training loss, Batch 91950: 0.004500214010477066\n",
      "training loss, Batch 92000: 0.006902938708662987\n",
      "training loss, Batch 92050: 0.002697765128687024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 92100: 0.005430976860225201\n",
      "training loss, Batch 92150: 0.0018593105487525463\n",
      "training loss, Batch 92200: 0.0025424037594348192\n",
      "training loss, Batch 92250: 0.004733245354145765\n",
      "training loss, Batch 92300: 0.005415380001068115\n",
      "training loss, Batch 92350: 0.004991983994841576\n",
      "training loss, Batch 92400: 0.003934022039175034\n",
      "training loss, Batch 92450: 0.002325948793441057\n",
      "training loss, Batch 92500: 0.006209010258316994\n",
      "training loss, Batch 92550: 0.005418625194579363\n",
      "training loss, Batch 92600: 0.017744002863764763\n",
      "training loss, Batch 92650: 0.005818889010697603\n",
      "training loss, Batch 92700: 0.002565990900620818\n",
      "training loss, Batch 92750: 0.00514376861974597\n",
      "training loss, Batch 92800: 0.004447779152542353\n",
      "training loss, Batch 92850: 0.003268027910962701\n",
      "training loss, Batch 92900: 0.00916452519595623\n",
      "training loss, Batch 92950: 0.0038418022450059652\n",
      "training loss, Batch 93000: 0.0030148527584969997\n",
      "training loss, Batch 93050: 0.0030161840841174126\n",
      "training loss, Batch 93100: 0.00629983376711607\n",
      "training loss, Batch 93150: 0.003014958929270506\n",
      "training loss, Batch 93200: 0.006971134804189205\n",
      "training loss, Batch 93250: 0.003983712289482355\n",
      "training loss, Batch 93300: 0.007662611082196236\n",
      "training loss, Batch 93350: 0.0035772165283560753\n",
      "training loss, Batch 93400: 0.00244327075779438\n",
      "training loss, Batch 93450: 0.004423514008522034\n",
      "training loss, Batch 93500: 0.0053117587231099606\n",
      "training loss, Batch 93550: 0.003687524236738682\n",
      "training loss, Batch 93600: 0.005598703399300575\n",
      "training loss, Batch 93650: 0.004921022802591324\n",
      "training loss, Batch 93700: 0.005286064930260181\n",
      "training loss, Batch 93750: 0.0030213419813662767\n",
      "training loss, Batch 93800: 0.005271059926599264\n",
      "training loss, Batch 93850: 0.005508519243448973\n",
      "training loss, Batch 93900: 0.0025031594559550285\n",
      "training loss, Batch 93950: 0.006642710417509079\n",
      "training loss, Batch 94000: 0.005211314186453819\n",
      "training loss, Batch 94050: 0.005942568648606539\n",
      "training loss, Batch 94100: 0.004211212508380413\n",
      "training loss, Batch 94150: 0.0017673338297754526\n",
      "training loss, Batch 94200: 0.003377453191205859\n",
      "training loss, Batch 94250: 0.00573320034891367\n",
      "training loss, Batch 94300: 0.002208688762038946\n",
      "training loss, Batch 94350: 0.004900767467916012\n",
      "training loss, Batch 94400: 0.004707221873104572\n",
      "training loss, Batch 94450: 0.00626763142645359\n",
      "training loss, Batch 94500: 0.00218455633148551\n",
      "training loss, Batch 94550: 0.005579263437539339\n",
      "training loss, Batch 94600: 0.004148862790316343\n",
      "training loss, Batch 94650: 0.00498555414378643\n",
      "training loss, Batch 94700: 0.004556267522275448\n",
      "training loss, Batch 94750: 0.00838669016957283\n",
      "training loss, Batch 94800: 0.010564680211246014\n",
      "training loss, Batch 94850: 0.004641609266400337\n",
      "training loss, Batch 94900: 0.00436624838039279\n",
      "training loss, Batch 94950: 0.0037535151932388544\n",
      "training loss, Batch 95000: 0.0026948063168674707\n",
      "training loss, Batch 95050: 0.004238071385771036\n",
      "training loss, Batch 95100: 0.005706828087568283\n",
      "training loss, Batch 95150: 0.002347987610846758\n",
      "training loss, Batch 95200: 0.003746633417904377\n",
      "training loss, Batch 95250: 0.006426762789487839\n",
      "training loss, Batch 95300: 0.003047313541173935\n",
      "training loss, Batch 95350: 0.0038432301953434944\n",
      "training loss, Batch 95400: 0.0043756491504609585\n",
      "training loss, Batch 95450: 0.004630495794117451\n",
      "training loss, Batch 95500: 0.007545361761003733\n",
      "training loss, Batch 95550: 0.004813212435692549\n",
      "training loss, Batch 95600: 0.005426260642707348\n",
      "training loss, Batch 95650: 0.002557342639192939\n",
      "training loss, Batch 95700: 0.0032585393637418747\n",
      "training loss, Batch 95750: 0.003585530910640955\n",
      "training loss, Batch 95800: 0.004254103638231754\n",
      "training loss, Batch 95850: 0.004530947655439377\n",
      "training loss, Batch 95900: 0.0037321290001273155\n",
      "training loss, Batch 95950: 0.004364439286291599\n",
      "training loss, Batch 96000: 0.0037449845112860203\n",
      "training loss, Batch 96050: 0.005722975358366966\n",
      "training loss, Batch 96100: 0.0065307896584272385\n",
      "training loss, Batch 96150: 0.009114610962569714\n",
      "training loss, Batch 96200: 0.004025325179100037\n",
      "training loss, Batch 96250: 0.003362220013514161\n",
      "training loss, Batch 96300: 0.00579051161184907\n",
      "training loss, Batch 96350: 0.0031667654402554035\n",
      "training loss, Batch 96400: 0.00416067847982049\n",
      "training loss, Batch 96450: 0.004778744652867317\n",
      "training loss, Batch 96500: 0.0063255601562559605\n",
      "training loss, Batch 96550: 0.005039985757321119\n",
      "training loss, Batch 96600: 0.006584210321307182\n",
      "training loss, Batch 96650: 0.003578647505491972\n",
      "training loss, Batch 96700: 0.008379840292036533\n",
      "training loss, Batch 96750: 0.0028074742294847965\n",
      "training loss, Batch 96800: 0.007072288542985916\n",
      "training loss, Batch 96850: 0.00228583044372499\n",
      "training loss, Batch 96900: 0.0032994579523801804\n",
      "training loss, Batch 96950: 0.004251634236425161\n",
      "training loss, Batch 97000: 0.0025827265344560146\n",
      "training loss, Batch 97050: 0.0029156021773815155\n",
      "training loss, Batch 97100: 0.002721104072406888\n",
      "training loss, Batch 97150: 0.002521487884223461\n",
      "training loss, Batch 97200: 0.0039047980681061745\n",
      "training loss, Batch 97250: 0.00362166715785861\n",
      "training loss, Batch 97300: 0.0062944767996668816\n",
      "training loss, Batch 97350: 0.009357014670968056\n",
      "training loss, Batch 97400: 0.015039805322885513\n",
      "training loss, Batch 97450: 0.006207101047039032\n",
      "training loss, Batch 97500: 0.0020770621486008167\n",
      "training loss, Batch 97550: 0.003778587793931365\n",
      "training loss, Batch 97600: 0.009837917052209377\n",
      "training loss, Batch 97650: 0.00392839964479208\n",
      "training loss, Batch 97700: 0.002981895813718438\n",
      "training loss, Batch 97750: 0.004277052357792854\n",
      "training loss, Batch 97800: 0.0056323884055018425\n",
      "training loss, Batch 97850: 0.0025107257533818483\n",
      "training loss, Batch 97900: 0.00495175551623106\n",
      "training loss, Batch 97950: 0.0053361933678388596\n",
      "training loss, Batch 98000: 0.004054277203977108\n",
      "training loss, Batch 98050: 0.007119597867131233\n",
      "training loss, Batch 98100: 0.0035302515607327223\n",
      "training loss, Batch 98150: 0.0037866514176130295\n",
      "training loss, Batch 98200: 0.003500323975458741\n",
      "training loss, Batch 98250: 0.004876825492829084\n",
      "training loss, Batch 98300: 0.004387891851365566\n",
      "training loss, Batch 98350: 0.003069870639592409\n",
      "training loss, Batch 98400: 0.0038530277088284492\n",
      "training loss, Batch 98450: 0.005911743268370628\n",
      "training loss, Batch 98500: 0.004696164280176163\n",
      "training loss, Batch 98550: 0.007467473857104778\n",
      "training loss, Batch 98600: 0.007407801225781441\n",
      "training loss, Batch 98650: 0.007257305085659027\n",
      "training loss, Batch 98700: 0.005168952979147434\n",
      "training loss, Batch 98750: 0.0038005514070391655\n",
      "training loss, Batch 98800: 0.006357857957482338\n",
      "training loss, Batch 98850: 0.005201526917517185\n",
      "training loss, Batch 98900: 0.002761343028396368\n",
      "training loss, Batch 98950: 0.010244544595479965\n",
      "training loss, Batch 99000: 0.013348495587706566\n",
      "training loss, Batch 99050: 0.003662804141640663\n",
      "training loss, Batch 99100: 0.007670168299227953\n",
      "training loss, Batch 99150: 0.003826854284852743\n",
      "training loss, Batch 99200: 0.002773465821519494\n",
      "training loss, Batch 99250: 0.0037178306374698877\n",
      "training loss, Batch 99300: 0.004607000853866339\n",
      "training loss, Batch 99350: 0.0054042828269302845\n",
      "training loss, Batch 99400: 0.004656612873077393\n",
      "training loss, Batch 99450: 0.0033099413849413395\n",
      "training loss, Batch 99500: 0.003875680733472109\n",
      "training loss, Batch 99550: 0.003741993336006999\n",
      "training loss, Batch 99600: 0.004222331568598747\n",
      "training loss, Batch 99650: 0.004318935796618462\n",
      "training loss, Batch 99700: 0.004010469652712345\n",
      "training loss, Batch 99750: 0.0019902593921869993\n",
      "training loss, Batch 99800: 0.005376502871513367\n",
      "training loss, Batch 99850: 0.002698078751564026\n",
      "training loss, Batch 99900: 0.002733110450208187\n",
      "training loss, Batch 99950: 0.004596072714775801\n",
      "training loss, Batch 100000: 0.0020234622061252594\n",
      "training loss, Batch 100050: 0.006067819893360138\n",
      "training loss, Batch 100100: 0.010212652385234833\n",
      "training loss, Batch 100150: 0.0019857031293213367\n",
      "training loss, Batch 100200: 0.00518384762108326\n",
      "training loss, Batch 100250: 0.0030672664288431406\n",
      "training loss, Batch 100300: 0.004069552756845951\n",
      "training loss, Batch 100350: 0.0020535248331725597\n",
      "training loss, Batch 100400: 0.00441973889246583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 100450: 0.005423693917691708\n",
      "training loss, Batch 100500: 0.012357826344668865\n",
      "training loss, Batch 100550: 0.0030567136127501726\n",
      "training loss, Batch 100600: 0.0036703438963741064\n",
      "training loss, Batch 100650: 0.0041164858266711235\n",
      "training loss, Batch 100700: 0.005026074126362801\n",
      "training loss, Batch 100750: 0.0044794632121920586\n",
      "training loss, Batch 100800: 0.005133605562150478\n",
      "training loss, Batch 100850: 0.0041051870211958885\n",
      "training loss, Batch 100900: 0.0029852786101400852\n",
      "training loss, Batch 100950: 0.005341945681720972\n",
      "training loss, Batch 101000: 0.0019107921980321407\n",
      "training loss, Batch 101050: 0.0034851129166781902\n",
      "training loss, Batch 101100: 0.007363305427134037\n",
      "training loss, Batch 101150: 0.007744849659502506\n",
      "training loss, Batch 101200: 0.007423055358231068\n",
      "training loss, Batch 101250: 0.003060048446059227\n",
      "training loss, Batch 101300: 0.0061973268166184425\n",
      "training loss, Batch 101350: 0.0038140788674354553\n",
      "training loss, Batch 101400: 0.003154925536364317\n",
      "training loss, Batch 101450: 0.005279761739075184\n",
      "training loss, Batch 101500: 0.0047615221701562405\n",
      "training loss, Batch 101550: 0.004352283664047718\n",
      "training loss, Batch 101600: 0.005312385968863964\n",
      "training loss, Batch 101650: 0.008418469689786434\n",
      "training loss, Batch 101700: 0.009195392020046711\n",
      "training loss, Batch 101750: 0.006895166821777821\n",
      "training loss, Batch 101800: 0.0036937068216502666\n",
      "training loss, Batch 101850: 0.008316555060446262\n",
      "training loss, Batch 101900: 0.0033996151760220528\n",
      "training loss, Batch 101950: 0.006230228580534458\n",
      "training loss, Batch 102000: 0.003014377551153302\n",
      "training loss, Batch 102050: 0.004737613257020712\n",
      "training loss, Batch 102100: 0.005103207193315029\n",
      "training loss, Batch 102150: 0.012390844523906708\n",
      "training loss, Batch 102200: 0.0032114938367158175\n",
      "training loss, Batch 102250: 0.01123715564608574\n",
      "training loss, Batch 102300: 0.0035017579793930054\n",
      "training loss, Batch 102350: 0.005679338704794645\n",
      "training loss, Batch 102400: 0.0034170786384493113\n",
      "training loss, Batch 102450: 0.0038747969083487988\n",
      "training loss, Batch 102500: 0.0015797681408002973\n",
      "training loss, Batch 102550: 0.00394099485129118\n",
      "training loss, Batch 102600: 0.008813520893454552\n",
      "training loss, Batch 102650: 0.0033910502679646015\n",
      "training loss, Batch 102700: 0.004674828611314297\n",
      "training loss, Batch 102750: 0.00433237012475729\n",
      "training loss, Batch 102800: 0.003795644734054804\n",
      "training loss, Batch 102850: 0.005765034817159176\n",
      "training loss, Batch 102900: 0.004523439332842827\n",
      "training loss, Batch 102950: 0.005793135613203049\n",
      "training loss, Batch 103000: 0.0030176579020917416\n",
      "training loss, Batch 103050: 0.00535884965211153\n",
      "training loss, Batch 103100: 0.005229768343269825\n",
      "training loss, Batch 103150: 0.0024789609014987946\n",
      "training loss, Batch 103200: 0.004103397950530052\n",
      "training loss, Batch 103250: 0.0031019803136587143\n",
      "training loss, Batch 103300: 0.00346264336258173\n",
      "training loss, Batch 103350: 0.009693167172372341\n",
      "training loss, Batch 103400: 0.00809873454272747\n",
      "training loss, Batch 103450: 0.00418971199542284\n",
      "training loss, Batch 103500: 0.008141618221998215\n",
      "training loss, Batch 103550: 0.002794842701405287\n",
      "training loss, Batch 103600: 0.0019362469902262092\n",
      "training loss, Batch 103650: 0.002773330546915531\n",
      "training loss, Batch 103700: 0.00537217129021883\n",
      "training loss, Batch 103750: 0.005439555272459984\n",
      "training loss, Batch 103800: 0.003683077869936824\n",
      "training loss, Batch 103850: 0.005798839032649994\n",
      "training loss, Batch 103900: 0.0036825789138674736\n",
      "training loss, Batch 103950: 0.003143362468108535\n",
      "training loss, Batch 104000: 0.004080201964825392\n",
      "training loss, Batch 104050: 0.002378680743277073\n",
      "training loss, Batch 104100: 0.004370904061943293\n",
      "training loss, Batch 104150: 0.005780866369605064\n",
      "training loss, Batch 104200: 0.004581235349178314\n",
      "training loss, Batch 104250: 0.0053268964402377605\n",
      "training loss, Batch 104300: 0.009423727169632912\n",
      "training loss, Batch 104350: 0.0035222670994699\n",
      "training loss, Batch 104400: 0.004689967259764671\n",
      "training loss, Batch 104450: 0.005672423169016838\n",
      "training loss, Batch 104500: 0.0022737591061741114\n",
      "training loss, Batch 104550: 0.003080992493778467\n",
      "training loss, Batch 104600: 0.005016258917748928\n",
      "training loss, Batch 104650: 0.00429296400398016\n",
      "training loss, Batch 104700: 0.006078907288610935\n",
      "training loss, Batch 104750: 0.0039128270000219345\n",
      "training loss, Batch 104800: 0.00372030190192163\n",
      "training loss, Batch 104850: 0.003098921151831746\n",
      "training loss, Batch 104900: 0.003976040054112673\n",
      "training loss, Batch 104950: 0.003275550901889801\n",
      "training loss, Batch 105000: 0.00797240436077118\n",
      "training loss, Batch 105050: 0.007687477394938469\n",
      "training loss, Batch 105100: 0.0032920786179602146\n",
      "training loss, Batch 105150: 0.00564234284684062\n",
      "training loss, Batch 105200: 0.0037926880177110434\n",
      "training loss, Batch 105250: 0.003105658106505871\n",
      "training loss, Batch 105300: 0.0049532316625118256\n",
      "training loss, Batch 105350: 0.008727540262043476\n",
      "training loss, Batch 105400: 0.003118369961157441\n",
      "training loss, Batch 105450: 0.0025416850112378597\n",
      "training loss, Batch 105500: 0.003291570581495762\n",
      "training loss, Batch 105550: 0.01793256774544716\n",
      "training loss, Batch 105600: 0.00603092648088932\n",
      "training loss, Batch 105650: 0.003270601388067007\n",
      "training loss, Batch 105700: 0.0037647841963917017\n",
      "training loss, Batch 105750: 0.0052066706120967865\n",
      "training loss, Batch 105800: 0.00431521562859416\n",
      "training loss, Batch 105850: 0.00545469019562006\n",
      "training loss, Batch 105900: 0.003963059280067682\n",
      "training loss, Batch 105950: 0.004057042300701141\n",
      "training loss, Batch 106000: 0.005598593037575483\n",
      "training loss, Batch 106050: 0.00453944131731987\n",
      "training loss, Batch 106100: 0.010377757251262665\n",
      "training loss, Batch 106150: 0.002543774899095297\n",
      "training loss, Batch 106200: 0.004589248448610306\n",
      "training loss, Batch 106250: 0.007414727471768856\n",
      "training loss, Batch 106300: 0.006369754672050476\n",
      "training loss, Batch 106350: 0.007418423425406218\n",
      "training loss, Batch 106400: 0.005510756280273199\n",
      "training loss, Batch 106450: 0.004686516709625721\n",
      "training loss, Batch 106500: 0.002208512742072344\n",
      "training loss, Batch 106550: 0.004003981593996286\n",
      "training loss, Batch 106600: 0.004801101051270962\n",
      "training loss, Batch 106650: 0.008250079117715359\n",
      "training loss, Batch 106700: 0.003107916098088026\n",
      "training loss, Batch 106750: 0.004382384475320578\n",
      "training loss, Batch 106800: 0.0037260237149894238\n",
      "training loss, Batch 106850: 0.0038276617415249348\n",
      "training loss, Batch 106900: 0.0033816234208643436\n",
      "training loss, Batch 106950: 0.008939811959862709\n",
      "training loss, Batch 107000: 0.006260014604777098\n",
      "training loss, Batch 107050: 0.00335881975479424\n",
      "training loss, Batch 107100: 0.00332537479698658\n",
      "training loss, Batch 107150: 0.003800191916525364\n",
      "training loss, Batch 107200: 0.0054100085981190205\n",
      "training loss, Batch 107250: 0.007279530167579651\n",
      "training loss, Batch 107300: 0.006600931752473116\n",
      "training loss, Batch 107350: 0.004477620590478182\n",
      "training loss, Batch 107400: 0.0018790502799674869\n",
      "training loss, Batch 107450: 0.003965415060520172\n",
      "training loss, Batch 107500: 0.00317959557287395\n",
      "training loss, Batch 107550: 0.004567504860460758\n",
      "training loss, Batch 107600: 0.00423760712146759\n",
      "training loss, Batch 107650: 0.003904575016349554\n",
      "training loss, Batch 107700: 0.00803632102906704\n",
      "training loss, Batch 107750: 0.0057501280680298805\n",
      "training loss, Batch 107800: 0.0039069075137376785\n",
      "training loss, Batch 107850: 0.00447219330817461\n",
      "training loss, Batch 107900: 0.004124183673411608\n",
      "training loss, Batch 107950: 0.0026308961678296328\n",
      "training loss, Batch 108000: 0.0051575880497694016\n",
      "training loss, Batch 108050: 0.0043153222650289536\n",
      "training loss, Batch 108100: 0.0025940220803022385\n",
      "training loss, Batch 108150: 0.00416781660169363\n",
      "training loss, Batch 108200: 0.001718215411528945\n",
      "training loss, Batch 108250: 0.007272202521562576\n",
      "training loss, Batch 108300: 0.006429237779229879\n",
      "training loss, Batch 108350: 0.017140688374638557\n",
      "training loss, Batch 108400: 0.00336132338270545\n",
      "training loss, Batch 108450: 0.005421987734735012\n",
      "training loss, Batch 108500: 0.003253000555559993\n",
      "training loss, Batch 108550: 0.003486103378236294\n",
      "training loss, Batch 108600: 0.0016413787379860878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 108650: 0.004341698717325926\n",
      "training loss, Batch 108700: 0.0031434278935194016\n",
      "training loss, Batch 108750: 0.002682336140424013\n",
      "training loss, Batch 108800: 0.004618508275598288\n",
      "training loss, Batch 108850: 0.004493852145969868\n",
      "training loss, Batch 108900: 0.005038408562541008\n",
      "training loss, Batch 108950: 0.007254820317029953\n",
      "training loss, Batch 109000: 0.00395355187356472\n",
      "training loss, Batch 109050: 0.0017991713248193264\n",
      "training loss, Batch 109100: 0.004068957641720772\n",
      "training loss, Batch 109150: 0.003095801919698715\n",
      "training loss, Batch 109200: 0.0043366458266973495\n",
      "training loss, Batch 109250: 0.0010147604625672102\n",
      "training loss, Batch 109300: 0.0035622476134449244\n",
      "training loss, Batch 109350: 0.007113432511687279\n",
      "training loss, Batch 109400: 0.0064117396250367165\n",
      "training loss, Batch 109450: 0.005898171104490757\n",
      "training loss, Batch 109500: 0.0016516605392098427\n",
      "training loss, Batch 109550: 0.0030347006395459175\n",
      "training loss, Batch 109600: 0.004048400558531284\n",
      "training loss, Batch 109650: 0.0034695349168032408\n",
      "training loss, Batch 109700: 0.004011653363704681\n",
      "training loss, Batch 109750: 0.0030335995834320784\n",
      "training loss, Batch 109800: 0.00343871908262372\n",
      "training loss, Batch 109850: 0.00442070746794343\n",
      "training loss, Batch 109900: 0.0047390516847372055\n",
      "training loss, Batch 109950: 0.0040214890614151955\n",
      "training loss, Batch 110000: 0.0026394780725240707\n",
      "training loss, Batch 110050: 0.0038709829095751047\n",
      "training loss, Batch 110100: 0.004259680863469839\n",
      "training loss, Batch 110150: 0.002301696687936783\n",
      "training loss, Batch 110200: 0.005450836848467588\n",
      "training loss, Batch 110250: 0.006837099324911833\n",
      "training loss, Batch 110300: 0.007840078324079514\n",
      "training loss, Batch 110350: 0.004574509337544441\n",
      "training loss, Batch 110400: 0.002479718066751957\n",
      "training loss, Batch 110450: 0.006360631436109543\n",
      "training loss, Batch 110500: 0.009974367916584015\n",
      "training loss, Batch 110550: 0.005562398117035627\n",
      "training loss, Batch 110600: 0.0029095993377268314\n",
      "training loss, Batch 110650: 0.0037229203153401613\n",
      "training loss, Batch 110700: 0.0038987603038549423\n",
      "training loss, Batch 110750: 0.005684277042746544\n",
      "training loss, Batch 110800: 0.0025757639668881893\n",
      "training loss, Batch 110850: 0.0047532650642097\n",
      "training loss, Batch 110900: 0.003107841592282057\n",
      "training loss, Batch 110950: 0.002749629318714142\n",
      "training loss, Batch 111000: 0.003960859030485153\n",
      "training loss, Batch 111050: 0.0025802413001656532\n",
      "training loss, Batch 111100: 0.0033714408054947853\n",
      "training loss, Batch 111150: 0.005262901075184345\n",
      "training loss, Batch 111200: 0.0037999891210347414\n",
      "training loss, Batch 111250: 0.00584234856069088\n",
      "training loss, Batch 111300: 0.0041750287637114525\n",
      "training loss, Batch 111350: 0.0034891213290393353\n",
      "training loss, Batch 111400: 0.0021394032519310713\n",
      "training loss, Batch 111450: 0.003758946666494012\n",
      "training loss, Batch 111500: 0.0038999475073069334\n",
      "training loss, Batch 111550: 0.004771952051669359\n",
      "training loss, Batch 111600: 0.007225812412798405\n",
      "training loss, Batch 111650: 0.003769423346966505\n",
      "training loss, Batch 111700: 0.004019943997263908\n",
      "training loss, Batch 111750: 0.0029970714822411537\n",
      "training loss, Batch 111800: 0.004885885398834944\n",
      "training loss, Batch 111850: 0.0041243573650717735\n",
      "training loss, Batch 111900: 0.004152294714003801\n",
      "training loss, Batch 111950: 0.003398744622245431\n",
      "training loss, Batch 112000: 0.006479473784565926\n",
      "training loss, Batch 112050: 0.0035009535495191813\n",
      "training loss, Batch 112100: 0.0034214332699775696\n",
      "training loss, Batch 112150: 0.005312107503414154\n",
      "training loss, Batch 112200: 0.0067807892337441444\n",
      "training loss, Batch 112250: 0.007458685897290707\n",
      "training loss, Batch 112300: 0.00409949803724885\n",
      "training loss, Batch 112350: 0.008515181951224804\n",
      "training loss, Batch 112400: 0.0027578948065638542\n",
      "training loss, Batch 112450: 0.0015846522292122245\n",
      "training loss, Batch 112500: 0.0031844330951571465\n",
      "training loss, Batch 112550: 0.002271036384627223\n",
      "training loss, Batch 112600: 0.00291764666326344\n",
      "training loss, Batch 112650: 0.008227010257542133\n",
      "training loss, Batch 112700: 0.002020599553361535\n",
      "training loss, Batch 112750: 0.0038265609182417393\n",
      "training loss, Batch 112800: 0.0036002134438604116\n",
      "training loss, Batch 112850: 0.0035877821501344442\n",
      "training loss, Batch 112900: 0.0029427665285766125\n",
      "training loss, Batch 112950: 0.0014499537646770477\n",
      "training loss, Batch 113000: 0.003284308360889554\n",
      "training loss, Batch 113050: 0.0031325824093073606\n",
      "training loss, Batch 113100: 0.0036930504720658064\n",
      "training loss, Batch 113150: 0.003882693825289607\n",
      "training loss, Batch 113200: 0.004660968203097582\n",
      "training loss, Batch 113250: 0.003490030998364091\n",
      "training loss, Batch 113300: 0.0038505105767399073\n",
      "training loss, Batch 113350: 0.002696179784834385\n",
      "training loss, Batch 113400: 0.00146942725405097\n",
      "training loss, Batch 113450: 0.005737088620662689\n",
      "training loss, Batch 113500: 0.003876208793371916\n",
      "training loss, Batch 113550: 0.01211919728666544\n",
      "training loss, Batch 113600: 0.0022913385182619095\n",
      "training loss, Batch 113650: 0.004859615582972765\n",
      "training loss, Batch 113700: 0.0017964696744456887\n",
      "training loss, Batch 113750: 0.005095675587654114\n",
      "training loss, Batch 113800: 0.001910551218315959\n",
      "training loss, Batch 113850: 0.005843694321811199\n",
      "training loss, Batch 113900: 0.002722494537010789\n",
      "training loss, Batch 113950: 0.0028159942012280226\n",
      "training loss, Batch 114000: 0.008415920659899712\n",
      "training loss, Batch 114050: 0.0069132172502577305\n",
      "training loss, Batch 114100: 0.0025469069369137287\n",
      "training loss, Batch 114150: 0.006880446337163448\n",
      "training loss, Batch 114200: 0.00415333965793252\n",
      "training loss, Batch 114250: 0.004165432881563902\n",
      "training loss, Batch 114300: 0.007429944351315498\n",
      "training loss, Batch 114350: 0.004521381575614214\n",
      "training loss, Batch 114400: 0.0026345052756369114\n",
      "training loss, Batch 114450: 0.004444625228643417\n",
      "training loss, Batch 114500: 0.0016919630579650402\n",
      "training loss, Batch 114550: 0.00382592692039907\n",
      "training loss, Batch 114600: 0.003344619181007147\n",
      "training loss, Batch 114650: 0.0033253729343414307\n",
      "training loss, Batch 114700: 0.0044133830815553665\n",
      "training loss, Batch 114750: 0.002874834928661585\n",
      "training loss, Batch 114800: 0.005573347210884094\n",
      "training loss, Batch 114850: 0.0027335942722857\n",
      "training loss, Batch 114900: 0.003010934218764305\n",
      "training loss, Batch 114950: 0.0033087115734815598\n",
      "training loss, Batch 115000: 0.004451880231499672\n",
      "training loss, Batch 115050: 0.0033833375200629234\n",
      "training loss, Batch 115100: 0.002705814316868782\n",
      "training loss, Batch 115150: 0.003132038749754429\n",
      "training loss, Batch 115200: 0.0040239738300442696\n",
      "training loss, Batch 115250: 0.0021111182868480682\n",
      "training loss, Batch 115300: 0.003443691413849592\n",
      "training loss, Batch 115350: 0.00622965581715107\n",
      "training loss, Batch 115400: 0.003233746625483036\n",
      "training loss, Batch 115450: 0.004364757798612118\n",
      "training loss, Batch 115500: 0.003232679795473814\n",
      "training loss, Batch 115550: 0.002856426639482379\n",
      "training loss, Batch 115600: 0.005597853101789951\n",
      "training loss, Batch 115650: 0.0016263508005067706\n",
      "training loss, Batch 115700: 0.004554360173642635\n",
      "training loss, Batch 115750: 0.004868423566222191\n",
      "training loss, Batch 115800: 0.0029857163317501545\n",
      "training loss, Batch 115850: 0.005170327145606279\n",
      "training loss, Batch 115900: 0.0037596840411424637\n",
      "training loss, Batch 115950: 0.003937053959816694\n",
      "training loss, Batch 116000: 0.003972086124122143\n",
      "training loss, Batch 116050: 0.004602312110364437\n",
      "training loss, Batch 116100: 0.0022183856926858425\n",
      "training loss, Batch 116150: 0.003085839794948697\n",
      "training loss, Batch 116200: 0.003195658791810274\n",
      "training loss, Batch 116250: 0.0023564500734210014\n",
      "training loss, Batch 116300: 0.005770415998995304\n",
      "training loss, Batch 116350: 0.002565274015069008\n",
      "training loss, Batch 116400: 0.0038515301421284676\n",
      "training loss, Batch 116450: 0.0017180407885462046\n",
      "training loss, Batch 116500: 0.003721837420016527\n",
      "training loss, Batch 116550: 0.008539905771613121\n",
      "training loss, Batch 116600: 0.0032220289576798677\n",
      "training loss, Batch 116650: 0.0030295588076114655\n",
      "training loss, Batch 116700: 0.0043052686378359795\n",
      "training loss, Batch 116750: 0.004276284947991371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 116800: 0.004119118209928274\n",
      "training loss, Batch 116850: 0.004790591541677713\n",
      "training loss, Batch 116900: 0.004837149754166603\n",
      "training loss, Batch 116950: 0.002969900844618678\n",
      "training loss, Batch 117000: 0.009363319724798203\n",
      "training loss, Batch 117050: 0.006094101816415787\n",
      "training loss, Batch 117100: 0.0023488420993089676\n",
      "training loss, Batch 117150: 0.003470875322818756\n",
      "training loss, Batch 117200: 0.003021545009687543\n",
      "training loss, Batch 117250: 0.0022014339920133352\n",
      "training loss, Batch 117300: 0.004231839440762997\n",
      "training loss, Batch 117350: 0.0037938510067760944\n",
      "training loss, Batch 117400: 0.005695745348930359\n",
      "training loss, Batch 117450: 0.003330992069095373\n",
      "training loss, Batch 117500: 0.002752000465989113\n",
      "training loss, Batch 117550: 0.003675345331430435\n",
      "training loss, Batch 117600: 0.003715165425091982\n",
      "training loss, Batch 117650: 0.006277009844779968\n",
      "training loss, Batch 117700: 0.002422286896035075\n",
      "training loss, Batch 117750: 0.003702638205140829\n",
      "training loss, Batch 117800: 0.0023961456026881933\n",
      "training loss, Batch 117850: 0.0023322287015616894\n",
      "training loss, Batch 117900: 0.0038477808702737093\n",
      "training loss, Batch 117950: 0.005611570551991463\n",
      "training loss, Batch 118000: 0.0034672203473746777\n",
      "training loss, Batch 118050: 0.0034290137700736523\n",
      "training loss, Batch 118100: 0.0027829052414745092\n",
      "training loss, Batch 118150: 0.004904571454972029\n",
      "training loss, Batch 118200: 0.0035465050023049116\n",
      "training loss, Batch 118250: 0.0018845966551452875\n",
      "training loss, Batch 118300: 0.0035398718900978565\n",
      "training loss, Batch 118350: 0.004399622790515423\n",
      "training loss, Batch 118400: 0.006194101646542549\n",
      "training loss, Batch 118450: 0.002631776500493288\n",
      "training loss, Batch 118500: 0.0036162096075713634\n",
      "training loss, Batch 118550: 0.004120516590774059\n",
      "training loss, Batch 118600: 0.0017334625590592623\n",
      "training loss, Batch 118650: 0.003111048601567745\n",
      "training loss, Batch 118700: 0.00449878815561533\n",
      "training loss, Batch 118750: 0.002549718599766493\n",
      "training loss, Batch 118800: 0.0031255981884896755\n",
      "training loss, Batch 118850: 0.007898125797510147\n",
      "training loss, Batch 118900: 0.003275895956903696\n",
      "training loss, Batch 118950: 0.005878525786101818\n",
      "training loss, Batch 119000: 0.003628804348409176\n",
      "training loss, Batch 119050: 0.003870429005473852\n",
      "training loss, Batch 119100: 0.003367201890796423\n",
      "training loss, Batch 119150: 0.0023566430900245905\n",
      "training loss, Batch 119200: 0.00818992406129837\n",
      "training loss, Batch 119250: 0.005394629202783108\n",
      "training loss, Batch 119300: 0.002856597537174821\n",
      "training loss, Batch 119350: 0.004602658562362194\n",
      "training loss, Batch 119400: 0.0022257897071540356\n",
      "training loss, Batch 119450: 0.0024118470028042793\n",
      "training loss, Batch 119500: 0.006375979632139206\n",
      "training loss, Batch 119550: 0.0037105008959770203\n",
      "training loss, Batch 119600: 0.0051267873495817184\n",
      "training loss, Batch 119650: 0.00463511236011982\n",
      "training loss, Batch 119700: 0.007034401409327984\n",
      "training loss, Batch 119750: 0.003058662870898843\n",
      "training loss, Batch 119800: 0.0019011537078768015\n",
      "training loss, Batch 119850: 0.02124384045600891\n",
      "training loss, Batch 119900: 0.0049916720017790794\n",
      "training loss, Batch 119950: 0.0059961676597595215\n",
      "training loss, Batch 120000: 0.003931473009288311\n",
      "training loss, Batch 120050: 0.004824462812393904\n",
      "training loss, Batch 120100: 0.007571052294224501\n",
      "training loss, Batch 120150: 0.004774622153490782\n",
      "training loss, Batch 120200: 0.0068360879085958\n",
      "training loss, Batch 120250: 0.004114645067602396\n",
      "training loss, Batch 120300: 0.0037546902894973755\n",
      "training loss, Batch 120350: 0.0028509427793323994\n",
      "training loss, Batch 120400: 0.00526940543204546\n",
      "training loss, Batch 120450: 0.005634480621665716\n",
      "training loss, Batch 120500: 0.0065775588154792786\n",
      "training loss, Batch 120550: 0.006573991850018501\n",
      "training loss, Batch 120600: 0.002472873777151108\n",
      "training loss, Batch 120650: 0.0030218474566936493\n",
      "training loss, Batch 120700: 0.002579167950898409\n",
      "training loss, Batch 120750: 0.0033404456917196512\n",
      "training loss, Batch 120800: 0.00837368331849575\n",
      "training loss, Batch 120850: 0.005447004921734333\n",
      "training loss, Batch 120900: 0.009752367623150349\n",
      "training loss, Batch 120950: 0.007903661578893661\n",
      "training loss, Batch 121000: 0.006359388120472431\n",
      "training loss, Batch 121050: 0.008527539670467377\n",
      "training loss, Batch 121100: 0.005118152126669884\n",
      "training loss, Batch 121150: 0.004224955569952726\n",
      "training loss, Batch 121200: 0.002842784160748124\n",
      "training loss, Batch 121250: 0.0029570874758064747\n",
      "training loss, Batch 121300: 0.005105585791170597\n",
      "training loss, Batch 121350: 0.0035903307143598795\n",
      "training loss, Batch 121400: 0.006352053955197334\n",
      "training loss, Batch 121450: 0.0028647633735090494\n",
      "training loss, Batch 121500: 0.00451350724324584\n",
      "training loss, Batch 121550: 0.003096486208960414\n",
      "training loss, Batch 121600: 0.0025517060421407223\n",
      "training loss, Batch 121650: 0.004442441277205944\n",
      "training loss, Batch 121700: 0.002308298833668232\n",
      "training loss, Batch 121750: 0.007367590442299843\n",
      "training loss, Batch 121800: 0.003679109737277031\n",
      "training loss, Batch 121850: 0.010493268258869648\n",
      "training loss, Batch 121900: 0.004493923857808113\n",
      "training loss, Batch 121950: 0.009167486801743507\n",
      "training loss, Batch 122000: 0.0034361996222287416\n",
      "training loss, Batch 122050: 0.002633916214108467\n",
      "training loss, Batch 122100: 0.004222753457725048\n",
      "training loss, Batch 122150: 0.004789022263139486\n",
      "training loss, Batch 122200: 0.00876330491155386\n",
      "training loss, Batch 122250: 0.005519113503396511\n",
      "training loss, Batch 122300: 0.007552716881036758\n",
      "training loss, Batch 122350: 0.003117101499810815\n",
      "training loss, Batch 122400: 0.0037550933193415403\n",
      "training loss, Batch 122450: 0.005254196468740702\n",
      "training loss, Batch 122500: 0.004972674883902073\n",
      "training loss, Batch 122550: 0.005459181498736143\n",
      "training loss, Batch 122600: 0.006043101195245981\n",
      "training loss, Batch 122650: 0.003778046229854226\n",
      "training loss, Batch 122700: 0.011961723677814007\n",
      "training loss, Batch 122750: 0.00210821651853621\n",
      "training loss, Batch 122800: 0.007445384748280048\n",
      "training loss, Batch 122850: 0.002779961097985506\n",
      "training loss, Batch 122900: 0.005618779920041561\n",
      "training loss, Batch 122950: 0.005734238773584366\n",
      "training loss, Batch 123000: 0.0032252278178930283\n",
      "training loss, Batch 123050: 0.0031978024635463953\n",
      "training loss, Batch 123100: 0.0029834588058292866\n",
      "training loss, Batch 123150: 0.0021920239087194204\n",
      "training loss, Batch 123200: 0.0020978664979338646\n",
      "training loss, Batch 123250: 0.0032717683352530003\n",
      "training loss, Batch 123300: 0.005624112673103809\n",
      "training loss, Batch 123350: 0.004715908318758011\n",
      "training loss, Batch 123400: 0.007527799811214209\n",
      "training loss, Batch 123450: 0.004180359188467264\n",
      "training loss, Batch 123500: 0.004407390486449003\n",
      "training loss, Batch 123550: 0.004988228436559439\n",
      "training loss, Batch 123600: 0.004838364198803902\n",
      "training loss, Batch 123650: 0.004898046143352985\n",
      "training loss, Batch 123700: 0.0038159601390361786\n",
      "training loss, Batch 123750: 0.005062014330178499\n",
      "training loss, Batch 123800: 0.005667140241712332\n",
      "training loss, Batch 123850: 0.004759227856993675\n",
      "training loss, Batch 123900: 0.006761245895177126\n",
      "training loss, Batch 123950: 0.0021055517718195915\n",
      "training loss, Batch 124000: 0.006664618384093046\n",
      "training loss, Batch 124050: 0.0046363649889826775\n",
      "training loss, Batch 124100: 0.0030521464068442583\n",
      "training loss, Batch 124150: 0.004398384131491184\n",
      "training loss, Batch 124200: 0.0026265159249305725\n",
      "training loss, Batch 124250: 0.002784412819892168\n",
      "training loss, Batch 124300: 0.0065511493012309074\n",
      "training loss, Batch 124350: 0.010099229402840137\n",
      "training loss, Batch 124400: 0.003613041713833809\n",
      "training loss, Batch 124450: 0.006019293330609798\n",
      "training loss, Batch 124500: 0.003036836162209511\n",
      "training loss, Batch 124550: 0.0037243806291371584\n",
      "training loss, Batch 124600: 0.0035926159471273422\n",
      "training loss, Batch 124650: 0.0046882666647434235\n",
      "training loss, Batch 124700: 0.005633767228573561\n",
      "training loss, Batch 124750: 0.0038732029497623444\n",
      "training loss, Batch 124800: 0.006141842342913151\n",
      "training loss, Batch 124850: 0.002767525613307953\n",
      "training loss, Batch 124900: 0.00357006280682981\n",
      "training loss, Batch 124950: 0.003336654044687748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 125000: 0.00745764933526516\n",
      "training loss, Batch 125050: 0.003967026714235544\n",
      "training loss, Batch 125100: 0.005786390975117683\n",
      "training loss, Batch 125150: 0.005474704317748547\n",
      "training loss, Batch 125200: 0.003311881795525551\n",
      "training loss, Batch 125250: 0.005063138902187347\n",
      "training loss, Batch 125300: 0.0016500280471518636\n",
      "training loss, Batch 125350: 0.008798671886324883\n",
      "training loss, Batch 125400: 0.004916992038488388\n",
      "training loss, Batch 125450: 0.005296687595546246\n",
      "training loss, Batch 125500: 0.006838257424533367\n",
      "training loss, Batch 125550: 0.017351225018501282\n",
      "training loss, Batch 125600: 0.0020284936763346195\n",
      "training loss, Batch 125650: 0.003549945540726185\n",
      "training loss, Batch 125700: 0.002029519295319915\n",
      "training loss, Batch 125750: 0.007517099846154451\n",
      "training loss, Batch 125800: 0.0028773928061127663\n",
      "training loss, Batch 125850: 0.0019260330591350794\n",
      "training loss, Batch 125900: 0.0030147205106914043\n",
      "training loss, Batch 125950: 0.0038653884548693895\n",
      "training loss, Batch 126000: 0.0033989963121712208\n",
      "training loss, Batch 126050: 0.003195571480318904\n",
      "training loss, Batch 126100: 0.0037903129123151302\n",
      "training loss, Batch 126150: 0.005183457396924496\n",
      "training loss, Batch 126200: 0.0032172496430575848\n",
      "training loss, Batch 126250: 0.007075997069478035\n",
      "training loss, Batch 126300: 0.00926253292709589\n",
      "training loss, Batch 126350: 0.001945206429809332\n",
      "training loss, Batch 126400: 0.004335497505962849\n",
      "training loss, Batch 126450: 0.003731854259967804\n",
      "training loss, Batch 126500: 0.013303468003869057\n",
      "training loss, Batch 126550: 0.016495976597070694\n",
      "training loss, Batch 126600: 0.004591950215399265\n",
      "training loss, Batch 126650: 0.003774661337956786\n",
      "training loss, Batch 126700: 0.00799455214291811\n",
      "training loss, Batch 126750: 0.0027308231219649315\n",
      "training loss, Batch 126800: 0.0043859099969267845\n",
      "training loss, Batch 126850: 0.00337015837430954\n",
      "training loss, Batch 126900: 0.004304406698793173\n",
      "training loss, Batch 126950: 0.004335285164415836\n",
      "training loss, Batch 127000: 0.0032848045229911804\n",
      "training loss, Batch 127050: 0.005644288845360279\n",
      "training loss, Batch 127100: 0.0032482596579939127\n",
      "training loss, Batch 127150: 0.0026352666318416595\n",
      "training loss, Batch 127200: 0.005653916858136654\n",
      "training loss, Batch 127250: 0.0037390002980828285\n",
      "training loss, Batch 127300: 0.0036907773464918137\n",
      "training loss, Batch 127350: 0.003586474573239684\n",
      "training loss, Batch 127400: 0.004066002555191517\n",
      "training loss, Batch 127450: 0.005800670012831688\n",
      "training loss, Batch 127500: 0.006709771230816841\n",
      "training loss, Batch 127550: 0.004045946057885885\n",
      "training loss, Batch 127600: 0.0026645143516361713\n",
      "training loss, Batch 127650: 0.004766745492815971\n",
      "training loss, Batch 127700: 0.0011194944381713867\n",
      "training loss, Batch 127750: 0.009114764630794525\n",
      "training loss, Batch 127800: 0.0029559005051851273\n",
      "training loss, Batch 127850: 0.0023289178498089314\n",
      "training loss, Batch 127900: 0.004502746742218733\n",
      "training loss, Batch 127950: 0.0029808960389345884\n",
      "training loss, Batch 128000: 0.004562603309750557\n",
      "training loss, Batch 128050: 0.0053287954069674015\n",
      "training loss, Batch 128100: 0.0036255219019949436\n",
      "training loss, Batch 128150: 0.0025556779000908136\n",
      "training loss, Batch 128200: 0.0034190071746706963\n",
      "training loss, Batch 128250: 0.003176391124725342\n",
      "training loss, Batch 128300: 0.006073750555515289\n",
      "training loss, Batch 128350: 0.0044358232989907265\n",
      "training loss, Batch 128400: 0.006584160961210728\n",
      "training loss, Batch 128450: 0.010381899774074554\n",
      "training loss, Batch 128500: 0.0031944834627211094\n",
      "training loss, Batch 128550: 0.0038480334915220737\n",
      "training loss, Batch 128600: 0.006620890460908413\n",
      "training loss, Batch 128650: 0.0027633467689156532\n",
      "training loss, Batch 128700: 0.0035190109629184008\n",
      "training loss, Batch 128750: 0.0020424709655344486\n",
      "training loss, Batch 128800: 0.0042024822905659676\n",
      "training loss, Batch 128850: 0.005339410621672869\n",
      "training loss, Batch 128900: 0.005434023216366768\n",
      "training loss, Batch 128950: 0.005978577770292759\n",
      "training loss, Batch 129000: 0.00507968757301569\n",
      "training loss, Batch 129050: 0.0023095430806279182\n",
      "training loss, Batch 129100: 0.0023507720325142145\n",
      "training loss, Batch 129150: 0.0027858703397214413\n",
      "training loss, Batch 129200: 0.002013096585869789\n",
      "training loss, Batch 129250: 0.0030743503011763096\n",
      "training loss, Batch 129300: 0.0028009628877043724\n",
      "training loss, Batch 129350: 0.0038729249499738216\n",
      "training loss, Batch 129400: 0.0034805575851351023\n",
      "training loss, Batch 129450: 0.00418584980070591\n",
      "training loss, Batch 129500: 0.007214464247226715\n",
      "training loss, Batch 129550: 0.0036536934785544872\n",
      "training loss, Batch 129600: 0.00382991717197001\n",
      "training loss, Batch 129650: 0.0031432320829480886\n",
      "training loss, Batch 129700: 0.0021132412366569042\n",
      "training loss, Batch 129750: 0.0033277624752372503\n",
      "training loss, Batch 129800: 0.003239232813939452\n",
      "training loss, Batch 129850: 0.0037323590368032455\n",
      "training loss, Batch 129900: 0.009654290042817593\n",
      "training loss, Batch 129950: 0.0033284416422247887\n",
      "training loss, Batch 130000: 0.00404921593144536\n",
      "training loss, Batch 130050: 0.004458106122910976\n",
      "training loss, Batch 130100: 0.004776251967996359\n",
      "training loss, Batch 130150: 0.005359298549592495\n",
      "training loss, Batch 130200: 0.0032587326131761074\n",
      "training loss, Batch 130250: 0.003823998849838972\n",
      "training loss, Batch 130300: 0.005228268448263407\n",
      "training loss, Batch 130350: 0.0029786317609250546\n",
      "training loss, Batch 130400: 0.004070413764566183\n",
      "training loss, Batch 130450: 0.011504107154905796\n",
      "training loss, Batch 130500: 0.009629275649785995\n",
      "training loss, Batch 130550: 0.009186477400362492\n",
      "training loss, Batch 130600: 0.00463310768827796\n",
      "training loss, Batch 130650: 0.003152312710881233\n",
      "training loss, Batch 130700: 0.0035445347893983126\n",
      "training loss, Batch 130750: 0.0035036292392760515\n",
      "training loss, Batch 130800: 0.004369696602225304\n",
      "training loss, Batch 130850: 0.002773313084617257\n",
      "training loss, Batch 130900: 0.014730972237884998\n",
      "training loss, Batch 130950: 0.0039832876063883305\n",
      "training loss, Batch 131000: 0.008999049663543701\n",
      "training loss, Batch 131050: 0.0038784295320510864\n",
      "training loss, Batch 131100: 0.009975583292543888\n",
      "training loss, Batch 131150: 0.004532412625849247\n",
      "training loss, Batch 131200: 0.003733800258487463\n",
      "training loss, Batch 131250: 0.002986602019518614\n",
      "training loss, Batch 131300: 0.006025587674230337\n",
      "training loss, Batch 131350: 0.005670112557709217\n",
      "training loss, Batch 131400: 0.0035532847978174686\n",
      "training loss, Batch 131450: 0.0032552992925047874\n",
      "training loss, Batch 131500: 0.0021720710210502148\n",
      "training loss, Batch 131550: 0.0027385286521166563\n",
      "training loss, Batch 131600: 0.002167761791497469\n",
      "training loss, Batch 131650: 0.006327013485133648\n",
      "training loss, Batch 131700: 0.0020088525488972664\n",
      "training loss, Batch 131750: 0.00526233296841383\n",
      "training loss, Batch 131800: 0.004854611121118069\n",
      "training loss, Batch 131850: 0.001831522909924388\n",
      "training loss, Batch 131900: 0.004591882228851318\n",
      "training loss, Batch 131950: 0.00443232012912631\n",
      "training loss, Batch 132000: 0.005258948542177677\n",
      "training loss, Batch 132050: 0.0029177372343838215\n",
      "training loss, Batch 132100: 0.0028548813425004482\n",
      "training loss, Batch 132150: 0.003772328607738018\n",
      "training loss, Batch 132200: 0.002587322611361742\n",
      "training loss, Batch 132250: 0.007501494605094194\n",
      "training loss, Batch 132300: 0.002386881271377206\n",
      "training loss, Batch 132350: 0.002156820148229599\n",
      "training loss, Batch 132400: 0.003486751113086939\n",
      "training loss, Batch 132450: 0.011254800483584404\n",
      "training loss, Batch 132500: 0.006085369270294905\n",
      "training loss, Batch 132550: 0.004527291748672724\n",
      "training loss, Batch 132600: 0.0067899152636528015\n",
      "training loss, Batch 132650: 0.003966062795370817\n",
      "training loss, Batch 132700: 0.005084564909338951\n",
      "training loss, Batch 132750: 0.0031122490763664246\n",
      "training loss, Batch 132800: 0.00414828397333622\n",
      "training loss, Batch 132850: 0.00593944126740098\n",
      "training loss, Batch 132900: 0.0030554793775081635\n",
      "training loss, Batch 132950: 0.006134189665317535\n",
      "training loss, Batch 133000: 0.0045471047051250935\n",
      "training loss, Batch 133050: 0.002715155016630888\n",
      "training loss, Batch 133100: 0.00648342864587903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss, Batch 133150: 0.004235154017806053\n",
      "training loss, Batch 133200: 0.0012352983467280865\n",
      "training loss, Batch 133250: 0.008053064346313477\n",
      "training loss, Batch 133300: 0.0029267838690429926\n",
      "training loss, Batch 133350: 0.008915454149246216\n",
      "training loss, Batch 133400: 0.005730926990509033\n",
      "training loss, Batch 133450: 0.004965703934431076\n",
      "training loss, Batch 133500: 0.0036684460937976837\n",
      "training loss, Batch 133550: 0.004348812624812126\n",
      "training loss, Batch 133600: 0.004658529534935951\n",
      "training loss, Batch 133650: 0.0034641376696527004\n",
      "training loss, Batch 133700: 0.0036244881339371204\n",
      "training loss, Batch 133750: 0.002542935311794281\n",
      "training loss, Batch 133800: 0.003944899886846542\n",
      "training loss, Batch 133850: 0.002842795569449663\n",
      "training loss, Batch 133900: 0.0035922816023230553\n",
      "training loss, Batch 133950: 0.00391179695725441\n",
      "training loss, Batch 134000: 0.0027458653785288334\n",
      "training loss, Batch 134050: 0.0034018829464912415\n",
      "training loss, Batch 134100: 0.006756570655852556\n",
      "training loss, Batch 134150: 0.0022999607026576996\n",
      "training loss, Batch 134200: 0.008931348100304604\n",
      "training loss, Batch 134250: 0.002190719358623028\n",
      "training loss, Batch 134300: 0.003093714127317071\n",
      "training loss, Batch 134350: 0.0064668613485991955\n",
      "training loss, Batch 134400: 0.0033799326047301292\n",
      "training loss, Batch 134450: 0.0035757413133978844\n",
      "training loss, Batch 134500: 0.00514522660523653\n",
      "training loss, Batch 134550: 0.003956513479351997\n",
      "training loss, Batch 134600: 0.002390592824667692\n",
      "training loss, Batch 134650: 0.0030830539762973785\n",
      "training loss, Batch 134700: 0.0038226614706218243\n",
      "training loss, Batch 134750: 0.0023951074108481407\n",
      "training loss, Batch 134800: 0.002696773037314415\n",
      "training loss, Batch 134850: 0.0018153439741581678\n",
      "training loss, Batch 134900: 0.0027788265142589808\n",
      "training loss, Batch 134950: 0.002356664277613163\n",
      "training loss, Batch 135000: 0.0032853023149073124\n",
      "training loss, Batch 135050: 0.003585489932447672\n",
      "training loss, Batch 135100: 0.0025208196602761745\n",
      "training loss, Batch 135150: 0.0018049317877739668\n",
      "training loss, Batch 135200: 0.003966917283833027\n",
      "training loss, Batch 135250: 0.006033887155354023\n",
      "training loss, Batch 135300: 0.006554318591952324\n",
      "training loss, Batch 135350: 0.006940911989659071\n",
      "training loss, Batch 135400: 0.0026301806792616844\n",
      "training loss, Batch 135450: 0.004974480718374252\n",
      "training loss, Batch 135500: 0.00960545800626278\n",
      "training loss, Batch 135550: 0.008523128926753998\n",
      "training loss, Batch 135600: 0.004296818282455206\n",
      "training loss, Batch 135650: 0.007284799590706825\n",
      "training loss, Batch 135700: 0.004841380752623081\n",
      "training loss, Batch 135750: 0.0037118280306458473\n",
      "training loss, Batch 135800: 0.004660396836698055\n",
      "training loss, Batch 135850: 0.006079878658056259\n",
      "training loss, Batch 135900: 0.006709180772304535\n",
      "training loss, Batch 135950: 0.0036922565195709467\n",
      "training loss, Batch 136000: 0.002960359212011099\n",
      "training loss, Batch 136050: 0.003085140371695161\n",
      "training loss, Batch 136100: 0.0029185363091528416\n",
      "training loss, Batch 136150: 0.007753258571028709\n",
      "training loss, Batch 136200: 0.009935968555510044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0406d6969d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch:\"+str(epoch))\n",
    "    for i, (image, point_cloud) in enumerate(data_loader):\n",
    "#         print('image type = ', type(image))\n",
    "#         print('image size = ', image.size())\n",
    "#         print('point_cloud size = ', point_cloud.size())\n",
    "        image, point_cloud = Variable(image), Variable(point_cloud)\n",
    "        \n",
    "        image, point_cloud = image.float().to(device=gpu_or_cpu), point_cloud.float().to(device=gpu_or_cpu)\n",
    "        pred = model(image)\n",
    "        dist1, dist2 = chamferDist(pred, point_cloud)\n",
    "        loss = (torch.mean(dist1)) + (torch.mean(dist2))\n",
    "#             emd_cost = torch.sum(dist(pred.cuda().double(), points.cuda().double()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(\"training loss, \" + \"Batch \"+ str(i) + \": \" + str(loss.item()))\n",
    "\n",
    "#         loss_test = 0\n",
    "#         for i, data in enumerate(dataloader, 0):\n",
    "#             im_test, points_test = data\n",
    "#             im_test, points_test = Variable(im_test), Variable(points_test)\n",
    "#             im_test, points_test = im_test.cuda(), points_test.cuda()\n",
    "#             pred_test = model(im_test)\n",
    "#             dist1, dist2 = chamferDist(pred_test, points_test)\n",
    "#             loss_test = (torch.mean(dist1)) + (torch.mean(dist2))\n",
    "# #             emd_test = torch.sum(dist(pred_test.cuda().double(), points_test.cuda().double()))\n",
    "#         print(\"Testing loss is:\" + str(loss_test.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "normalize = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "x = torch.randn((256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = normalize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
